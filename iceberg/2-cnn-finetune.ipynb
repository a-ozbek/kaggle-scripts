{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras.applications import vgg16, vgg19, inception_v3\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1604, 75, 75, 3)\n",
      "y_train.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "output_shape = (139, 139)\n",
    "df = pd.read_json('./data/train.json')\n",
    "X_train, y_train = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)  \n",
    "    im_band1_fft = fftpack.fftshift(np.log(np.abs(fftpack.fft2(im_band1))))\n",
    "    im_bands_avg = (im_band1 + im_band2) / 2.0\n",
    "#     # Preprocess - resize\n",
    "#     im_band1 = transform.resize(im_band1, output_shape=output_shape)\n",
    "#     im_band2 = transform.resize(im_band2, output_shape=output_shape)\n",
    "#     im_bands_avg = transform.resize(im_bands_avg, output_shape=output_shape)\n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    im_bands_avg -= np.mean(im_bands_avg)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)\n",
    "    im_bands_avg /= np.std(im_bands_avg)\n",
    "    # Concatenate\n",
    "    im = np.concatenate([im_band1, im_band2, im_bands_avg], axis=2)\n",
    "    X_train.append(im)\n",
    "    y_train.append(label)    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1283, 75, 75, 3)\n",
      "X_val.shape: (321, 75, 75, 3)\n",
      "y_train.shape: (1283,)\n",
      "y_val.shape: (321,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8981, 75, 75, 3)\n",
      "y_train.shape: (8981,)\n",
      "X_val.shape: (2247, 75, 75, 3)\n",
      "y_val.shape: (2247,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make model\n",
    "INPUT_SHAPE = (75, 75, 3)\n",
    "base_model = vgg16.VGG16(include_top=False, input_shape=INPUT_SHAPE)\n",
    "x = base_model.layers[-5].output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = GlobalMaxPooling2D()(x)\n",
    "# x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.7598Epoch 00000: val_loss improved from inf to 0.35130, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 7s - loss: 0.4793 - acc: 0.7602 - val_loss: 0.3513 - val_acc: 0.8296\n",
      "Epoch 2/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8222Epoch 00001: val_loss improved from 0.35130 to 0.31198, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 7s - loss: 0.3581 - acc: 0.8224 - val_loss: 0.3120 - val_acc: 0.8482\n",
      "Epoch 3/200\n",
      "8896/8981 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8377Epoch 00002: val_loss improved from 0.31198 to 0.29930, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 6s - loss: 0.3386 - acc: 0.8379 - val_loss: 0.2993 - val_acc: 0.8594\n",
      "Epoch 4/200\n",
      "8896/8981 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8540Epoch 00003: val_loss improved from 0.29930 to 0.28651, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 6s - loss: 0.3082 - acc: 0.8542 - val_loss: 0.2865 - val_acc: 0.8629\n",
      "Epoch 5/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.8625Epoch 00004: val_loss did not improve\n",
      "8981/8981 [==============================] - 7s - loss: 0.2984 - acc: 0.8625 - val_loss: 0.2990 - val_acc: 0.8558\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.8694Epoch 00005: val_loss improved from 0.28651 to 0.28497, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 6s - loss: 0.2864 - acc: 0.8694 - val_loss: 0.2850 - val_acc: 0.8700\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.8716Epoch 00006: val_loss did not improve\n",
      "8981/8981 [==============================] - 6s - loss: 0.2794 - acc: 0.8715 - val_loss: 0.2932 - val_acc: 0.8687\n",
      "Epoch 8/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.8743Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "8981/8981 [==============================] - 6s - loss: 0.2751 - acc: 0.8744 - val_loss: 0.2924 - val_acc: 0.8616\n",
      "Epoch 9/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.8938Epoch 00008: val_loss improved from 0.28497 to 0.28138, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 6s - loss: 0.2461 - acc: 0.8937 - val_loss: 0.2814 - val_acc: 0.8776\n",
      "Epoch 10/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.8990Epoch 00009: val_loss did not improve\n",
      "8981/8981 [==============================] - 6s - loss: 0.2313 - acc: 0.8991 - val_loss: 0.2847 - val_acc: 0.8781\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.8981Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000108900003252.\n",
      "8981/8981 [==============================] - 6s - loss: 0.2304 - acc: 0.8985 - val_loss: 0.2920 - val_acc: 0.8772\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9066Epoch 00011: val_loss improved from 0.28138 to 0.27889, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 6s - loss: 0.2198 - acc: 0.9067 - val_loss: 0.2789 - val_acc: 0.8821\n",
      "Epoch 13/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9076Epoch 00012: val_loss did not improve\n",
      "8981/8981 [==============================] - 6s - loss: 0.2143 - acc: 0.9077 - val_loss: 0.2807 - val_acc: 0.8807\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9061Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "8981/8981 [==============================] - 6s - loss: 0.2147 - acc: 0.9064 - val_loss: 0.2810 - val_acc: 0.8856\n",
      "Epoch 15/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9075Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 1.18592095896e-05.\n",
      "8981/8981 [==============================] - 6s - loss: 0.2108 - acc: 0.9074 - val_loss: 0.2808 - val_acc: 0.8843\n",
      "Epoch 16/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9145Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.91353921259e-06.\n",
      "8981/8981 [==============================] - 6s - loss: 0.2030 - acc: 0.9145 - val_loss: 0.2820 - val_acc: 0.8830\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9cf692d6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Top Model Train --\n",
    "for layer in  base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.Adam()\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "MODEL_PATH = './models/model3.h5'\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9058Epoch 00000: val_loss improved from inf to 0.27493, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 15s - loss: 0.2153 - acc: 0.9058 - val_loss: 0.2749 - val_acc: 0.8856\n",
      "Epoch 2/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9138Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 15s - loss: 0.2015 - acc: 0.9139 - val_loss: 0.2821 - val_acc: 0.8874\n",
      "Epoch 3/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9193Epoch 00002: val_loss improved from 0.27493 to 0.27221, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 15s - loss: 0.1939 - acc: 0.9193 - val_loss: 0.2722 - val_acc: 0.8896\n",
      "Epoch 4/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9228Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 15s - loss: 0.1873 - acc: 0.9229 - val_loss: 0.2727 - val_acc: 0.8883\n",
      "Epoch 5/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9230Epoch 00004: val_loss improved from 0.27221 to 0.27045, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 15s - loss: 0.1798 - acc: 0.9231 - val_loss: 0.2705 - val_acc: 0.8945\n",
      "Epoch 6/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9261Epoch 00005: val_loss improved from 0.27045 to 0.26322, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 15s - loss: 0.1725 - acc: 0.9261 - val_loss: 0.2632 - val_acc: 0.8941\n",
      "Epoch 7/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9302Epoch 00006: val_loss did not improve\n",
      "8981/8981 [==============================] - 15s - loss: 0.1695 - acc: 0.9304 - val_loss: 0.2754 - val_acc: 0.8927\n",
      "Epoch 8/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9327Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1632 - acc: 0.9327 - val_loss: 0.2694 - val_acc: 0.8945\n",
      "Epoch 9/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9368Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1560 - acc: 0.9369 - val_loss: 0.2692 - val_acc: 0.8892\n",
      "Epoch 10/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9364Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1514 - acc: 0.9365 - val_loss: 0.2665 - val_acc: 0.8932\n",
      "Epoch 11/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9417Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1476 - acc: 0.9418 - val_loss: 0.2707 - val_acc: 0.8968\n",
      "Epoch 12/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9398Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 3.12499992106e-06.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1483 - acc: 0.9400 - val_loss: 0.2686 - val_acc: 0.8963\n",
      "Epoch 13/200\n",
      "8960/8981 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9407Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 1.56249996053e-06.\n",
      "8981/8981 [==============================] - 15s - loss: 0.1474 - acc: 0.9404 - val_loss: 0.2706 - val_acc: 0.8976\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c3f745290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# -- Entire Model Train --\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=6, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.8591Epoch 00000: val_loss improved from inf to 0.25865, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2955 - acc: 0.8591 - val_loss: 0.2587 - val_acc: 0.8763\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.8598Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.3002 - acc: 0.8595 - val_loss: 0.2606 - val_acc: 0.8763\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.8622Epoch 00002: val_loss improved from 0.25865 to 0.25817, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2952 - acc: 0.8625 - val_loss: 0.2582 - val_acc: 0.8772\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.8576Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2966 - acc: 0.8574 - val_loss: 0.2593 - val_acc: 0.8727\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8606Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 4.99999987369e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2944 - acc: 0.8604 - val_loss: 0.2582 - val_acc: 0.8736\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8594Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 2.49999993684e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2980 - acc: 0.8596 - val_loss: 0.2590 - val_acc: 0.8763\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.8616Epoch 00006: val_loss improved from 0.25817 to 0.25810, saving model to ./models/model3.h5\n",
      "\n",
      "Epoch 00006: reducing learning rate to 1.24999996842e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2930 - acc: 0.8613 - val_loss: 0.2581 - val_acc: 0.8758\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.8657Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 6.24999984211e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2931 - acc: 0.8653 - val_loss: 0.2586 - val_acc: 0.8749\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.8573Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 3.12499992106e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2981 - acc: 0.8570 - val_loss: 0.2583 - val_acc: 0.8758\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8665Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 1.56249996053e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2944 - acc: 0.8665 - val_loss: 0.2582 - val_acc: 0.8754\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.8592Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 7.81249980264e-08.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2965 - acc: 0.8590 - val_loss: 0.2583 - val_acc: 0.8758\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.8614Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 3.90624990132e-08.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2969 - acc: 0.8614 - val_loss: 0.2583 - val_acc: 0.8758\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.8599Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 1.95312495066e-08.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2926 - acc: 0.8598 - val_loss: 0.2583 - val_acc: 0.8758\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.8592Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 9.7656247533e-09.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2932 - acc: 0.8596 - val_loss: 0.2583 - val_acc: 0.8758\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff344f6c190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-5)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=6, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.8604Epoch 00000: val_loss improved from inf to 0.25668, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2920 - acc: 0.8603 - val_loss: 0.2567 - val_acc: 0.8723\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.8608Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2953 - acc: 0.8609 - val_loss: 0.2577 - val_acc: 0.8749\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8641Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 4.99999987369e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2923 - acc: 0.8636 - val_loss: 0.2572 - val_acc: 0.8741\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.8636Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 2.49999993684e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2977 - acc: 0.8633 - val_loss: 0.2588 - val_acc: 0.8741\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.8618Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 1.24999996842e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2950 - acc: 0.8615 - val_loss: 0.2579 - val_acc: 0.8745\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.8566Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 6.24999984211e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2908 - acc: 0.8568 - val_loss: 0.2581 - val_acc: 0.8758\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8630Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 3.12499992106e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2937 - acc: 0.8633 - val_loss: 0.2582 - val_acc: 0.8763\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff34ed32210>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-5)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.8561Epoch 00000: val_loss improved from inf to 0.27557, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2982 - acc: 0.8560 - val_loss: 0.2756 - val_acc: 0.8660\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.8573Epoch 00001: val_loss improved from 0.27557 to 0.25265, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2979 - acc: 0.8576 - val_loss: 0.2527 - val_acc: 0.8714\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.8588Epoch 00002: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2938 - acc: 0.8585 - val_loss: 0.2697 - val_acc: 0.8736\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8598Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2951 - acc: 0.8596 - val_loss: 0.2626 - val_acc: 0.8723\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.8674Epoch 00004: val_loss improved from 0.25265 to 0.25057, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2862 - acc: 0.8679 - val_loss: 0.2506 - val_acc: 0.8781\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.8617Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2890 - acc: 0.8617 - val_loss: 0.2514 - val_acc: 0.8798\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.8602Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2893 - acc: 0.8598 - val_loss: 0.2522 - val_acc: 0.8803\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.8655Epoch 00007: val_loss improved from 0.25057 to 0.25052, saving model to ./models/model3.h5\n",
      "\n",
      "Epoch 00007: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2868 - acc: 0.8653 - val_loss: 0.2505 - val_acc: 0.8798\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.8702Epoch 00008: val_loss improved from 0.25052 to 0.25046, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2832 - acc: 0.8704 - val_loss: 0.2505 - val_acc: 0.8776\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.8600Epoch 00009: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2937 - acc: 0.8601 - val_loss: 0.2518 - val_acc: 0.8772\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.8655Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 3.12499992106e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2846 - acc: 0.8655 - val_loss: 0.2526 - val_acc: 0.8776\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.8668Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 1.56249996053e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2831 - acc: 0.8672 - val_loss: 0.2517 - val_acc: 0.8781\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.8656Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 7.81249980264e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2866 - acc: 0.8656 - val_loss: 0.2522 - val_acc: 0.8781\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.8675Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.90624990132e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2873 - acc: 0.8673 - val_loss: 0.2520 - val_acc: 0.8794\n",
      "Epoch 15/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.8684Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 1.95312495066e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2854 - acc: 0.8682 - val_loss: 0.2520 - val_acc: 0.8794\n",
      "Epoch 16/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.8646Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 9.7656247533e-08.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2925 - acc: 0.8647 - val_loss: 0.2521 - val_acc: 0.8794\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff315e05d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=5e-5)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=6, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.8655Epoch 00000: val_loss improved from inf to 0.24918, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2860 - acc: 0.8655 - val_loss: 0.2492 - val_acc: 0.8772\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.8627Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2896 - acc: 0.8628 - val_loss: 0.2516 - val_acc: 0.8741\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.8628Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2882 - acc: 0.8626 - val_loss: 0.2560 - val_acc: 0.8772\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.8673Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2875 - acc: 0.8675 - val_loss: 0.2498 - val_acc: 0.8807\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.8688Epoch 00004: val_loss improved from 0.24918 to 0.24897, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2824 - acc: 0.8692 - val_loss: 0.2490 - val_acc: 0.8776\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8665Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2851 - acc: 0.8664 - val_loss: 0.2507 - val_acc: 0.8807\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.8663Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2800 - acc: 0.8665 - val_loss: 0.2493 - val_acc: 0.8825\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.8655- ETA: 5s - loss: 0.2844 - ETA: 4s - loss: 0.Epoch 00007: val_loss improved from 0.24897 to 0.24785, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2831 - acc: 0.8653 - val_loss: 0.2478 - val_acc: 0.8789\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.8677Epoch 00008: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2842 - acc: 0.8678 - val_loss: 0.2485 - val_acc: 0.8794\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.8702Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 3.12499992106e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2794 - acc: 0.8703 - val_loss: 0.2486 - val_acc: 0.8794\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.8707Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 1.56249996053e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2762 - acc: 0.8707 - val_loss: 0.2494 - val_acc: 0.8789\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.8703Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 7.81249980264e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2794 - acc: 0.8705 - val_loss: 0.2481 - val_acc: 0.8798\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.8705Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 3.90624990132e-07.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2793 - acc: 0.8703 - val_loss: 0.2484 - val_acc: 0.8803\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff315c6e290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=5e-5)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=4, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.8593Epoch 00000: val_loss improved from inf to 0.24621, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2924 - acc: 0.8599 - val_loss: 0.2462 - val_acc: 0.8798\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.8621Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2889 - acc: 0.8625 - val_loss: 0.2637 - val_acc: 0.8763\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.8654- ETA: 5s - ETA: 4s - loss:Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 4.50000006822e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2906 - acc: 0.8654 - val_loss: 0.2602 - val_acc: 0.8772\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.8721Epoch 00003: val_loss improved from 0.24621 to 0.24591, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2799 - acc: 0.8721 - val_loss: 0.2459 - val_acc: 0.8798\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.8670- E - ETA: 1s -Epoch 00004: val_loss improved from 0.24591 to 0.24583, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2822 - acc: 0.8668 - val_loss: 0.2458 - val_acc: 0.8789\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.8709- ETA: 0s - loss: 0.2769 - acc: 0.8Epoch 00005: val_loss improved from 0.24583 to 0.24353, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2771 - acc: 0.8712 - val_loss: 0.2435 - val_acc: 0.8821\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.8722Epoch 00006: val_loss improved from 0.24353 to 0.24101, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2743 - acc: 0.8723 - val_loss: 0.2410 - val_acc: 0.8807\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.8723Epoch 00007: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2809 - acc: 0.8723 - val_loss: 0.2442 - val_acc: 0.8785\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.8709- ETA:Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 2.25000003411e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2763 - acc: 0.8711 - val_loss: 0.2433 - val_acc: 0.8821\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.8751Epoch 00009: val_loss improved from 0.24101 to 0.23952, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2738 - acc: 0.8753 - val_loss: 0.2395 - val_acc: 0.8825\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.8779- ETA: 3s - loss: 0.2679  - ETA: 2s - ETA: 0s - loss: 0.2714 - acc: Epoch 00010: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2734 - acc: 0.8777 - val_loss: 0.2410 - val_acc: 0.8834\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.8784- ETA: 6s - ETA: 4s - loss: - ETA: 3s - ETA: 1s - loss:Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 1.12500001705e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2727 - acc: 0.8786 - val_loss: 0.2431 - val_acc: 0.8825\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.8730- ETA:  - ETA: 1s - loss: Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 5.62500008527e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2724 - acc: 0.8732 - val_loss: 0.2396 - val_acc: 0.8883\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.8823Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 2.81250004264e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2680 - acc: 0.8819 - val_loss: 0.2403 - val_acc: 0.8879\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff314d2ef10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=9e-5)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.8682Epoch 00000: val_loss improved from inf to 0.25232, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2814 - acc: 0.8684 - val_loss: 0.2523 - val_acc: 0.8870\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.8712Epoch 00001: val_loss improved from 0.25232 to 0.24461, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2805 - acc: 0.8712 - val_loss: 0.2446 - val_acc: 0.8870\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.8719Epoch 00002: val_loss improved from 0.24461 to 0.23570, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2811 - acc: 0.8716 - val_loss: 0.2357 - val_acc: 0.8874\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.8692Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2760 - acc: 0.8691 - val_loss: 0.2462 - val_acc: 0.8825\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.8748Epoch 00004: val_loss improved from 0.23570 to 0.23424, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2741 - acc: 0.8746 - val_loss: 0.2342 - val_acc: 0.8905\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.8757- ETA: 3s - loss: 0. - ETA: 1s Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2747 - acc: 0.8760 - val_loss: 0.2385 - val_acc: 0.8843\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.8733Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2746 - acc: 0.8737 - val_loss: 0.2487 - val_acc: 0.8861\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.8822- ETA: 0s - loss: 0.2614 - acc: 0Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2636 - acc: 0.8821 - val_loss: 0.2457 - val_acc: 0.8892\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.8832Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2618 - acc: 0.8831 - val_loss: 0.2391 - val_acc: 0.8865\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2cbd75450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.8767Epoch 00000: val_loss improved from inf to 0.30553, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2721 - acc: 0.8763 - val_loss: 0.3055 - val_acc: 0.8460\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.8757Epoch 00001: val_loss improved from 0.30553 to 0.24652, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2743 - acc: 0.8760 - val_loss: 0.2465 - val_acc: 0.8856\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.8799- ETA: 0s - loss: 0.2713 - acEpoch 00002: val_loss improved from 0.24652 to 0.23970, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2745 - acc: 0.8800 - val_loss: 0.2397 - val_acc: 0.8803\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.8742Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2724 - acc: 0.8738 - val_loss: 0.2621 - val_acc: 0.8589\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.8799Epoch 00004: val_loss improved from 0.23970 to 0.23672, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2659 - acc: 0.8795 - val_loss: 0.2367 - val_acc: 0.8847\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.8814- ETA: 6s - loss: 0.2746 - acc: 0 - ETA: 6s - l - ETA: 4s Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2643 - acc: 0.8816 - val_loss: 0.2494 - val_acc: 0.8741\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.8789- ETA: 0s - loss: 0.2636 - acc: 0.Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 5.50000004296e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2643 - acc: 0.8784 - val_loss: 0.2433 - val_acc: 0.8781\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.8822Epoch 00007: val_loss improved from 0.23672 to 0.23465, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2610 - acc: 0.8821 - val_loss: 0.2347 - val_acc: 0.8941\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.8889Epoch 00008: val_loss improved from 0.23465 to 0.22870, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2575 - acc: 0.8882 - val_loss: 0.2287 - val_acc: 0.8914\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.8862Epoch 00009: val_loss improved from 0.22870 to 0.22869, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2525 - acc: 0.8863 - val_loss: 0.2287 - val_acc: 0.8927\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.8830Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 2.75000002148e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2556 - acc: 0.8833 - val_loss: 0.2300 - val_acc: 0.8932\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.8888- ETA: 0s - loss: 0.2508 - acc: 0.Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 1.37500001074e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2496 - acc: 0.8889 - val_loss: 0.2318 - val_acc: 0.8896\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.8860Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 6.8750000537e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2514 - acc: 0.8860 - val_loss: 0.2298 - val_acc: 0.8901\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.8924- ETA: 0s - loss: 0.2512 - acc: Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.43750002685e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2513 - acc: 0.8922 - val_loss: 0.2297 - val_acc: 0.8923\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2cadd7f10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1.1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8833Epoch 00000: val_loss improved from inf to 0.23505, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2602 - acc: 0.8835 - val_loss: 0.2351 - val_acc: 0.8896\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8914- ETA: 0s - loss: 0.2537 - acc: 0Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2551 - acc: 0.8915 - val_loss: 0.2439 - val_acc: 0.8741\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8868Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2564 - acc: 0.8869 - val_loss: 0.2353 - val_acc: 0.8879\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.8914Epoch 00003: val_loss improved from 0.23505 to 0.22953, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2494 - acc: 0.8918 - val_loss: 0.2295 - val_acc: 0.8941\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.8879- ETA: 3s - loss: 0.24 - ETA: 2s - lo - ETA: 1s - loss: 0.Epoch 00004: val_loss improved from 0.22953 to 0.22895, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2545 - acc: 0.8881 - val_loss: 0.2290 - val_acc: 0.8914\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.8901Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2455 - acc: 0.8904 - val_loss: 0.2312 - val_acc: 0.8968\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.8880Epoch 00006: val_loss improved from 0.22895 to 0.22535, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2521 - acc: 0.8874 - val_loss: 0.2254 - val_acc: 0.8941\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.8900Epoch 00007: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2483 - acc: 0.8899 - val_loss: 0.2330 - val_acc: 0.8905\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.8918- ETA: 1s - Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2468 - acc: 0.8915 - val_loss: 0.2304 - val_acc: 0.8927\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.8914Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2474 - acc: 0.8911 - val_loss: 0.2321 - val_acc: 0.8945\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.8916- ETA: 1s  - ETA: 0s - loss: 0.2438 - acc: 0.8915Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2436 - acc: 0.8917 - val_loss: 0.2311 - val_acc: 0.8941\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff346fb07d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.8797Epoch 00000: val_loss improved from inf to 0.23275, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2691 - acc: 0.8791 - val_loss: 0.2328 - val_acc: 0.8865\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.8732Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2685 - acc: 0.8732 - val_loss: 0.2395 - val_acc: 0.8887\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.8828- ETA: 0s - loss: 0.2627 - a - ETA: 0s - loss: 0.2622 - acc: 0.Epoch 00002: val_loss improved from 0.23275 to 0.22923, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2611 - acc: 0.8823 - val_loss: 0.2292 - val_acc: 0.8954\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.8814Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2603 - acc: 0.8813 - val_loss: 0.2397 - val_acc: 0.8830\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.8884- ETA: 4s - los - Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 9.99999974738e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2547 - acc: 0.8889 - val_loss: 0.2396 - val_acc: 0.8976\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.8915Epoch 00005: val_loss improved from 0.22923 to 0.22655, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2438 - acc: 0.8918 - val_loss: 0.2266 - val_acc: 0.9021\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.8930Epoch 00006: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2439 - acc: 0.8931 - val_loss: 0.2276 - val_acc: 0.8963\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.8966- ETA: 1s - Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2424 - acc: 0.8963 - val_loss: 0.2345 - val_acc: 0.8994\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.8983- ETA: 2s - loss: 0.2445 - acc: 0.8 - ETA: - ETA: 0s - loss: 0.2416 - acc: 0Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2404 - acc: 0.8982 - val_loss: 0.2297 - val_acc: 0.9025\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.8983Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2325 - acc: 0.8983 - val_loss: 0.2295 - val_acc: 0.8976\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2c9d85090>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=2e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.8870Epoch 00000: val_loss improved from inf to 0.23344, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2507 - acc: 0.8869 - val_loss: 0.2334 - val_acc: 0.8870\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.8958Epoch 00001: val_loss improved from 0.23344 to 0.22192, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2412 - acc: 0.8961 - val_loss: 0.2219 - val_acc: 0.8936\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.8952Epoch 00002: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2401 - acc: 0.8953 - val_loss: 0.2314 - val_acc: 0.8887\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.8928Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2388 - acc: 0.8927 - val_loss: 0.2401 - val_acc: 0.8972\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.8999Epoch 00004: val_loss improved from 0.22192 to 0.22081, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2363 - acc: 0.8996 - val_loss: 0.2208 - val_acc: 0.9034\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.8995- ET - ETA: 1s - loss: 0.240 - ETA: 0s - loss: 0.2353 - acc: 0.Epoch 00005: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2343 - acc: 0.8996 - val_loss: 0.2311 - val_acc: 0.8985\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.8945Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2373 - acc: 0.8943 - val_loss: 0.2288 - val_acc: 0.8932\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9002Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2300 - acc: 0.9002 - val_loss: 0.2263 - val_acc: 0.9016\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9041- ETA: 3s - loss: 0.2245 - acc: 0 - ETA:  - ETA: 1s - loss: 0.22Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2287 - acc: 0.9039 - val_loss: 0.2265 - val_acc: 0.8950\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2c91dc910>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.8976- ETA: 0s - loss: 0.2350 - acc: 0.89Epoch 00000: val_loss improved from inf to 0.25478, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2348 - acc: 0.8975 - val_loss: 0.2548 - val_acc: 0.8652\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.8954- ETA: 4s - loss: Epoch 00001: val_loss improved from 0.25478 to 0.23786, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2363 - acc: 0.8953 - val_loss: 0.2379 - val_acc: 0.8985\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.8935- ETA: 5s - lo - ETA: 3s - loss: 0.2 - ETEpoch 00002: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2373 - acc: 0.8936 - val_loss: 0.2403 - val_acc: 0.8812\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.8990- ETA: 0s - loss: 0.2385 - - ETA: 0s - loss: 0.2394 - acc: 0.899Epoch 00003: val_loss improved from 0.23786 to 0.22816, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2398 - acc: 0.8992 - val_loss: 0.2282 - val_acc: 0.8865\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9018- ETA: 4s - loEpoch 00004: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2358 - acc: 0.9019 - val_loss: 0.2294 - val_acc: 0.9016\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.8992Epoch 00005: val_loss improved from 0.22816 to 0.22781, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2310 - acc: 0.8993 - val_loss: 0.2278 - val_acc: 0.8945\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9024- ETA: 0s - loss: 0.2317 - acEpoch 00006: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2326 - acc: 0.9018 - val_loss: 0.2294 - val_acc: 0.8950\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.8987Epoch 00007: val_loss improved from 0.22781 to 0.22638, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2366 - acc: 0.8989 - val_loss: 0.2264 - val_acc: 0.8959\n",
      "Epoch 9/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9030- ETA: 1s - loss: 0Epoch 00008: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2313 - acc: 0.9030 - val_loss: 0.2342 - val_acc: 0.8896\n",
      "Epoch 10/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.8989Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2354 - acc: 0.8990 - val_loss: 0.2368 - val_acc: 0.8883\n",
      "Epoch 11/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9027Epoch 00010: val_loss improved from 0.22638 to 0.22573, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2272 - acc: 0.9026 - val_loss: 0.2257 - val_acc: 0.9021\n",
      "Epoch 12/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9033Epoch 00011: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2243 - acc: 0.9034 - val_loss: 0.2274 - val_acc: 0.8976\n",
      "Epoch 13/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9033Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2215 - acc: 0.9032 - val_loss: 0.2353 - val_acc: 0.8879\n",
      "Epoch 14/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9073Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2237 - acc: 0.9074 - val_loss: 0.2269 - val_acc: 0.8990\n",
      "Epoch 15/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9040Epoch 00014: val_loss improved from 0.22573 to 0.22475, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2219 - acc: 0.9039 - val_loss: 0.2247 - val_acc: 0.8959\n",
      "Epoch 16/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9082Epoch 00015: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2205 - acc: 0.9081 - val_loss: 0.2256 - val_acc: 0.8994\n",
      "Epoch 17/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9078Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 6.24999984211e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2191 - acc: 0.9078 - val_loss: 0.2264 - val_acc: 0.9016\n",
      "Epoch 18/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9075- ETA: 0s - loss: 0.2222 - acEpoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.12499992106e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2212 - acc: 0.9076 - val_loss: 0.2251 - val_acc: 0.8994\n",
      "Epoch 19/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9110Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.56249996053e-06.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2180 - acc: 0.9108 - val_loss: 0.2256 - val_acc: 0.8981\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2c8751f90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.8999Epoch 00000: val_loss improved from inf to 0.22269, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2301 - acc: 0.8995 - val_loss: 0.2227 - val_acc: 0.8968\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9041Epoch 00001: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2246 - acc: 0.9040 - val_loss: 0.2235 - val_acc: 0.8976\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.8996- ETA:Epoch 00002: val_loss improved from 0.22269 to 0.22182, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2264 - acc: 0.8997 - val_loss: 0.2218 - val_acc: 0.8963\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9067- ETA: 0s - loss: 0.2319 - - ETA: 0s - loss: 0.2304 - acc: 0.9067Epoch 00003: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2304 - acc: 0.9066 - val_loss: 0.2368 - val_acc: 0.9039\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9011- ETA: 4s - loss: 0.2208 - acc: 0. - ETA - ETA: 1s - Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2248 - acc: 0.9015 - val_loss: 0.2349 - val_acc: 0.9008\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9079- ETA: 1s - loss: 0.2184 - acc:  - ETA: 0s - loss: 0.2173 - Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2203 - acc: 0.9078 - val_loss: 0.2256 - val_acc: 0.8927\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9117Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2197 - acc: 0.9116 - val_loss: 0.2250 - val_acc: 0.9003\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2ade0e910>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8981 samples, validate on 2247 samples\n",
      "Epoch 1/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.8952Epoch 00000: val_loss improved from inf to 0.22918, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2392 - acc: 0.8947 - val_loss: 0.2292 - val_acc: 0.8905\n",
      "Epoch 2/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.8939Epoch 00001: val_loss improved from 0.22918 to 0.22562, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2398 - acc: 0.8934 - val_loss: 0.2256 - val_acc: 0.8959\n",
      "Epoch 3/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.8966Epoch 00002: val_loss improved from 0.22562 to 0.22435, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2350 - acc: 0.8970 - val_loss: 0.2244 - val_acc: 0.8919\n",
      "Epoch 4/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.8957Epoch 00003: val_loss improved from 0.22435 to 0.22090, saving model to ./models/model3.h5\n",
      "8981/8981 [==============================] - 10s - loss: 0.2424 - acc: 0.8959 - val_loss: 0.2209 - val_acc: 0.9030\n",
      "Epoch 5/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.8978- ETA: 6s - loss: 0.2396 - - ETA: 0s - loss: 0.2402 Epoch 00004: val_loss did not improve\n",
      "8981/8981 [==============================] - 10s - loss: 0.2383 - acc: 0.8976 - val_loss: 0.2278 - val_acc: 0.8976\n",
      "Epoch 6/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.8983- ETA: 4s - l - EEpoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 4.99999987369e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2351 - acc: 0.8981 - val_loss: 0.2290 - val_acc: 0.8981\n",
      "Epoch 7/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9032- ETA: 0s - loss: 0.2320Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 2.49999993684e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2296 - acc: 0.9036 - val_loss: 0.2325 - val_acc: 0.8972\n",
      "Epoch 8/200\n",
      "8928/8981 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9001- ETA: 5s - loss: 0 - ETA: 4s - Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 1.24999996842e-05.\n",
      "8981/8981 [==============================] - 10s - loss: 0.2295 - acc: 0.8999 - val_loss: 0.2263 - val_acc: 0.8981\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2ad199d10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model('./models/model3 (copy).h5')\n",
    "\n",
    "# Compile\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD(lr=1e-4)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Fit\n",
    "m_q = 'val_loss'\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=MODEL_PATH, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=3, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)  \n",
    "    im_bands_avg = (im_band1 + im_band2) / 2.0\n",
    "    # Preprocess\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)  \n",
    "    im_bands_avg /= np.std(im_bands_avg)\n",
    "    im = np.concatenate([im_band1, im_band2, im_bands_avg], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "MODEL_PATH = './models/model3 (copy).h5'\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 3s     \n",
      "8352/8424 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# predict - tta\n",
    "y_test_p = 0\n",
    "for func in aug_funcs:\n",
    "    y_test_p += model.predict(func(X_test), verbose=1).flatten()\n",
    "y_test_p = y_test_p / len(aug_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
