{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add more LeakyReLU, more dropout etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers, layers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1284, 75, 75, 2)\n",
      "y_train.shape: (1284,)\n",
      "X_val.shape: (320, 75, 75, 2)\n",
      "y_val.shape: (320,)\n",
      "np.mean(y_train): 0.469626168224\n",
      "np.mean(y_val): 0.46875\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8988, 75, 75, 2)\n",
      "y_train.shape: (8988,)\n",
      "X_val.shape: (2240, 75, 75, 2)\n",
      "y_val.shape: (2240,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU())          \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.33))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.SGD()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8988 samples, validate on 2240 samples\n",
      "lr: 0.0244853858868\n",
      "Epoch 1/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.6373 - acc: 0.6158Epoch 00001: val_loss improved from inf to 0.62154, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 674us/step - loss: 0.6374 - acc: 0.6157 - val_loss: 0.6215 - val_acc: 0.6509\n",
      "lr: 0.000870361477409\n",
      "Epoch 2/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.7321Epoch 00002: val_loss improved from 0.62154 to 0.59888, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 626us/step - loss: 0.5349 - acc: 0.7322 - val_loss: 0.5989 - val_acc: 0.6964\n",
      "lr: 0.000967010312102\n",
      "Epoch 3/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.7461Epoch 00003: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.5176 - acc: 0.7460 - val_loss: 0.6025 - val_acc: 0.6464\n",
      "lr: 0.025\n",
      "Epoch 4/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.7473Epoch 00004: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 625us/step - loss: 0.5103 - acc: 0.7472 - val_loss: 0.6080 - val_acc: 0.6054\n",
      "lr: 0.025\n",
      "Epoch 5/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.7993Epoch 00005: val_loss improved from 0.59888 to 0.37370, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 619us/step - loss: 0.4256 - acc: 0.7996 - val_loss: 0.3737 - val_acc: 0.8585\n",
      "lr: 0.00682498422932\n",
      "Epoch 6/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8570Epoch 00006: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 605us/step - loss: 0.3306 - acc: 0.8573 - val_loss: 0.3942 - val_acc: 0.8210\n",
      "lr: 0.025\n",
      "Epoch 7/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8323Epoch 00007: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 600us/step - loss: 0.3794 - acc: 0.8326 - val_loss: 0.3819 - val_acc: 0.8330\n",
      "lr: 0.00661150953201\n",
      "Epoch 8/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.8660Epoch 00008: val_loss improved from 0.37370 to 0.31587, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 638us/step - loss: 0.3059 - acc: 0.8654 - val_loss: 0.3159 - val_acc: 0.8696\n",
      "lr: 0.025\n",
      "Epoch 9/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8575Epoch 00009: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.3294 - acc: 0.8575 - val_loss: 0.3788 - val_acc: 0.8232\n",
      "lr: 0.00391068223637\n",
      "Epoch 10/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.8798Epoch 00010: val_loss improved from 0.31587 to 0.31553, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 5s 605us/step - loss: 0.2804 - acc: 0.8797 - val_loss: 0.3155 - val_acc: 0.8737\n",
      "lr: 0.00532006672537\n",
      "Epoch 11/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.8837Epoch 00011: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 598us/step - loss: 0.2743 - acc: 0.8836 - val_loss: 0.3162 - val_acc: 0.8750\n",
      "lr: 0.00426299691203\n",
      "Epoch 12/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.8863Epoch 00012: val_loss improved from 0.31553 to 0.29862, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.2645 - acc: 0.8861 - val_loss: 0.2986 - val_acc: 0.8821\n",
      "lr: 0.025\n",
      "Epoch 13/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8692Epoch 00013: val_loss improved from 0.29862 to 0.28211, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 5s 609us/step - loss: 0.3084 - acc: 0.8692 - val_loss: 0.2821 - val_acc: 0.8857\n",
      "lr: 0.0134162394559\n",
      "Epoch 14/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.8860Epoch 00014: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.2706 - acc: 0.8862 - val_loss: 0.2849 - val_acc: 0.8906\n",
      "lr: 0.00116093756203\n",
      "Epoch 15/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.8899Epoch 00015: val_loss improved from 0.28211 to 0.28073, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.2534 - acc: 0.8899 - val_loss: 0.2807 - val_acc: 0.8866\n",
      "lr: 0.025\n",
      "Epoch 16/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.8787Epoch 00016: val_loss improved from 0.28073 to 0.27192, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.2810 - acc: 0.8788 - val_loss: 0.2719 - val_acc: 0.8821\n",
      "lr: 0.00230899588384\n",
      "Epoch 17/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.8953Epoch 00017: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 619us/step - loss: 0.2432 - acc: 0.8954 - val_loss: 0.2810 - val_acc: 0.8848\n",
      "lr: 0.000315306506989\n",
      "Epoch 18/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.8939Epoch 00018: val_loss improved from 0.27192 to 0.27015, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 5s 603us/step - loss: 0.2435 - acc: 0.8939 - val_loss: 0.2702 - val_acc: 0.8857\n",
      "lr: 0.0153908961893\n",
      "Epoch 19/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.8927Epoch 00019: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.2528 - acc: 0.8926 - val_loss: 0.2726 - val_acc: 0.8933\n",
      "lr: 0.025\n",
      "Epoch 20/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.8840Epoch 00020: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 626us/step - loss: 0.2776 - acc: 0.8841 - val_loss: 0.2989 - val_acc: 0.8875\n",
      "lr: 0.0051077163494\n",
      "Epoch 21/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.8991Epoch 00021: val_loss improved from 0.27015 to 0.25749, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 613us/step - loss: 0.2335 - acc: 0.8990 - val_loss: 0.2575 - val_acc: 0.8875\n",
      "lr: 0.0116630498612\n",
      "Epoch 22/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9036Epoch 00022: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.2332 - acc: 0.9038 - val_loss: 0.2786 - val_acc: 0.8920\n",
      "lr: 0.00671200314555\n",
      "Epoch 23/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9058Epoch 00023: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 613us/step - loss: 0.2279 - acc: 0.9059 - val_loss: 0.2703 - val_acc: 0.8920\n",
      "lr: 0.0120803821977\n",
      "Epoch 24/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9039Epoch 00024: val_loss improved from 0.25749 to 0.24598, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.2299 - acc: 0.9041 - val_loss: 0.2460 - val_acc: 0.8835\n",
      "lr: 0.025\n",
      "Epoch 25/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.8923Epoch 00025: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.2492 - acc: 0.8923 - val_loss: 0.3406 - val_acc: 0.8665\n",
      "lr: 0.025\n",
      "Epoch 26/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.8980Epoch 00026: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 618us/step - loss: 0.2384 - acc: 0.8976 - val_loss: 0.2546 - val_acc: 0.8969\n",
      "lr: 0.025\n",
      "Epoch 27/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9006Epoch 00027: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.2399 - acc: 0.9004 - val_loss: 0.2521 - val_acc: 0.8888\n",
      "lr: 0.025\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9037Epoch 00028: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 633us/step - loss: 0.2263 - acc: 0.9039 - val_loss: 0.3546 - val_acc: 0.8643\n",
      "lr: 0.00298225497248\n",
      "Epoch 29/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9088Epoch 00029: val_loss improved from 0.24598 to 0.24162, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 630us/step - loss: 0.2100 - acc: 0.9088 - val_loss: 0.2416 - val_acc: 0.8960\n",
      "lr: 0.0190365920592\n",
      "Epoch 30/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9095Epoch 00030: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 620us/step - loss: 0.2178 - acc: 0.9091 - val_loss: 0.2477 - val_acc: 0.9009\n",
      "lr: 0.025\n",
      "Epoch 31/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9076Epoch 00031: val_loss improved from 0.24162 to 0.23660, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.2219 - acc: 0.9077 - val_loss: 0.2366 - val_acc: 0.9013\n",
      "lr: 0.025\n",
      "Epoch 32/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9088Epoch 00032: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 618us/step - loss: 0.2127 - acc: 0.9088 - val_loss: 0.2914 - val_acc: 0.8920\n",
      "lr: 0.025\n",
      "Epoch 33/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9138Epoch 00033: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 622us/step - loss: 0.2078 - acc: 0.9139 - val_loss: 0.2381 - val_acc: 0.8951\n",
      "lr: 0.0216093139095\n",
      "Epoch 34/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9134Epoch 00034: val_loss improved from 0.23660 to 0.23190, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 621us/step - loss: 0.2003 - acc: 0.9136 - val_loss: 0.2319 - val_acc: 0.8991\n",
      "lr: 0.0150795232496\n",
      "Epoch 35/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9201Epoch 00035: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 618us/step - loss: 0.1880 - acc: 0.9198 - val_loss: 0.2365 - val_acc: 0.8942\n",
      "lr: 0.00566118397815\n",
      "Epoch 36/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9263Epoch 00036: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.1743 - acc: 0.9263 - val_loss: 0.2429 - val_acc: 0.9067\n",
      "lr: 0.00964140373352\n",
      "Epoch 37/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9267Epoch 00037: val_loss improved from 0.23190 to 0.22455, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.1739 - acc: 0.9267 - val_loss: 0.2246 - val_acc: 0.9040\n",
      "lr: 0.0117313926357\n",
      "Epoch 38/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9261Epoch 00038: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 611us/step - loss: 0.1733 - acc: 0.9260 - val_loss: 0.2311 - val_acc: 0.9116\n",
      "lr: 0.025\n",
      "Epoch 39/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9196Epoch 00039: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 609us/step - loss: 0.1948 - acc: 0.9197 - val_loss: 0.2614 - val_acc: 0.8933\n",
      "lr: 0.0227914367089\n",
      "Epoch 40/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9227Epoch 00040: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 620us/step - loss: 0.1812 - acc: 0.9225 - val_loss: 0.2351 - val_acc: 0.8969\n",
      "lr: 0.025\n",
      "Epoch 41/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9277Epoch 00041: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 617us/step - loss: 0.1829 - acc: 0.9277 - val_loss: 0.2694 - val_acc: 0.9049\n",
      "lr: 0.00947889699643\n",
      "Epoch 42/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9346Epoch 00042: val_loss improved from 0.22455 to 0.21955, saving model to ./models/model11/model5.h5\n",
      "8988/8988 [==============================] - 6s 613us/step - loss: 0.1596 - acc: 0.9347 - val_loss: 0.2196 - val_acc: 0.9112\n",
      "lr: 0.025\n",
      "Epoch 43/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9262Epoch 00043: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 605us/step - loss: 0.1758 - acc: 0.9262 - val_loss: 0.2238 - val_acc: 0.9103\n",
      "lr: 0.025\n",
      "Epoch 44/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9291Epoch 00044: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 612us/step - loss: 0.1691 - acc: 0.9291 - val_loss: 0.2412 - val_acc: 0.9031\n",
      "lr: 0.025\n",
      "Epoch 45/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9329Epoch 00045: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 613us/step - loss: 0.1679 - acc: 0.9330 - val_loss: 0.2289 - val_acc: 0.9027\n",
      "lr: 0.025\n",
      "Epoch 46/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9346Epoch 00046: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 610us/step - loss: 0.1611 - acc: 0.9346 - val_loss: 0.2374 - val_acc: 0.9022\n",
      "lr: 0.025\n",
      "Epoch 47/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9325Epoch 00047: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 604us/step - loss: 0.1580 - acc: 0.9325 - val_loss: 0.2222 - val_acc: 0.9067\n",
      "lr: 0.025\n",
      "Epoch 48/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9408Epoch 00048: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 612us/step - loss: 0.1482 - acc: 0.9407 - val_loss: 0.2239 - val_acc: 0.9058\n",
      "lr: 0.025\n",
      "Epoch 49/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9449Epoch 00049: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 611us/step - loss: 0.1408 - acc: 0.9448 - val_loss: 0.2371 - val_acc: 0.9098\n",
      "lr: 0.0219850070921\n",
      "Epoch 50/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9401Epoch 00050: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 613us/step - loss: 0.1403 - acc: 0.9403 - val_loss: 0.2362 - val_acc: 0.9058\n",
      "lr: 0.025\n",
      "Epoch 51/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9452Epoch 00051: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.1376 - acc: 0.9454 - val_loss: 0.2313 - val_acc: 0.9080\n",
      "lr: 0.025\n",
      "Epoch 52/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9432Epoch 00052: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 615us/step - loss: 0.1367 - acc: 0.9431 - val_loss: 0.2931 - val_acc: 0.8888\n",
      "lr: 0.0167748879234\n",
      "Epoch 53/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9511Epoch 00053: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 611us/step - loss: 0.1201 - acc: 0.9513 - val_loss: 0.2374 - val_acc: 0.9129\n",
      "lr: 0.0161270060075\n",
      "Epoch 54/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9567Epoch 00054: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 607us/step - loss: 0.1101 - acc: 0.9567 - val_loss: 0.2406 - val_acc: 0.9129\n",
      "lr: 0.025\n",
      "Epoch 55/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9510Epoch 00055: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 611us/step - loss: 0.1216 - acc: 0.9509 - val_loss: 0.2467 - val_acc: 0.9018\n",
      "lr: 0.025\n",
      "Epoch 56/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9550Epoch 00056: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.1137 - acc: 0.9551 - val_loss: 0.3130 - val_acc: 0.8911\n",
      "lr: 0.00312175287554\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9605Epoch 00057: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 647us/step - loss: 0.0953 - acc: 0.9605 - val_loss: 0.2428 - val_acc: 0.9121\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa1e162690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model11/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=15, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, schedule_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 1s 172us/step\n",
      "8424/8424 [==============================] - 2s 190us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 249us/step\n",
      "8424/8424 [==============================] - 2s 200us/step\n",
      "8424/8424 [==============================] - 2s 184us/step\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 187us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 201us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 178us/step\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 263us/step\n",
      "8424/8424 [==============================] - 2s 253us/step\n",
      "8424/8424 [==============================] - 2s 267us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 197us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 227us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 200us/step\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 194us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "weights = [0.25, 0.50 / 3, 0.35, 0.40 / 3, 0.55 / 3]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model11/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
