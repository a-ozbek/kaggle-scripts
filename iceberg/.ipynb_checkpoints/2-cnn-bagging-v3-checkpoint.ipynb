{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add LeakyReLU, more dropout etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1284, 75, 75, 2)\n",
      "y_train.shape: (1284,)\n",
      "X_val.shape: (320, 75, 75, 2)\n",
      "y_val.shape: (320,)\n",
      "np.mean(y_train): 0.469626168224\n",
      "np.mean(y_val): 0.46875\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8988, 75, 75, 2)\n",
      "y_train.shape: (8988,)\n",
      "X_val.shape: (2240, 75, 75, 2)\n",
      "y_val.shape: (2240,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU())          \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.SGD()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8988 samples, validate on 2240 samples\n",
      "lr: 0.0237932429546\n",
      "Epoch 1/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.6134Epoch 00001: val_loss improved from inf to 0.64523, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 684us/step - loss: 0.6404 - acc: 0.6132 - val_loss: 0.6452 - val_acc: 0.5821\n",
      "lr: 0.000870361477409\n",
      "Epoch 2/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5469 - acc: 0.7251Epoch 00002: val_loss improved from 0.64523 to 0.61655, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 636us/step - loss: 0.5466 - acc: 0.7254 - val_loss: 0.6165 - val_acc: 0.6679\n",
      "lr: 0.000967010312102\n",
      "Epoch 3/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7386Epoch 00003: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 628us/step - loss: 0.5272 - acc: 0.7385 - val_loss: 0.6197 - val_acc: 0.6161\n",
      "lr: 0.0327901395233\n",
      "Epoch 4/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.5162 - acc: 0.7434Epoch 00004: val_loss improved from 0.61655 to 0.59289, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 628us/step - loss: 0.5162 - acc: 0.7433 - val_loss: 0.5929 - val_acc: 0.6362\n",
      "lr: 0.0252524702392\n",
      "Epoch 5/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.7988Epoch 00005: val_loss improved from 0.59289 to 0.38230, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.4303 - acc: 0.7992 - val_loss: 0.3823 - val_acc: 0.8567\n",
      "lr: 0.00682498422932\n",
      "Epoch 6/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8547Epoch 00006: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 621us/step - loss: 0.3326 - acc: 0.8549 - val_loss: 0.4167 - val_acc: 0.8076\n",
      "lr: 0.0293907683963\n",
      "Epoch 7/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8324Epoch 00007: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 635us/step - loss: 0.3781 - acc: 0.8327 - val_loss: 0.3841 - val_acc: 0.8290\n",
      "lr: 0.00661150953201\n",
      "Epoch 8/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.8641Epoch 00008: val_loss improved from 0.38230 to 0.32057, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 634us/step - loss: 0.3068 - acc: 0.8635 - val_loss: 0.3206 - val_acc: 0.8741\n",
      "lr: 0.035545649158\n",
      "Epoch 9/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8528Epoch 00009: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 610us/step - loss: 0.3345 - acc: 0.8528 - val_loss: 0.3577 - val_acc: 0.8442\n",
      "lr: 0.00391068223637\n",
      "Epoch 10/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.8786Epoch 00010: val_loss improved from 0.32057 to 0.31469, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 627us/step - loss: 0.2806 - acc: 0.8785 - val_loss: 0.3147 - val_acc: 0.8746\n",
      "lr: 0.00532006672537\n",
      "Epoch 11/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.8811Epoch 00011: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 621us/step - loss: 0.2740 - acc: 0.8810 - val_loss: 0.3196 - val_acc: 0.8737\n",
      "lr: 0.00426299691203\n",
      "Epoch 12/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.8869Epoch 00012: val_loss improved from 0.31469 to 0.29682, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 640us/step - loss: 0.2663 - acc: 0.8868 - val_loss: 0.2968 - val_acc: 0.8857\n",
      "lr: 0.0287777634394\n",
      "Epoch 13/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8700Epoch 00013: val_loss improved from 0.29682 to 0.28015, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 616us/step - loss: 0.3106 - acc: 0.8698 - val_loss: 0.2802 - val_acc: 0.8821\n",
      "lr: 0.0134162394559\n",
      "Epoch 14/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.8838Epoch 00014: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.2721 - acc: 0.8840 - val_loss: 0.2813 - val_acc: 0.8893\n",
      "lr: 0.00116093756203\n",
      "Epoch 15/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.8926Epoch 00015: val_loss improved from 0.28015 to 0.27831, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 674us/step - loss: 0.2538 - acc: 0.8924 - val_loss: 0.2783 - val_acc: 0.8871\n",
      "lr: 0.0309670925305\n",
      "Epoch 16/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.8802Epoch 00016: val_loss improved from 0.27831 to 0.27352, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 645us/step - loss: 0.2779 - acc: 0.8805 - val_loss: 0.2735 - val_acc: 0.8790\n",
      "lr: 0.00230899588384\n",
      "Epoch 17/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.8961Epoch 00017: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 651us/step - loss: 0.2422 - acc: 0.8962 - val_loss: 0.2807 - val_acc: 0.8839\n",
      "lr: 0.000315306506989\n",
      "Epoch 18/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.8959Epoch 00018: val_loss improved from 0.27352 to 0.27154, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 674us/step - loss: 0.2441 - acc: 0.8960 - val_loss: 0.2715 - val_acc: 0.8897\n",
      "lr: 0.0153908961893\n",
      "Epoch 19/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.8927Epoch 00019: val_loss improved from 0.27154 to 0.26873, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 641us/step - loss: 0.2540 - acc: 0.8925 - val_loss: 0.2687 - val_acc: 0.8938\n",
      "lr: 0.0261846034453\n",
      "Epoch 20/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.8816Epoch 00020: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 629us/step - loss: 0.2767 - acc: 0.8816 - val_loss: 0.2805 - val_acc: 0.8915\n",
      "lr: 0.0051077163494\n",
      "Epoch 21/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.8988Epoch 00021: val_loss improved from 0.26873 to 0.25699, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 658us/step - loss: 0.2347 - acc: 0.8984 - val_loss: 0.2570 - val_acc: 0.8911\n",
      "lr: 0.0116630498612\n",
      "Epoch 22/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9030Epoch 00022: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 657us/step - loss: 0.2347 - acc: 0.9032 - val_loss: 0.2719 - val_acc: 0.8884\n",
      "lr: 0.00671200314555\n",
      "Epoch 23/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9040Epoch 00023: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 656us/step - loss: 0.2301 - acc: 0.9041 - val_loss: 0.2691 - val_acc: 0.8938\n",
      "lr: 0.0120803821977\n",
      "Epoch 24/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9046Epoch 00024: val_loss improved from 0.25699 to 0.24693, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 663us/step - loss: 0.2320 - acc: 0.9048 - val_loss: 0.2469 - val_acc: 0.8817\n",
      "lr: 0.0303618788459\n",
      "Epoch 25/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.8907Epoch 00025: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 606us/step - loss: 0.2515 - acc: 0.8907 - val_loss: 0.3179 - val_acc: 0.8746\n",
      "lr: 0.0280118569071\n",
      "Epoch 26/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.8950Epoch 00026: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 629us/step - loss: 0.2377 - acc: 0.8945 - val_loss: 0.2559 - val_acc: 0.8991\n",
      "lr: 0.0307236160079\n",
      "Epoch 27/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9045Epoch 00027: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 640us/step - loss: 0.2349 - acc: 0.9043 - val_loss: 0.2568 - val_acc: 0.8875\n",
      "lr: 0.0276703987741\n",
      "Epoch 28/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9021Epoch 00028: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 648us/step - loss: 0.2286 - acc: 0.9022 - val_loss: 0.3404 - val_acc: 0.8674\n",
      "lr: 0.00298225497248\n",
      "Epoch 29/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9105Epoch 00029: val_loss improved from 0.24693 to 0.24072, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 645us/step - loss: 0.2115 - acc: 0.9102 - val_loss: 0.2407 - val_acc: 0.8987\n",
      "lr: 0.0190365920592\n",
      "Epoch 30/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9109Epoch 00030: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 629us/step - loss: 0.2163 - acc: 0.9105 - val_loss: 0.2536 - val_acc: 0.9080\n",
      "lr: 0.0273148544565\n",
      "Epoch 31/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9074Epoch 00031: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 631us/step - loss: 0.2222 - acc: 0.9077 - val_loss: 0.2420 - val_acc: 0.9027\n",
      "lr: 0.0347536910444\n",
      "Epoch 32/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9075Epoch 00032: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.2141 - acc: 0.9075 - val_loss: 0.2768 - val_acc: 0.9031\n",
      "lr: 0.0398708861902\n",
      "Epoch 33/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9107Epoch 00033: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 615us/step - loss: 0.2097 - acc: 0.9109 - val_loss: 0.2431 - val_acc: 0.8938\n",
      "lr: 0.0216093139095\n",
      "Epoch 34/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9146Epoch 00034: val_loss improved from 0.24072 to 0.22816, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 5s 608us/step - loss: 0.2036 - acc: 0.9148 - val_loss: 0.2282 - val_acc: 0.9094\n",
      "lr: 0.0150795232496\n",
      "Epoch 35/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9179Epoch 00035: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 645us/step - loss: 0.1932 - acc: 0.9176 - val_loss: 0.2378 - val_acc: 0.8924\n",
      "lr: 0.00566118397815\n",
      "Epoch 36/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9253Epoch 00036: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 607us/step - loss: 0.1768 - acc: 0.9252 - val_loss: 0.2363 - val_acc: 0.9134\n",
      "lr: 0.00964140373352\n",
      "Epoch 37/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9269Epoch 00037: val_loss improved from 0.22816 to 0.22340, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 635us/step - loss: 0.1763 - acc: 0.9270 - val_loss: 0.2234 - val_acc: 0.9062\n",
      "lr: 0.0117313926357\n",
      "Epoch 38/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9297Epoch 00038: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 627us/step - loss: 0.1738 - acc: 0.9296 - val_loss: 0.2296 - val_acc: 0.9129\n",
      "lr: 0.0372369204909\n",
      "Epoch 39/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9184Epoch 00039: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 627us/step - loss: 0.1937 - acc: 0.9184 - val_loss: 0.2536 - val_acc: 0.8929\n",
      "lr: 0.0227914367089\n",
      "Epoch 40/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9227Epoch 00040: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 615us/step - loss: 0.1819 - acc: 0.9225 - val_loss: 0.2284 - val_acc: 0.8991\n",
      "lr: 0.0256576107721\n",
      "Epoch 41/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9262Epoch 00041: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 625us/step - loss: 0.1855 - acc: 0.9261 - val_loss: 0.2511 - val_acc: 0.9080\n",
      "lr: 0.00947889699643\n",
      "Epoch 42/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9320Epoch 00042: val_loss improved from 0.22340 to 0.22194, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 631us/step - loss: 0.1660 - acc: 0.9321 - val_loss: 0.2219 - val_acc: 0.9103\n",
      "lr: 0.0294450072045\n",
      "Epoch 43/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9263Epoch 00043: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 629us/step - loss: 0.1738 - acc: 0.9262 - val_loss: 0.2224 - val_acc: 0.9147\n",
      "lr: 0.0284447273072\n",
      "Epoch 44/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9286Epoch 00044: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 625us/step - loss: 0.1742 - acc: 0.9286 - val_loss: 0.2448 - val_acc: 0.8978\n",
      "lr: 0.0334656840726\n",
      "Epoch 45/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9336Epoch 00045: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 614us/step - loss: 0.1688 - acc: 0.9336 - val_loss: 0.2267 - val_acc: 0.9036\n",
      "lr: 0.0379051419787\n",
      "Epoch 46/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9303Epoch 00046: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 620us/step - loss: 0.1652 - acc: 0.9305 - val_loss: 0.2388 - val_acc: 0.9089\n",
      "lr: 0.0314388254647\n",
      "Epoch 47/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9333Epoch 00047: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s 606us/step - loss: 0.1641 - acc: 0.9332 - val_loss: 0.2269 - val_acc: 0.9054\n",
      "lr: 0.0397757071125\n",
      "Epoch 48/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9401Epoch 00048: val_loss improved from 0.22194 to 0.21944, saving model to ./models/model8/model5.h5\n",
      "8988/8988 [==============================] - 6s 629us/step - loss: 0.1504 - acc: 0.9401 - val_loss: 0.2194 - val_acc: 0.9138\n",
      "lr: 0.0308656721256\n",
      "Epoch 49/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9391Epoch 00049: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 621us/step - loss: 0.1491 - acc: 0.9390 - val_loss: 0.2370 - val_acc: 0.9067\n",
      "lr: 0.0219850070921\n",
      "Epoch 50/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9404Epoch 00050: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 654us/step - loss: 0.1461 - acc: 0.9406 - val_loss: 0.2265 - val_acc: 0.9116\n",
      "lr: 0.0338986576792\n",
      "Epoch 51/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9453Epoch 00051: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 641us/step - loss: 0.1392 - acc: 0.9453 - val_loss: 0.2392 - val_acc: 0.9045\n",
      "lr: 0.039359030355\n",
      "Epoch 52/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9411Epoch 00052: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 651us/step - loss: 0.1419 - acc: 0.9413 - val_loss: 0.2472 - val_acc: 0.9080\n",
      "lr: 0.0167748879234\n",
      "Epoch 53/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9491Epoch 00053: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 655us/step - loss: 0.1247 - acc: 0.9493 - val_loss: 0.2365 - val_acc: 0.9165\n",
      "lr: 0.0161270060075\n",
      "Epoch 54/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9545Epoch 00054: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 632us/step - loss: 0.1146 - acc: 0.9546 - val_loss: 0.2456 - val_acc: 0.9165\n",
      "lr: 0.0333983518964\n",
      "Epoch 55/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9473Epoch 00055: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.1298 - acc: 0.9472 - val_loss: 0.2282 - val_acc: 0.9152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0351235434071\n",
      "Epoch 56/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9503Epoch 00056: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 617us/step - loss: 0.1218 - acc: 0.9504 - val_loss: 0.2985 - val_acc: 0.8920\n",
      "lr: 0.00312175287554\n",
      "Epoch 57/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9610Epoch 00057: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 628us/step - loss: 0.1011 - acc: 0.9608 - val_loss: 0.2372 - val_acc: 0.9107\n",
      "lr: 0.014957714241\n",
      "Epoch 58/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9593Epoch 00058: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 623us/step - loss: 0.1060 - acc: 0.9594 - val_loss: 0.2386 - val_acc: 0.9103\n",
      "lr: 0.0277999573639\n",
      "Epoch 59/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9585Epoch 00059: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 621us/step - loss: 0.1039 - acc: 0.9586 - val_loss: 0.2481 - val_acc: 0.9062\n",
      "lr: 0.00346150893738\n",
      "Epoch 60/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9626Epoch 00060: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.0923 - acc: 0.9627 - val_loss: 0.2479 - val_acc: 0.9071\n",
      "lr: 0.0252395358238\n",
      "Epoch 61/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9561Epoch 00061: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 635us/step - loss: 0.1113 - acc: 0.9562 - val_loss: 0.2402 - val_acc: 0.9112\n",
      "lr: 0.0112313032033\n",
      "Epoch 62/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9625Epoch 00062: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 617us/step - loss: 0.0966 - acc: 0.9623 - val_loss: 0.2526 - val_acc: 0.9129\n",
      "lr: 0.00212074160783\n",
      "Epoch 63/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9656Epoch 00063: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 618us/step - loss: 0.0840 - acc: 0.9656 - val_loss: 0.2560 - val_acc: 0.9134\n",
      "lr: 0.0265132032404\n",
      "Epoch 64/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9593Epoch 00064: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.1002 - acc: 0.9592 - val_loss: 0.3035 - val_acc: 0.8982\n",
      "lr: 0.0138398968143\n",
      "Epoch 65/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9650Epoch 00065: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 628us/step - loss: 0.0877 - acc: 0.9651 - val_loss: 0.2487 - val_acc: 0.9089\n",
      "lr: 0.00423233111914\n",
      "Epoch 66/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9714Epoch 00066: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 624us/step - loss: 0.0765 - acc: 0.9715 - val_loss: 0.2585 - val_acc: 0.9112\n",
      "lr: 0.00742234652367\n",
      "Epoch 67/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9741Epoch 00067: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 620us/step - loss: 0.0675 - acc: 0.9742 - val_loss: 0.2756 - val_acc: 0.9089\n",
      "lr: 0.0118548798818\n",
      "Epoch 68/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9704Epoch 00068: val_loss did not improve\n",
      "8988/8988 [==============================] - 6s 628us/step - loss: 0.0749 - acc: 0.9704 - val_loss: 0.2644 - val_acc: 0.9094\n",
      "Epoch 00068: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc53c292c90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model8/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=20, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, schedule_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 3s 317us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 2s 218us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 223us/step\n",
      "8424/8424 [==============================] - 2s 278us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 207us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 257us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 202us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 207us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 208us/step\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 2s 198us/step\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8424/8424 [==============================] - 2s 202us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model8/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
