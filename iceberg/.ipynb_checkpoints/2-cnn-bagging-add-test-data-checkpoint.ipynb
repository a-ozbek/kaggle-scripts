{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add Test data predictions to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.signal import fftconvolve\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df_train = pd.read_json('./data/train.json')\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_json('./data/test.json')\n",
    "df_test_labels = pd.read_csv('./submissions/ourbest_explorestack_2.csv')\n",
    "df_test = pd.merge(df_test, df_test_labels, on='id')\n",
    "iceberg_cond = (df_test['is_iceberg'] >= 0.9) & (df_test['is_iceberg'] <= 1.0)\n",
    "not_iceberg_cond = (df_test['is_iceberg'] >= 0.0) & (df_test['is_iceberg'] <= 0.1)\n",
    "df_test = df_test[iceberg_cond | not_iceberg_cond]\n",
    "\n",
    "# Merge Train and Test\n",
    "df = pd.concat([df_train, df_test])\n",
    "df\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    im = im - np.mean(im)\n",
    "    im = im / np.std(im)\n",
    "    return im\n",
    "\n",
    "def get_convolve(im1, im2):\n",
    "    im1 = im1 - np.mean(im1)\n",
    "    im2 = im2 - np.mean(im2)\n",
    "    im_conv = fftconvolve(im1, im2[::-1, ::-1], mode='same')\n",
    "    return normalize(im_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4927, 75, 75, 2)\n",
      "y.shape: (4927,)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "#     im = np.concatenate([normalize(im_band1), normalize(im_band2), get_convolve(im_band1, im_band2)], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 9\n",
    "MODEL_NUMBER = 9\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4380, 75, 75, 2)\n",
      "y_train.shape: (4380,)\n",
      "X_val.shape: (547, 75, 75, 2)\n",
      "y_val.shape: (547,)\n",
      "np.mean(y_train): 0.430118419441\n",
      "np.mean(y_val): 0.430064447715\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30660, 75, 75, 2)\n",
      "y_train.shape: (30660,)\n",
      "X_val.shape: (3829, 75, 75, 2)\n",
      "y_val.shape: (3829,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU())          \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Adam()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30660 samples, validate on 3829 samples\n",
      "Epoch 1/200\n",
      "30624/30660 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.2683Epoch 00001: val_loss improved from inf to 0.14921, saving model to ./models/model12-v5/model9.h5\n",
      "30660/30660 [==============================] - 20s 640us/step - loss: 0.2674 - acc: 0.2683 - val_loss: 0.1492 - val_acc: 0.3377\n",
      "Epoch 2/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.2899Epoch 00002: val_loss improved from 0.14921 to 0.13842, saving model to ./models/model12-v5/model9.h5\n",
      "30660/30660 [==============================] - 19s 623us/step - loss: 0.1694 - acc: 0.2899 - val_loss: 0.1384 - val_acc: 0.3374\n",
      "Epoch 3/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.2929Epoch 00003: val_loss improved from 0.13842 to 0.13472, saving model to ./models/model12-v5/model9.h5\n",
      "30660/30660 [==============================] - 19s 614us/step - loss: 0.1556 - acc: 0.2929 - val_loss: 0.1347 - val_acc: 0.3413\n",
      "Epoch 4/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.2955Epoch 00004: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 616us/step - loss: 0.1481 - acc: 0.2955 - val_loss: 0.1482 - val_acc: 0.3382\n",
      "Epoch 5/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.2973Epoch 00005: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 635us/step - loss: 0.1423 - acc: 0.2973 - val_loss: 0.1467 - val_acc: 0.3301\n",
      "Epoch 6/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.2985Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000330000015674.\n",
      "30660/30660 [==============================] - 19s 633us/step - loss: 0.1363 - acc: 0.2985 - val_loss: 0.1588 - val_acc: 0.3312\n",
      "Epoch 7/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.3043Epoch 00007: val_loss improved from 0.13472 to 0.12847, saving model to ./models/model12-v5/model9.h5\n",
      "30660/30660 [==============================] - 20s 638us/step - loss: 0.1195 - acc: 0.3043 - val_loss: 0.1285 - val_acc: 0.3348\n",
      "Epoch 8/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.3059Epoch 00008: val_loss improved from 0.12847 to 0.12647, saving model to ./models/model12-v5/model9.h5\n",
      "30660/30660 [==============================] - 19s 624us/step - loss: 0.1152 - acc: 0.3058 - val_loss: 0.1265 - val_acc: 0.3364\n",
      "Epoch 9/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.3067Epoch 00009: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 609us/step - loss: 0.1139 - acc: 0.3066 - val_loss: 0.1386 - val_acc: 0.3267\n",
      "Epoch 10/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.3083Epoch 00010: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 613us/step - loss: 0.1102 - acc: 0.3083 - val_loss: 0.1313 - val_acc: 0.3346\n",
      "Epoch 11/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.3089Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000108900003252.\n",
      "30660/30660 [==============================] - 19s 619us/step - loss: 0.1093 - acc: 0.3089 - val_loss: 0.1339 - val_acc: 0.3338\n",
      "Epoch 12/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.3112Epoch 00012: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 623us/step - loss: 0.1020 - acc: 0.3112 - val_loss: 0.1401 - val_acc: 0.3338\n",
      "Epoch 13/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.3124Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "30660/30660 [==============================] - 19s 613us/step - loss: 0.1008 - acc: 0.3124 - val_loss: 0.1327 - val_acc: 0.3346\n",
      "Epoch 14/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.3134Epoch 00014: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 609us/step - loss: 0.0978 - acc: 0.3134 - val_loss: 0.1383 - val_acc: 0.3319\n",
      "Epoch 15/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.3136Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 1.18592095896e-05.\n",
      "30660/30660 [==============================] - 19s 615us/step - loss: 0.0976 - acc: 0.3136 - val_loss: 0.1359 - val_acc: 0.3353\n",
      "Epoch 16/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.3138Epoch 00016: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 616us/step - loss: 0.0966 - acc: 0.3138 - val_loss: 0.1380 - val_acc: 0.3330\n",
      "Epoch 17/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.3137Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.91353921259e-06.\n",
      "30660/30660 [==============================] - 19s 620us/step - loss: 0.0963 - acc: 0.3137 - val_loss: 0.1400 - val_acc: 0.3309\n",
      "Epoch 18/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.3134Epoch 00018: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 615us/step - loss: 0.0966 - acc: 0.3134 - val_loss: 0.1384 - val_acc: 0.3325\n",
      "Epoch 19/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.3143Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 1.29146797917e-06.\n",
      "30660/30660 [==============================] - 19s 615us/step - loss: 0.0963 - acc: 0.3143 - val_loss: 0.1376 - val_acc: 0.3327\n",
      "Epoch 20/200\n",
      "30624/30660 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.3148Epoch 00020: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 617us/step - loss: 0.0956 - acc: 0.3147 - val_loss: 0.1386 - val_acc: 0.3322\n",
      "Epoch 21/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.3135Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 4.26184415119e-07.\n",
      "30660/30660 [==============================] - 19s 610us/step - loss: 0.0965 - acc: 0.3135 - val_loss: 0.1384 - val_acc: 0.3322\n",
      "Epoch 22/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.3145Epoch 00022: val_loss did not improve\n",
      "30660/30660 [==============================] - 19s 621us/step - loss: 0.0950 - acc: 0.3145 - val_loss: 0.1384 - val_acc: 0.3322\n",
      "Epoch 23/200\n",
      "30656/30660 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3140Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 1.40640856614e-07.\n",
      "30660/30660 [==============================] - 19s 613us/step - loss: 0.0960 - acc: 0.3140 - val_loss: 0.1384 - val_acc: 0.3322\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa6d77b3310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model12-v5/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=15, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-2)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, schedule_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 244us/step\n",
      "8424/8424 [==============================] - 2s 297us/step\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 3s 302us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 236us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 2s 226us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 285us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "8424/8424 [==============================] - 2s 238us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 238us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 283us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "5\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 288us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "6\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "8424/8424 [==============================] - 2s 286us/step\n",
      "8424/8424 [==============================] - 2s 241us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "7\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "8424/8424 [==============================] - 2s 288us/step\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 278us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 226us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 274us/step\n",
      "8424/8424 [==============================] - 2s 188us/step\n",
      "8424/8424 [==============================] - 2s 219us/step\n",
      "8424/8424 [==============================] - 2s 210us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "# weights = [0.2, 0.18, 0.2, 0.2, 0.22]\n",
    "weights = [0.2] * 9\n",
    "for i, w in zip(range(9), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model12-v5/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub31.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
