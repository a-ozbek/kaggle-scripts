{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add Test data predictions to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.signal import fftconvolve\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5583\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df_train = pd.read_json('./data/train.json')\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_json('./data/test.json')\n",
    "df_test_labels = pd.read_csv('./submissions/sub5-6-7-8-10-11-12-18-19.csv')\n",
    "df_test = pd.merge(df_test, df_test_labels, on='id')\n",
    "iceberg_cond = (df_test['is_iceberg'] >= 0.9) & (df_test['is_iceberg'] <= 1.0)\n",
    "not_iceberg_cond = (df_test['is_iceberg'] >= 0.0) & (df_test['is_iceberg'] <= 0.1)\n",
    "df_test = df_test[iceberg_cond | not_iceberg_cond]\n",
    "\n",
    "# Merge Train and Test\n",
    "df = pd.concat([df_train, df_test])\n",
    "df\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    im = im - np.mean(im)\n",
    "    im = im / np.std(im)\n",
    "    return im\n",
    "\n",
    "def get_convolve(im1, im2):\n",
    "    im1 = im1 - np.mean(im1)\n",
    "    im2 = im2 - np.mean(im2)\n",
    "    im_conv = fftconvolve(im1, im2[::-1, ::-1], mode='same')\n",
    "    return normalize(im_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (5583, 75, 75, 2)\n",
      "y.shape: (5583,)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "#     im = np.concatenate([normalize(im_band1), normalize(im_band2), get_convolve(im_band1, im_band2)], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4467, 75, 75, 2)\n",
      "y_train.shape: (4467,)\n",
      "X_val.shape: (1116, 75, 75, 2)\n",
      "y_val.shape: (1116,)\n",
      "np.mean(y_train): 0.562049636694\n",
      "np.mean(y_val): 0.56099981505\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (31269, 75, 75, 2)\n",
      "y_train.shape: (31269,)\n",
      "X_val.shape: (7812, 75, 75, 2)\n",
      "y_val.shape: (7812,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU())          \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Adam()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31269 samples, validate on 7812 samples\n",
      "Epoch 1/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.2324Epoch 00001: val_loss improved from inf to 0.18013, saving model to ./models/model12/model5.h5\n",
      "31269/31269 [==============================] - 22s 699us/step - loss: 0.2688 - acc: 0.2324 - val_loss: 0.1801 - val_acc: 0.2969\n",
      "Epoch 2/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.2473Epoch 00002: val_loss improved from 0.18013 to 0.15389, saving model to ./models/model12/model5.h5\n",
      "31269/31269 [==============================] - 20s 641us/step - loss: 0.1888 - acc: 0.2472 - val_loss: 0.1539 - val_acc: 0.2980\n",
      "Epoch 3/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.2502Epoch 00003: val_loss did not improve\n",
      "31269/31269 [==============================] - 21s 674us/step - loss: 0.1792 - acc: 0.2503 - val_loss: 0.1691 - val_acc: 0.2952\n",
      "Epoch 4/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.2540Epoch 00004: val_loss did not improve\n",
      "31269/31269 [==============================] - 21s 687us/step - loss: 0.1683 - acc: 0.2541 - val_loss: 0.1663 - val_acc: 0.2965\n",
      "Epoch 5/200\n",
      "31200/31269 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.2554Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.000330000015674.\n",
      "31269/31269 [==============================] - 22s 695us/step - loss: 0.1638 - acc: 0.2555 - val_loss: 0.1597 - val_acc: 0.2972\n",
      "Epoch 6/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.2600Epoch 00006: val_loss did not improve\n",
      "31269/31269 [==============================] - 21s 657us/step - loss: 0.1488 - acc: 0.2599 - val_loss: 0.1544 - val_acc: 0.2957\n",
      "Epoch 7/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.2603Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000108900003252.\n",
      "31269/31269 [==============================] - 20s 655us/step - loss: 0.1461 - acc: 0.2603 - val_loss: 0.1637 - val_acc: 0.2962\n",
      "Epoch 8/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.2636Epoch 00008: val_loss improved from 0.15389 to 0.14340, saving model to ./models/model12/model5.h5\n",
      "31269/31269 [==============================] - 20s 651us/step - loss: 0.1400 - acc: 0.2635 - val_loss: 0.1434 - val_acc: 0.3018\n",
      "Epoch 9/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.2641Epoch 00009: val_loss improved from 0.14340 to 0.14276, saving model to ./models/model12/model5.h5\n",
      "31269/31269 [==============================] - 21s 671us/step - loss: 0.1384 - acc: 0.2643 - val_loss: 0.1428 - val_acc: 0.3035\n",
      "Epoch 10/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.2643Epoch 00010: val_loss did not improve\n",
      "31269/31269 [==============================] - 21s 673us/step - loss: 0.1369 - acc: 0.2643 - val_loss: 0.1469 - val_acc: 0.3020\n",
      "Epoch 11/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.2653Epoch 00011: val_loss did not improve\n",
      "31269/31269 [==============================] - 21s 670us/step - loss: 0.1364 - acc: 0.2653 - val_loss: 0.1468 - val_acc: 0.3009\n",
      "Epoch 12/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.2658Epoch 00012: val_loss improved from 0.14276 to 0.14171, saving model to ./models/model12/model5.h5\n",
      "31269/31269 [==============================] - 21s 668us/step - loss: 0.1350 - acc: 0.2658 - val_loss: 0.1417 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.2667Epoch 00013: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 646us/step - loss: 0.1330 - acc: 0.2666 - val_loss: 0.1513 - val_acc: 0.2989\n",
      "Epoch 14/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.2675Epoch 00014: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 653us/step - loss: 0.1316 - acc: 0.2676 - val_loss: 0.1428 - val_acc: 0.3041\n",
      "Epoch 15/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.2683Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "31269/31269 [==============================] - 20s 648us/step - loss: 0.1309 - acc: 0.2683 - val_loss: 0.1431 - val_acc: 0.3029\n",
      "Epoch 16/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.2686Epoch 00016: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 653us/step - loss: 0.1290 - acc: 0.2686 - val_loss: 0.1444 - val_acc: 0.3031\n",
      "Epoch 17/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.2687Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "31269/31269 [==============================] - 20s 640us/step - loss: 0.1289 - acc: 0.2687 - val_loss: 0.1449 - val_acc: 0.3030\n",
      "Epoch 18/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.2684Epoch 00018: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 655us/step - loss: 0.1288 - acc: 0.2685 - val_loss: 0.1446 - val_acc: 0.3029\n",
      "Epoch 19/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.2693Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.91353921259e-06.\n",
      "31269/31269 [==============================] - 20s 645us/step - loss: 0.1281 - acc: 0.2693 - val_loss: 0.1460 - val_acc: 0.3030\n",
      "Epoch 20/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.2687Epoch 00020: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 636us/step - loss: 0.1289 - acc: 0.2687 - val_loss: 0.1461 - val_acc: 0.3031\n",
      "Epoch 21/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.2693Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.29146797917e-06.\n",
      "31269/31269 [==============================] - 20s 643us/step - loss: 0.1277 - acc: 0.2692 - val_loss: 0.1451 - val_acc: 0.3033\n",
      "Epoch 22/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.2690Epoch 00022: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 642us/step - loss: 0.1278 - acc: 0.2691 - val_loss: 0.1458 - val_acc: 0.3031\n",
      "Epoch 23/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.2699Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 4.26184415119e-07.\n",
      "31269/31269 [==============================] - 20s 638us/step - loss: 0.1267 - acc: 0.2700 - val_loss: 0.1454 - val_acc: 0.3033\n",
      "Epoch 24/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.2691Epoch 00024: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 640us/step - loss: 0.1283 - acc: 0.2691 - val_loss: 0.1456 - val_acc: 0.3031\n",
      "Epoch 25/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.2695Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.40640856614e-07.\n",
      "31269/31269 [==============================] - 20s 640us/step - loss: 0.1275 - acc: 0.2694 - val_loss: 0.1457 - val_acc: 0.3031\n",
      "Epoch 26/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.2691Epoch 00026: val_loss did not improve\n",
      "31269/31269 [==============================] - 20s 643us/step - loss: 0.1284 - acc: 0.2689 - val_loss: 0.1457 - val_acc: 0.3031\n",
      "Epoch 27/200\n",
      "31232/31269 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.2689Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 4.64114805254e-08.\n",
      "31269/31269 [==============================] - 20s 640us/step - loss: 0.1283 - acc: 0.2689 - val_loss: 0.1456 - val_acc: 0.3031\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9b10fd0d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model12/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=15, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-2)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, schedule_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 253us/step\n",
      "8424/8424 [==============================] - 2s 262us/step\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 208us/step\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 238us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 244us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 3s 302us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 292us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 236us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "weights = [0.2, 0.18, 0.2, 0.2, 0.22]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model12/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
