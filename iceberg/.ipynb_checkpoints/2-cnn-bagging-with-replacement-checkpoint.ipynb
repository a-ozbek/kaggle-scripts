{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add Test data predictions to Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Create Dataset](#Create-Dataset)\n",
    "* [Train - Val Split](#Train---Val-Split)\n",
    "* [Data Augmentation](#Data-Augmentation)\n",
    "* [Training](#Training)\n",
    "* [Predict Test](#Predict-Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.signal import fftconvolve\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df_train = pd.read_json('./data/train.json')\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_json('./data/test.json')\n",
    "df_test_labels = pd.read_csv('./submissions/ourbest_explorestack_2.csv')\n",
    "df_test = pd.merge(df_test, df_test_labels, on='id')\n",
    "iceberg_cond = (df_test['is_iceberg'] >= 0.9) & (df_test['is_iceberg'] <= 1.0)\n",
    "not_iceberg_cond = (df_test['is_iceberg'] >= 0.0) & (df_test['is_iceberg'] <= 0.1)\n",
    "df_test = df_test[iceberg_cond | not_iceberg_cond]\n",
    "\n",
    "# Merge Train and Test\n",
    "df = pd.concat([df_train, df_test])\n",
    "df\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    im = im - np.mean(im)\n",
    "    im = im / np.std(im)\n",
    "    return im\n",
    "\n",
    "def get_convolve(im1, im2):\n",
    "    im1 = im1 - np.mean(im1)\n",
    "    im2 = im2 - np.mean(im2)\n",
    "    im_conv = fftconvolve(im1, im2[::-1, ::-1], mode='same')\n",
    "    return normalize(im_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4927, 75, 75, 2)\n",
      "y.shape: (4927,)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "#     im = np.concatenate([normalize(im_band1), normalize(im_band2), get_convolve(im_band1, im_band2)], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_random_split(size, train_ratio):\n",
    "    indices = range(size)\n",
    "    cutoff = int(round(train_ratio * size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_i = indices[:cutoff]\n",
    "    test_i = indices[cutoff:]\n",
    "    return train_i, test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "        # Architecture\n",
    "        model = Sequential()\n",
    "        # Block 1\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                         activation='relu',\n",
    "                         input_shape=input_shape))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 2\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 3\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 4\n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(LeakyReLU())          \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        # FC\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model\n",
    "        loss = losses.binary_crossentropy\n",
    "        optimizer = optimizers.Adam()\n",
    "        metrics = ['accuracy']\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "        #     \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For - Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** MODEL: 1  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.433649795455\n",
      "np.mean(y_val): 0.426573623244\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.2600Epoch 00001: val_loss improved from inf to 0.19944, saving model to ./models/model15/model1.h5\n",
      "17248/17248 [==============================] - 14s 837us/step - loss: 0.3665 - acc: 0.2599 - val_loss: 0.1994 - val_acc: 0.2805\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.2910Epoch 00002: val_loss improved from 0.19944 to 0.17984, saving model to ./models/model15/model1.h5\n",
      "17248/17248 [==============================] - 14s 807us/step - loss: 0.2147 - acc: 0.2909 - val_loss: 0.1798 - val_acc: 0.2857\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.2975Epoch 00003: val_loss improved from 0.17984 to 0.15255, saving model to ./models/model15/model1.h5\n",
      "17248/17248 [==============================] - 14s 820us/step - loss: 0.1808 - acc: 0.2975 - val_loss: 0.1525 - val_acc: 0.2880\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.3001Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 811us/step - loss: 0.1660 - acc: 0.3000 - val_loss: 0.1669 - val_acc: 0.2900\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.3025Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 793us/step - loss: 0.1563 - acc: 0.3025 - val_loss: 0.1549 - val_acc: 0.2933\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.3069Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 13s 782us/step - loss: 0.1476 - acc: 0.3069 - val_loss: 0.1699 - val_acc: 0.2864\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.3112Epoch 00007: val_loss improved from 0.15255 to 0.13850, saving model to ./models/model15/model1.h5\n",
      "17248/17248 [==============================] - 14s 805us/step - loss: 0.1291 - acc: 0.3111 - val_loss: 0.1385 - val_acc: 0.2952\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.3134Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 808us/step - loss: 0.1246 - acc: 0.3133 - val_loss: 0.1484 - val_acc: 0.2923\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.3150Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.1216 - acc: 0.3148 - val_loss: 0.1626 - val_acc: 0.2891\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.3166Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 804us/step - loss: 0.1161 - acc: 0.3163 - val_loss: 0.1412 - val_acc: 0.2924\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.3201Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 809us/step - loss: 0.1084 - acc: 0.3202 - val_loss: 0.1423 - val_acc: 0.2945\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.3205Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 14s 815us/step - loss: 0.1077 - acc: 0.3203 - val_loss: 0.1472 - val_acc: 0.2946\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.3222Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 813us/step - loss: 0.1040 - acc: 0.3220 - val_loss: 0.1386 - val_acc: 0.2958\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.3219Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 843us/step - loss: 0.1042 - acc: 0.3218 - val_loss: 0.1426 - val_acc: 0.2951\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.3222Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.1023 - acc: 0.3225 - val_loss: 0.1418 - val_acc: 0.2948\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.3236Epoch 00016: val_loss improved from 0.13850 to 0.13665, saving model to ./models/model15/model1.h5\n",
      "17248/17248 [==============================] - 15s 848us/step - loss: 0.1010 - acc: 0.3236 - val_loss: 0.1366 - val_acc: 0.2958\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.3236Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.1016 - acc: 0.3239 - val_loss: 0.1399 - val_acc: 0.2951\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.3240Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.0996 - acc: 0.3243 - val_loss: 0.1465 - val_acc: 0.2931\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.3224Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 869us/step - loss: 0.1033 - acc: 0.3225 - val_loss: 0.1411 - val_acc: 0.2945\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.3228Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 803us/step - loss: 0.1004 - acc: 0.3229 - val_loss: 0.1423 - val_acc: 0.2944\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.3235Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 14s 789us/step - loss: 0.1000 - acc: 0.3235 - val_loss: 0.1405 - val_acc: 0.2946\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.3232Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 803us/step - loss: 0.1011 - acc: 0.3233 - val_loss: 0.1413 - val_acc: 0.2945\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.3243Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 4.26184415119e-07.\n",
      "17248/17248 [==============================] - 14s 834us/step - loss: 0.1005 - acc: 0.3242 - val_loss: 0.1422 - val_acc: 0.2942\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.3224Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 842us/step - loss: 0.1014 - acc: 0.3227 - val_loss: 0.1416 - val_acc: 0.2944\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.3233Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.40640856614e-07.\n",
      "17248/17248 [==============================] - 14s 822us/step - loss: 0.1009 - acc: 0.3235 - val_loss: 0.1412 - val_acc: 0.2945\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.3238Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.0997 - acc: 0.3239 - val_loss: 0.1414 - val_acc: 0.2945\n",
      "Epoch 00026: early stopping\n",
      "*** MODEL: 2  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.441402162399\n",
      "np.mean(y_val): 0.41881810877\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.2594Epoch 00001: val_loss improved from inf to 0.22446, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.3310 - acc: 0.2597 - val_loss: 0.2245 - val_acc: 0.2829\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.2891Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.1860 - acc: 0.2893 - val_loss: 0.2245 - val_acc: 0.2792\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.2943Epoch 00003: val_loss improved from 0.22446 to 0.17579, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1615 - acc: 0.2943 - val_loss: 0.1758 - val_acc: 0.2907\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.2986Epoch 00004: val_loss improved from 0.17579 to 0.16328, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 885us/step - loss: 0.1527 - acc: 0.2985 - val_loss: 0.1633 - val_acc: 0.2897\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.3007Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1432 - acc: 0.3010 - val_loss: 0.1642 - val_acc: 0.2944\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.3015Epoch 00006: val_loss improved from 0.16328 to 0.15256, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 876us/step - loss: 0.1391 - acc: 0.3016 - val_loss: 0.1526 - val_acc: 0.2942\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.3044Epoch 00007: val_loss improved from 0.15256 to 0.14885, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1345 - acc: 0.3044 - val_loss: 0.1489 - val_acc: 0.2967\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.3056Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1322 - acc: 0.3055 - val_loss: 0.1624 - val_acc: 0.2922\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.3095Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1241 - acc: 0.3093 - val_loss: 0.1628 - val_acc: 0.2951\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.3086Epoch 00010: val_loss improved from 0.14885 to 0.14600, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1262 - acc: 0.3088 - val_loss: 0.1460 - val_acc: 0.2997\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.3115Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.1176 - acc: 0.3115 - val_loss: 0.1513 - val_acc: 0.2972\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.3122Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1146 - acc: 0.3122 - val_loss: 0.1513 - val_acc: 0.2974\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.3131Epoch 00013: val_loss improved from 0.14600 to 0.14397, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 876us/step - loss: 0.1115 - acc: 0.3132 - val_loss: 0.1440 - val_acc: 0.2977\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.3140Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 869us/step - loss: 0.1110 - acc: 0.3141 - val_loss: 0.1545 - val_acc: 0.2989\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.3147Epoch 00015: val_loss improved from 0.14397 to 0.14325, saving model to ./models/model15/model2.h5\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1096 - acc: 0.3148 - val_loss: 0.1433 - val_acc: 0.2985\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.3170Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1030 - acc: 0.3170 - val_loss: 0.1554 - val_acc: 0.2963\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.3173Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.1046 - acc: 0.3173 - val_loss: 0.1584 - val_acc: 0.2971\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.3188Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 876us/step - loss: 0.0985 - acc: 0.3189 - val_loss: 0.1785 - val_acc: 0.2941\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.3212Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 868us/step - loss: 0.0890 - acc: 0.3215 - val_loss: 0.1547 - val_acc: 0.2989\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.3229Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 876us/step - loss: 0.0870 - acc: 0.3231 - val_loss: 0.1474 - val_acc: 0.2984\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.3241Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.0824 - acc: 0.3241 - val_loss: 0.1491 - val_acc: 0.2996\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.3256Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 855us/step - loss: 0.0814 - acc: 0.3254 - val_loss: 0.1512 - val_acc: 0.2996\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.3255Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.0804 - acc: 0.3254 - val_loss: 0.1482 - val_acc: 0.2994\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.3255Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.0798 - acc: 0.3257 - val_loss: 0.1492 - val_acc: 0.2999\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.3250Epoch 00025: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.0795 - acc: 0.3253 - val_loss: 0.1492 - val_acc: 0.2998\n",
      "Epoch 00025: early stopping\n",
      "*** MODEL: 3  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.427704227171\n",
      "np.mean(y_val): 0.432521605481\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.2530Epoch 00001: val_loss improved from inf to 0.18109, saving model to ./models/model15/model3.h5\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.3626 - acc: 0.2532 - val_loss: 0.1811 - val_acc: 0.2851\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.2893Epoch 00002: val_loss improved from 0.18109 to 0.16538, saving model to ./models/model15/model3.h5\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1962 - acc: 0.2892 - val_loss: 0.1654 - val_acc: 0.2909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.2962Epoch 00003: val_loss improved from 0.16538 to 0.16261, saving model to ./models/model15/model3.h5\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.1717 - acc: 0.2962 - val_loss: 0.1626 - val_acc: 0.2906\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.2993Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 889us/step - loss: 0.1567 - acc: 0.2993 - val_loss: 0.1760 - val_acc: 0.2883\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.3025Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 884us/step - loss: 0.1469 - acc: 0.3023 - val_loss: 0.1637 - val_acc: 0.2915\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.3033Epoch 00006: val_loss improved from 0.16261 to 0.15739, saving model to ./models/model15/model3.h5\n",
      "17248/17248 [==============================] - 14s 840us/step - loss: 0.1482 - acc: 0.3033 - val_loss: 0.1574 - val_acc: 0.2933\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.3053Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.1401 - acc: 0.3051 - val_loss: 0.1661 - val_acc: 0.2920\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.3055Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 797us/step - loss: 0.1372 - acc: 0.3053 - val_loss: 0.1577 - val_acc: 0.2919\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.3079Epoch 00009: val_loss improved from 0.15739 to 0.14278, saving model to ./models/model15/model3.h5\n",
      "17248/17248 [==============================] - 14s 813us/step - loss: 0.1298 - acc: 0.3079 - val_loss: 0.1428 - val_acc: 0.2980\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.3089Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 786us/step - loss: 0.1293 - acc: 0.3090 - val_loss: 0.1680 - val_acc: 0.2925\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.3123Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 814us/step - loss: 0.1238 - acc: 0.3122 - val_loss: 0.1550 - val_acc: 0.2935\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.3119Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 14s 798us/step - loss: 0.1213 - acc: 0.3120 - val_loss: 0.1703 - val_acc: 0.2942\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.3177Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 813us/step - loss: 0.1050 - acc: 0.3179 - val_loss: 0.1480 - val_acc: 0.2959\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.3199Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 819us/step - loss: 0.1003 - acc: 0.3201 - val_loss: 0.1431 - val_acc: 0.2969\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.3200Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 831us/step - loss: 0.0976 - acc: 0.3203 - val_loss: 0.1521 - val_acc: 0.2956\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.3218Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 14s 832us/step - loss: 0.0947 - acc: 0.3217 - val_loss: 0.1466 - val_acc: 0.2952\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3216Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 828us/step - loss: 0.0943 - acc: 0.3218 - val_loss: 0.1508 - val_acc: 0.2952\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.3210Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 825us/step - loss: 0.0947 - acc: 0.3210 - val_loss: 0.1491 - val_acc: 0.2956\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.3225Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 831us/step - loss: 0.0919 - acc: 0.3225 - val_loss: 0.1520 - val_acc: 0.2956\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 4  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.443360559862\n",
      "np.mean(y_val): 0.416858916179\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.2538Epoch 00001: val_loss improved from inf to 0.25763, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 15s 850us/step - loss: 0.3366 - acc: 0.2537 - val_loss: 0.2576 - val_acc: 0.2764\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.2834Epoch 00002: val_loss improved from 0.25763 to 0.19665, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 14s 831us/step - loss: 0.1856 - acc: 0.2832 - val_loss: 0.1966 - val_acc: 0.2909\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.2869Epoch 00003: val_loss improved from 0.19665 to 0.16430, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 15s 873us/step - loss: 0.1663 - acc: 0.2869 - val_loss: 0.1643 - val_acc: 0.2969\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.2907Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.1582 - acc: 0.2906 - val_loss: 0.1707 - val_acc: 0.2978\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.2926Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1494 - acc: 0.2925 - val_loss: 0.1949 - val_acc: 0.2991\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.2938Epoch 00006: val_loss improved from 0.16430 to 0.14138, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 15s 867us/step - loss: 0.1425 - acc: 0.2940 - val_loss: 0.1414 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.2973Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 863us/step - loss: 0.1365 - acc: 0.2977 - val_loss: 0.1525 - val_acc: 0.3006\n",
      "Epoch 8/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.2983Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1331 - acc: 0.2985 - val_loss: 0.1691 - val_acc: 0.2991\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.3004Epoch 00009: val_loss improved from 0.14138 to 0.13947, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 15s 864us/step - loss: 0.1296 - acc: 0.3004 - val_loss: 0.1395 - val_acc: 0.3065\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.2986Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1295 - acc: 0.2985 - val_loss: 0.1488 - val_acc: 0.3030\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.3025Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1242 - acc: 0.3024 - val_loss: 0.1595 - val_acc: 0.3002\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.3028Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.1171 - acc: 0.3028 - val_loss: 0.1509 - val_acc: 0.3047\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.3087Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 823us/step - loss: 0.1030 - acc: 0.3086 - val_loss: 0.1668 - val_acc: 0.3003\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.3099Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 833us/step - loss: 0.1011 - acc: 0.3096 - val_loss: 0.1480 - val_acc: 0.3039\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3111Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 789us/step - loss: 0.0961 - acc: 0.3112 - val_loss: 0.1448 - val_acc: 0.3052\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.3117Epoch 00016: val_loss improved from 0.13947 to 0.13640, saving model to ./models/model15/model4.h5\n",
      "17248/17248 [==============================] - 14s 795us/step - loss: 0.0939 - acc: 0.3117 - val_loss: 0.1364 - val_acc: 0.3086\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.3131Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 810us/step - loss: 0.0925 - acc: 0.3131 - val_loss: 0.1417 - val_acc: 0.3074\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.3134Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 829us/step - loss: 0.0913 - acc: 0.3132 - val_loss: 0.1451 - val_acc: 0.3065\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.3137Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 14s 816us/step - loss: 0.0919 - acc: 0.3135 - val_loss: 0.1425 - val_acc: 0.3076\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.3138Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 825us/step - loss: 0.0895 - acc: 0.3136 - val_loss: 0.1442 - val_acc: 0.3069\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.3140Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 814us/step - loss: 0.0895 - acc: 0.3140 - val_loss: 0.1430 - val_acc: 0.3064\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.3140Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 820us/step - loss: 0.0890 - acc: 0.3144 - val_loss: 0.1456 - val_acc: 0.3059\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.3142Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 14s 797us/step - loss: 0.0888 - acc: 0.3142 - val_loss: 0.1438 - val_acc: 0.3062\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.3141Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 832us/step - loss: 0.0893 - acc: 0.3141 - val_loss: 0.1443 - val_acc: 0.3062\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.3152Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 14s 816us/step - loss: 0.0892 - acc: 0.3149 - val_loss: 0.1454 - val_acc: 0.3057\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.3139Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 830us/step - loss: 0.0898 - acc: 0.3138 - val_loss: 0.1453 - val_acc: 0.3057\n",
      "Epoch 00026: early stopping\n",
      "*** MODEL: 5  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.435150458847\n",
      "np.mean(y_val): 0.425072350568\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.2567Epoch 00001: val_loss improved from inf to 0.17476, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 15s 846us/step - loss: 0.3303 - acc: 0.2568 - val_loss: 0.1748 - val_acc: 0.2957\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.2864Epoch 00002: val_loss improved from 0.17476 to 0.17020, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 815us/step - loss: 0.1861 - acc: 0.2863 - val_loss: 0.1702 - val_acc: 0.2986\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.2910Epoch 00003: val_loss improved from 0.17020 to 0.16227, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 818us/step - loss: 0.1657 - acc: 0.2907 - val_loss: 0.1623 - val_acc: 0.2952\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.2937Epoch 00004: val_loss improved from 0.16227 to 0.15085, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 813us/step - loss: 0.1596 - acc: 0.2937 - val_loss: 0.1509 - val_acc: 0.2980\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.2951Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 817us/step - loss: 0.1479 - acc: 0.2950 - val_loss: 0.1870 - val_acc: 0.2904\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.2982Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 821us/step - loss: 0.1436 - acc: 0.2980 - val_loss: 0.1614 - val_acc: 0.2988\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.3000Epoch 00007: val_loss improved from 0.15085 to 0.15048, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 807us/step - loss: 0.1383 - acc: 0.3002 - val_loss: 0.1505 - val_acc: 0.2996\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.3025Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 806us/step - loss: 0.1334 - acc: 0.3023 - val_loss: 0.1574 - val_acc: 0.2978\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.3025Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 789us/step - loss: 0.1313 - acc: 0.3023 - val_loss: 0.1584 - val_acc: 0.2973\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.3026Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 14s 807us/step - loss: 0.1292 - acc: 0.3026 - val_loss: 0.1664 - val_acc: 0.2947\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.3083Epoch 00011: val_loss improved from 0.15048 to 0.15039, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 797us/step - loss: 0.1127 - acc: 0.3083 - val_loss: 0.1504 - val_acc: 0.2986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.3091Epoch 00012: val_loss improved from 0.15039 to 0.13996, saving model to ./models/model15/model5.h5\n",
      "17248/17248 [==============================] - 14s 792us/step - loss: 0.1084 - acc: 0.3091 - val_loss: 0.1400 - val_acc: 0.3019\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.3115Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 789us/step - loss: 0.1054 - acc: 0.3115 - val_loss: 0.1594 - val_acc: 0.2956\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.3129Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 812us/step - loss: 0.1005 - acc: 0.3131 - val_loss: 0.1652 - val_acc: 0.2959\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.3141Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 803us/step - loss: 0.1003 - acc: 0.3139 - val_loss: 0.1692 - val_acc: 0.2959\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3156Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 801us/step - loss: 0.0946 - acc: 0.3155 - val_loss: 0.1527 - val_acc: 0.3000\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.3166Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 14s 799us/step - loss: 0.0932 - acc: 0.3166 - val_loss: 0.1566 - val_acc: 0.2985\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3154Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 788us/step - loss: 0.0925 - acc: 0.3153 - val_loss: 0.1644 - val_acc: 0.2968\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.3165Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 806us/step - loss: 0.0927 - acc: 0.3166 - val_loss: 0.1571 - val_acc: 0.2983\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.3169Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 822us/step - loss: 0.0905 - acc: 0.3170 - val_loss: 0.1613 - val_acc: 0.2977\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.3161Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 14s 816us/step - loss: 0.0918 - acc: 0.3161 - val_loss: 0.1563 - val_acc: 0.2986\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.3155Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 821us/step - loss: 0.0924 - acc: 0.3156 - val_loss: 0.1587 - val_acc: 0.2982\n",
      "Epoch 00022: early stopping\n",
      "*** MODEL: 6  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.422177246814\n",
      "np.mean(y_val): 0.438050829842\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.2664Epoch 00001: val_loss improved from inf to 0.18012, saving model to ./models/model15/model6.h5\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.3138 - acc: 0.2663 - val_loss: 0.1801 - val_acc: 0.2873\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.2928Epoch 00002: val_loss improved from 0.18012 to 0.15646, saving model to ./models/model15/model6.h5\n",
      "17248/17248 [==============================] - 14s 819us/step - loss: 0.1919 - acc: 0.2926 - val_loss: 0.1565 - val_acc: 0.2908\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.2975Epoch 00003: val_loss improved from 0.15646 to 0.14445, saving model to ./models/model15/model6.h5\n",
      "17248/17248 [==============================] - 14s 821us/step - loss: 0.1747 - acc: 0.2974 - val_loss: 0.1444 - val_acc: 0.2960\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.3004Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 803us/step - loss: 0.1609 - acc: 0.3002 - val_loss: 0.1568 - val_acc: 0.2923\n",
      "Epoch 5/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.3040Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 789us/step - loss: 0.1476 - acc: 0.3039 - val_loss: 0.1581 - val_acc: 0.2933\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.3038Epoch 00006: val_loss improved from 0.14445 to 0.13659, saving model to ./models/model15/model6.h5\n",
      "17248/17248 [==============================] - 14s 810us/step - loss: 0.1473 - acc: 0.3037 - val_loss: 0.1366 - val_acc: 0.2945\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.3074Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 797us/step - loss: 0.1385 - acc: 0.3073 - val_loss: 0.1408 - val_acc: 0.2911\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.3091Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 863us/step - loss: 0.1326 - acc: 0.3093 - val_loss: 0.1396 - val_acc: 0.2965\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.3116Epoch 00009: val_loss improved from 0.13659 to 0.13256, saving model to ./models/model15/model6.h5\n",
      "17248/17248 [==============================] - 15s 848us/step - loss: 0.1282 - acc: 0.3114 - val_loss: 0.1326 - val_acc: 0.2989\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.3120Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.1283 - acc: 0.3118 - val_loss: 0.1460 - val_acc: 0.2963\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.3117Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.1227 - acc: 0.3116 - val_loss: 0.1355 - val_acc: 0.2973\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.3148Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 878us/step - loss: 0.1189 - acc: 0.3149 - val_loss: 0.1382 - val_acc: 0.2970\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.3194Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 864us/step - loss: 0.1038 - acc: 0.3196 - val_loss: 0.1346 - val_acc: 0.2979\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.3212Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 853us/step - loss: 0.0985 - acc: 0.3214 - val_loss: 0.1493 - val_acc: 0.2954\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.3237Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.0935 - acc: 0.3239 - val_loss: 0.1494 - val_acc: 0.2956\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.3235Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.0942 - acc: 0.3238 - val_loss: 0.1367 - val_acc: 0.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.3245Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 842us/step - loss: 0.0917 - acc: 0.3244 - val_loss: 0.1385 - val_acc: 0.2980\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.3239Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.0916 - acc: 0.3239 - val_loss: 0.1394 - val_acc: 0.2983\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.3249Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.0900 - acc: 0.3250 - val_loss: 0.1388 - val_acc: 0.2980\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 7  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.434207313839\n",
      "np.mean(y_val): 0.426015878502\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.2533Epoch 00001: val_loss improved from inf to 0.18381, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 15s 878us/step - loss: 0.3290 - acc: 0.2532 - val_loss: 0.1838 - val_acc: 0.2978\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.2803Epoch 00002: val_loss improved from 0.18381 to 0.17560, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 15s 846us/step - loss: 0.1976 - acc: 0.2803 - val_loss: 0.1756 - val_acc: 0.2998\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.2876Epoch 00003: val_loss improved from 0.17560 to 0.15584, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 15s 882us/step - loss: 0.1725 - acc: 0.2878 - val_loss: 0.1558 - val_acc: 0.3048\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.2887Epoch 00004: val_loss improved from 0.15584 to 0.14897, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1605 - acc: 0.2884 - val_loss: 0.1490 - val_acc: 0.3022\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.2921Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 855us/step - loss: 0.1515 - acc: 0.2922 - val_loss: 0.1624 - val_acc: 0.3001\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.2945Epoch 00006: val_loss improved from 0.14897 to 0.14519, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 15s 870us/step - loss: 0.1425 - acc: 0.2946 - val_loss: 0.1452 - val_acc: 0.3030\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.2984Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 876us/step - loss: 0.1343 - acc: 0.2985 - val_loss: 0.1799 - val_acc: 0.2969\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.2991Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.1330 - acc: 0.2992 - val_loss: 0.1596 - val_acc: 0.3000\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.2997Epoch 00009: val_loss improved from 0.14519 to 0.14133, saving model to ./models/model15/model7.h5\n",
      "17248/17248 [==============================] - 14s 787us/step - loss: 0.1305 - acc: 0.2997 - val_loss: 0.1413 - val_acc: 0.3060\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.3010Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 809us/step - loss: 0.1233 - acc: 0.3009 - val_loss: 0.1482 - val_acc: 0.3038\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.3027Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 855us/step - loss: 0.1230 - acc: 0.3025 - val_loss: 0.1650 - val_acc: 0.3039\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1203 - acc: 0.3025Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 14s 819us/step - loss: 0.1202 - acc: 0.3026 - val_loss: 0.1462 - val_acc: 0.3046\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.3084Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 809us/step - loss: 0.1031 - acc: 0.3084 - val_loss: 0.1414 - val_acc: 0.3061\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.3104Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.1001 - acc: 0.3104 - val_loss: 0.1432 - val_acc: 0.3073\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3121Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 838us/step - loss: 0.0943 - acc: 0.3124 - val_loss: 0.1535 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.3124Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.0931 - acc: 0.3128 - val_loss: 0.1420 - val_acc: 0.3064\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.3125Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.0921 - acc: 0.3124 - val_loss: 0.1520 - val_acc: 0.3044\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.3131Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 832us/step - loss: 0.0908 - acc: 0.3131 - val_loss: 0.1495 - val_acc: 0.3047\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.3140Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.0905 - acc: 0.3141 - val_loss: 0.1431 - val_acc: 0.3065\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 8  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.444616150751\n",
      "np.mean(y_val): 0.41560281551\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.2523Epoch 00001: val_loss improved from inf to 0.22087, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.3521 - acc: 0.2524 - val_loss: 0.2209 - val_acc: 0.2812\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.2832Epoch 00002: val_loss improved from 0.22087 to 0.15396, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.1958 - acc: 0.2831 - val_loss: 0.1540 - val_acc: 0.2972\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.2915Epoch 00003: val_loss improved from 0.15396 to 0.15137, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1690 - acc: 0.2915 - val_loss: 0.1514 - val_acc: 0.3010\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.2940Epoch 00004: val_loss improved from 0.15137 to 0.14310, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 870us/step - loss: 0.1562 - acc: 0.2941 - val_loss: 0.1431 - val_acc: 0.2999\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.2962Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 871us/step - loss: 0.1512 - acc: 0.2962 - val_loss: 0.1435 - val_acc: 0.3013\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.2980Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.1451 - acc: 0.2982 - val_loss: 0.1450 - val_acc: 0.2998\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.2994Epoch 00007: val_loss improved from 0.14310 to 0.13793, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.1423 - acc: 0.2996 - val_loss: 0.1379 - val_acc: 0.3047\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.3023Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 870us/step - loss: 0.1337 - acc: 0.3023 - val_loss: 0.1443 - val_acc: 0.3012\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.3023Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 882us/step - loss: 0.1312 - acc: 0.3024 - val_loss: 0.1384 - val_acc: 0.3033\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.3044Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1311 - acc: 0.3042 - val_loss: 0.1442 - val_acc: 0.3006\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.3092Epoch 00011: val_loss improved from 0.13793 to 0.13411, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 14s 819us/step - loss: 0.1122 - acc: 0.3091 - val_loss: 0.1341 - val_acc: 0.3042\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.3099Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 795us/step - loss: 0.1095 - acc: 0.3098 - val_loss: 0.1345 - val_acc: 0.3031\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.3106Epoch 00013: val_loss improved from 0.13411 to 0.13063, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 868us/step - loss: 0.1076 - acc: 0.3105 - val_loss: 0.1306 - val_acc: 0.3046\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.3130Epoch 00014: val_loss improved from 0.13063 to 0.12994, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1037 - acc: 0.3133 - val_loss: 0.1299 - val_acc: 0.3050\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.3131Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 863us/step - loss: 0.1016 - acc: 0.3135 - val_loss: 0.1343 - val_acc: 0.3036\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.3151Epoch 00016: val_loss improved from 0.12994 to 0.12981, saving model to ./models/model15/model8.h5\n",
      "17248/17248 [==============================] - 15s 873us/step - loss: 0.0984 - acc: 0.3149 - val_loss: 0.1298 - val_acc: 0.3080\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.3148Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 878us/step - loss: 0.0996 - acc: 0.3149 - val_loss: 0.1431 - val_acc: 0.3005\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.3163Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.0949 - acc: 0.3163 - val_loss: 0.1569 - val_acc: 0.2975\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.3158Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.0957 - acc: 0.3160 - val_loss: 0.1371 - val_acc: 0.3037\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.3180Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.0912 - acc: 0.3178 - val_loss: 0.1413 - val_acc: 0.3025\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.3187Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 867us/step - loss: 0.0892 - acc: 0.3188 - val_loss: 0.1396 - val_acc: 0.3029\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.3187Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 846us/step - loss: 0.0889 - acc: 0.3186 - val_loss: 0.1414 - val_acc: 0.3026\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.3185Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 858us/step - loss: 0.0884 - acc: 0.3184 - val_loss: 0.1387 - val_acc: 0.3033\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.3189Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 869us/step - loss: 0.0879 - acc: 0.3190 - val_loss: 0.1378 - val_acc: 0.3035\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.3192Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.0877 - acc: 0.3193 - val_loss: 0.1397 - val_acc: 0.3032\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.3194Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 853us/step - loss: 0.0864 - acc: 0.3194 - val_loss: 0.1383 - val_acc: 0.3038\n",
      "Epoch 00026: early stopping\n",
      "*** MODEL: 9  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.437456994338\n",
      "np.mean(y_val): 0.422764878603\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.2562Epoch 00001: val_loss improved from inf to 0.19520, saving model to ./models/model15/model9.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.3350 - acc: 0.2562 - val_loss: 0.1952 - val_acc: 0.2916\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.2860Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 800us/step - loss: 0.2060 - acc: 0.2857 - val_loss: 0.3050 - val_acc: 0.2663\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.2927Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 800us/step - loss: 0.1787 - acc: 0.2927 - val_loss: 0.2438 - val_acc: 0.2832\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.2934Epoch 00004: val_loss improved from 0.19520 to 0.14794, saving model to ./models/model15/model9.h5\n",
      "17248/17248 [==============================] - 14s 815us/step - loss: 0.1735 - acc: 0.2935 - val_loss: 0.1479 - val_acc: 0.2991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.2962Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 830us/step - loss: 0.1667 - acc: 0.2960 - val_loss: 0.1503 - val_acc: 0.2980\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.2991Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 830us/step - loss: 0.1561 - acc: 0.2992 - val_loss: 0.1521 - val_acc: 0.2983\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.3000Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 845us/step - loss: 0.1471 - acc: 0.2998 - val_loss: 0.1516 - val_acc: 0.2990\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.3045Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 802us/step - loss: 0.1302 - acc: 0.3048 - val_loss: 0.1684 - val_acc: 0.2935\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.3061Epoch 00009: val_loss improved from 0.14794 to 0.13647, saving model to ./models/model15/model9.h5\n",
      "17248/17248 [==============================] - 14s 819us/step - loss: 0.1270 - acc: 0.3061 - val_loss: 0.1365 - val_acc: 0.3006\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.3069Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 799us/step - loss: 0.1234 - acc: 0.3071 - val_loss: 0.1639 - val_acc: 0.2953\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.3086Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 794us/step - loss: 0.1217 - acc: 0.3084 - val_loss: 0.1825 - val_acc: 0.2898\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.3091Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 798us/step - loss: 0.1177 - acc: 0.3093 - val_loss: 0.1439 - val_acc: 0.3018\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.3126Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 824us/step - loss: 0.1098 - acc: 0.3127 - val_loss: 0.1462 - val_acc: 0.2999\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.3143Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.1049 - acc: 0.3143 - val_loss: 0.1569 - val_acc: 0.2968\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.3153Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 837us/step - loss: 0.1052 - acc: 0.3151 - val_loss: 0.1435 - val_acc: 0.2999\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.3151Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 870us/step - loss: 0.1053 - acc: 0.3150 - val_loss: 0.1480 - val_acc: 0.2994\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.3148Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1047 - acc: 0.3147 - val_loss: 0.1453 - val_acc: 0.2997\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.3148Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1026 - acc: 0.3145 - val_loss: 0.1460 - val_acc: 0.2995\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.3154Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1032 - acc: 0.3156 - val_loss: 0.1444 - val_acc: 0.2996\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 10  ***\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.426047848498\n",
      "np.mean(y_val): 0.434178656659\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.2688Epoch 00001: val_loss improved from inf to 0.20988, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.3290 - acc: 0.2687 - val_loss: 0.2099 - val_acc: 0.2739\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.2946Epoch 00002: val_loss improved from 0.20988 to 0.16129, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 15s 880us/step - loss: 0.1936 - acc: 0.2947 - val_loss: 0.1613 - val_acc: 0.2868\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.3004Epoch 00003: val_loss improved from 0.16129 to 0.15849, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 15s 882us/step - loss: 0.1742 - acc: 0.3005 - val_loss: 0.1585 - val_acc: 0.2893\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.3026Epoch 00004: val_loss improved from 0.15849 to 0.14179, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.1615 - acc: 0.3028 - val_loss: 0.1418 - val_acc: 0.2897\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.3079Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.1527 - acc: 0.3078 - val_loss: 0.1720 - val_acc: 0.2854\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.3086Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 869us/step - loss: 0.1524 - acc: 0.3087 - val_loss: 0.1504 - val_acc: 0.2883\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.3112Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1432 - acc: 0.3112 - val_loss: 0.1440 - val_acc: 0.2906\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.3185Epoch 00008: val_loss improved from 0.14179 to 0.14152, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.1220 - acc: 0.3184 - val_loss: 0.1415 - val_acc: 0.2894\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.3207Epoch 00009: val_loss improved from 0.14152 to 0.13067, saving model to ./models/model15/model10.h5\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1162 - acc: 0.3204 - val_loss: 0.1307 - val_acc: 0.2925\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.3217Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 871us/step - loss: 0.1133 - acc: 0.3218 - val_loss: 0.1321 - val_acc: 0.2931\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.3223Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.1120 - acc: 0.3224 - val_loss: 0.1368 - val_acc: 0.2909\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.3241Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.1082 - acc: 0.3240 - val_loss: 0.1445 - val_acc: 0.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.3253Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1024 - acc: 0.3253 - val_loss: 0.1370 - val_acc: 0.2916\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.3266Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.0996 - acc: 0.3269 - val_loss: 0.1364 - val_acc: 0.2923\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.3277Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.0975 - acc: 0.3279 - val_loss: 0.1351 - val_acc: 0.2923\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.3285Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.0965 - acc: 0.3284 - val_loss: 0.1368 - val_acc: 0.2913\n",
      "Epoch 17/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.3287Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 865us/step - loss: 0.0952 - acc: 0.3290 - val_loss: 0.1386 - val_acc: 0.2910\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.3296Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 848us/step - loss: 0.0938 - acc: 0.3297 - val_loss: 0.1390 - val_acc: 0.2912\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3298Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 873us/step - loss: 0.0944 - acc: 0.3298 - val_loss: 0.1388 - val_acc: 0.2909\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_MODELS = 30\n",
    "train_ratio = 0.5\n",
    "MODEL_NUMBERS = range(1, N_MODELS + 1)\n",
    "# For - Loop\n",
    "for MODEL_NUMBER in MODEL_NUMBERS:\n",
    "    print '*** MODEL:', MODEL_NUMBER, ' ***'\n",
    "    # *** Train - Val Split ***\n",
    "    train_i, val_i = get_random_split(len(X), train_ratio)\n",
    "\n",
    "    #\n",
    "    X_train, y_train = X[train_i], y[train_i]\n",
    "    X_val, y_val = X[val_i], y[val_i]\n",
    "    print 'train_i[:5]', train_i[:5]\n",
    "    print 'val_i[:5]', val_i[:5]\n",
    "    print 'X_train.shape:', X_train.shape\n",
    "    print 'y_train.shape:', y_train.shape\n",
    "    print 'X_val.shape:', X_val.shape\n",
    "    print 'y_val.shape:', y_val.shape\n",
    "    print 'np.mean(y_train):', np.mean(y_train)\n",
    "    print 'np.mean(y_val):', np.mean(y_val)\n",
    "    \n",
    "    # *** Data Augmentation ***\n",
    "    # Train\n",
    "    X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "    y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "    # Validation\n",
    "    X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "    y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "    # \n",
    "    print 'X_train.shape:', X_train.shape\n",
    "    print 'y_train.shape:', y_train.shape\n",
    "    print 'X_val.shape:', X_val.shape\n",
    "    print 'y_val.shape:', y_val.shape\n",
    "    \n",
    "    # *** Training ***\n",
    "    model = get_model(input_shape=(75, 75, 2))\n",
    "    # Callbacks\n",
    "    def get_lr(epoch):\n",
    "        lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "        lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "        print 'lr:', lr\n",
    "        return lr\n",
    "    MODEL_PATH = './models/model16/model' + str(MODEL_NUMBER) + '.h5'\n",
    "    m_q = 'val_loss'\n",
    "    model_path = MODEL_PATH\n",
    "    check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "    early_stop = callbacks.EarlyStopping(patience=10, monitor=m_q, verbose=1)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "    schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "    callback_list = [check_pt, early_stop, reduce_lr]\n",
    "    # fit\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 2s 274us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "8424/8424 [==============================] - 2s 220us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "8424/8424 [==============================] - 2s 261us/step\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 226us/step\n",
      "8424/8424 [==============================] - 2s 184us/step\n",
      "8424/8424 [==============================] - 2s 181us/step\n",
      "8424/8424 [==============================] - 2s 181us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 208us/step\n",
      "8424/8424 [==============================] - 2s 185us/step\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8424/8424 [==============================] - 2s 187us/step\n",
      "8424/8424 [==============================] - 2s 198us/step\n",
      "8424/8424 [==============================] - 2s 211us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 208us/step\n",
      "8424/8424 [==============================] - 2s 181us/step\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 244us/step\n",
      "8424/8424 [==============================] - 2s 187us/step\n",
      "5\n",
      "8424/8424 [==============================] - 2s 218us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 184us/step\n",
      "8424/8424 [==============================] - 2s 205us/step\n",
      "6\n",
      "8424/8424 [==============================] - 2s 218us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 189us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 201us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "7\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8424/8424 [==============================] - 2s 261us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "8424/8424 [==============================] - 2s 191us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8\n",
      "8424/8424 [==============================] - 2s 236us/step\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "9\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 262us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 258us/step\n",
      "8424/8424 [==============================] - 2s 212us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "# weights = [0.2, 0.18, 0.2, 0.2, 0.22]\n",
    "weights = [0.1] * N_MODELS\n",
    "for i, w in zip(range(N_MODELS), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model16/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub33.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
