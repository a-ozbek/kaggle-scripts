{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, no data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 1\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1282, 75, 75, 2)\n",
      "y_train.shape: (1282,)\n",
      "X_val.shape: (322, 75, 75, 2)\n",
      "y_val.shape: (322,)\n",
      "np.mean(y_train): 0.469578783151\n",
      "np.mean(y_val): 0.468944099379\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1282, 75, 75, 2)\n",
      "y_train.shape: (1282,)\n",
      "X_val.shape: (322, 75, 75, 2)\n",
      "y_val.shape: (322,)\n"
     ]
    }
   ],
   "source": [
    "# # Train\n",
    "# X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "# y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# # Validation\n",
    "# X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "# y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# # \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.33))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(LeakyReLU())          \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # FC\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.SGD()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1282 samples, validate on 322 samples\n",
      "lr: 0.0244853858868\n",
      "Epoch 1/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.5555Epoch 00001: val_loss improved from inf to 0.65405, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 1ms/step - loss: 0.7132 - acc: 0.5562 - val_loss: 0.6540 - val_acc: 0.5342\n",
      "lr: 0.025\n",
      "Epoch 2/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.6524 - acc: 0.6039Epoch 00002: val_loss improved from 0.65405 to 0.63722, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 719us/step - loss: 0.6521 - acc: 0.6045 - val_loss: 0.6372 - val_acc: 0.5311\n",
      "lr: 0.025\n",
      "Epoch 3/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.6438Epoch 00003: val_loss improved from 0.63722 to 0.62321, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 733us/step - loss: 0.6166 - acc: 0.6443 - val_loss: 0.6232 - val_acc: 0.5248\n",
      "lr: 0.025\n",
      "Epoch 4/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.6625Epoch 00004: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 749us/step - loss: 0.5936 - acc: 0.6622 - val_loss: 0.6515 - val_acc: 0.7112\n",
      "lr: 0.00443804085621\n",
      "Epoch 5/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.6651Epoch 00005: val_loss improved from 0.62321 to 0.61526, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 755us/step - loss: 0.5799 - acc: 0.6685 - val_loss: 0.6153 - val_acc: 0.8168\n",
      "lr: 0.0194013753029\n",
      "Epoch 6/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.5601 - acc: 0.6828Epoch 00006: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 754us/step - loss: 0.5607 - acc: 0.6817 - val_loss: 0.7816 - val_acc: 0.5311\n",
      "lr: 0.025\n",
      "Epoch 7/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.7035Epoch 00007: val_loss improved from 0.61526 to 0.54538, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 819us/step - loss: 0.5777 - acc: 0.7020 - val_loss: 0.5454 - val_acc: 0.8292\n",
      "lr: 0.00177595756003\n",
      "Epoch 8/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.7102Epoch 00008: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 650us/step - loss: 0.5240 - acc: 0.7106 - val_loss: 0.5561 - val_acc: 0.7981\n",
      "lr: 0.0232440411498\n",
      "Epoch 9/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.7460Epoch 00009: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 734us/step - loss: 0.5096 - acc: 0.7480 - val_loss: 0.5529 - val_acc: 0.7329\n",
      "lr: 0.0139823670924\n",
      "Epoch 10/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.7836Epoch 00010: val_loss improved from 0.54538 to 0.50962, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 735us/step - loss: 0.4585 - acc: 0.7839 - val_loss: 0.5096 - val_acc: 0.7671\n",
      "lr: 0.00335207606037\n",
      "Epoch 11/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.7852Epoch 00011: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 672us/step - loss: 0.4401 - acc: 0.7855 - val_loss: 0.5118 - val_acc: 0.7547\n",
      "lr: 0.025\n",
      "Epoch 12/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.7742Epoch 00012: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 760us/step - loss: 0.4577 - acc: 0.7746 - val_loss: 0.5676 - val_acc: 0.6770\n",
      "lr: 0.00867554203113\n",
      "Epoch 13/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8005Epoch 00013: val_loss improved from 0.50962 to 0.43624, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 765us/step - loss: 0.4234 - acc: 0.7988 - val_loss: 0.4362 - val_acc: 0.8261\n",
      "lr: 0.0231889798457\n",
      "Epoch 14/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.7773Epoch 00014: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 702us/step - loss: 0.4486 - acc: 0.7777 - val_loss: 0.7521 - val_acc: 0.5248\n",
      "lr: 0.0147672552212\n",
      "Epoch 15/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8047Epoch 00015: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 779us/step - loss: 0.4022 - acc: 0.8050 - val_loss: 0.4664 - val_acc: 0.7950\n",
      "lr: 0.025\n",
      "Epoch 16/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.7937Epoch 00016: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 748us/step - loss: 0.4203 - acc: 0.7941 - val_loss: 0.4676 - val_acc: 0.8137\n",
      "lr: 0.0056231330413\n",
      "Epoch 17/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8250Epoch 00017: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 743us/step - loss: 0.3812 - acc: 0.8253 - val_loss: 0.4536 - val_acc: 0.8012\n",
      "lr: 0.0092648607171\n",
      "Epoch 18/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8289Epoch 00018: val_loss improved from 0.43624 to 0.37755, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 761us/step - loss: 0.3675 - acc: 0.8292 - val_loss: 0.3776 - val_acc: 0.8447\n",
      "lr: 0.025\n",
      "Epoch 19/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.8133Epoch 00019: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 695us/step - loss: 0.4191 - acc: 0.8136 - val_loss: 0.5769 - val_acc: 0.6646\n",
      "lr: 0.00877584074921\n",
      "Epoch 20/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8328Epoch 00020: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 683us/step - loss: 0.3663 - acc: 0.8323 - val_loss: 0.4527 - val_acc: 0.8012\n",
      "lr: 0.025\n",
      "Epoch 21/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8313Epoch 00021: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 809us/step - loss: 0.3714 - acc: 0.8307 - val_loss: 0.3899 - val_acc: 0.8199\n",
      "lr: 0.0137969162982\n",
      "Epoch 22/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8320Epoch 00022: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 699us/step - loss: 0.3401 - acc: 0.8323 - val_loss: 0.4341 - val_acc: 0.8106\n",
      "lr: 0.00482041362053\n",
      "Epoch 23/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8539Epoch 00023: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 809us/step - loss: 0.3328 - acc: 0.8541 - val_loss: 0.4409 - val_acc: 0.7981\n",
      "lr: 0.025\n",
      "Epoch 24/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8352Epoch 00024: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 777us/step - loss: 0.3674 - acc: 0.8346 - val_loss: 0.7664 - val_acc: 0.6304\n",
      "lr: 0.025\n",
      "Epoch 25/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.3971 - acc: 0.8207Epoch 00025: val_loss improved from 0.37755 to 0.36467, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 875us/step - loss: 0.3884 - acc: 0.8261 - val_loss: 0.3647 - val_acc: 0.8137\n",
      "lr: 0.000807701384504\n",
      "Epoch 26/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8109Epoch 00026: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 692us/step - loss: 0.4163 - acc: 0.8105 - val_loss: 0.4314 - val_acc: 0.8137\n",
      "lr: 0.0178745658824\n",
      "Epoch 27/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8582Epoch 00027: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 893us/step - loss: 0.3366 - acc: 0.8549 - val_loss: 0.8263 - val_acc: 0.5714\n",
      "lr: 0.025\n",
      "Epoch 28/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8523Epoch 00028: val_loss improved from 0.36467 to 0.36015, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 751us/step - loss: 0.3294 - acc: 0.8526 - val_loss: 0.3602 - val_acc: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.00376659631732\n",
      "Epoch 29/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8492Epoch 00029: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 680us/step - loss: 0.3363 - acc: 0.8495 - val_loss: 0.3942 - val_acc: 0.8199\n",
      "lr: 0.0239774846194\n",
      "Epoch 30/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8539- ETA: 0s - loss: 0.3298 - aEpoch 00030: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 694us/step - loss: 0.3267 - acc: 0.8534 - val_loss: 0.5707 - val_acc: 0.7143\n",
      "lr: 0.025\n",
      "Epoch 31/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8461Epoch 00031: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 729us/step - loss: 0.3371 - acc: 0.8456 - val_loss: 0.4005 - val_acc: 0.8199\n",
      "lr: 0.0246769788633\n",
      "Epoch 32/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.8602Epoch 00032: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 693us/step - loss: 0.3041 - acc: 0.8604 - val_loss: 0.4235 - val_acc: 0.8043\n",
      "lr: 0.00205869931629\n",
      "Epoch 33/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.2731 - acc: 0.8857Epoch 00033: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 749us/step - loss: 0.2730 - acc: 0.8853 - val_loss: 0.3811 - val_acc: 0.8230\n",
      "lr: 0.025\n",
      "Epoch 34/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.8805Epoch 00034: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 723us/step - loss: 0.2803 - acc: 0.8807 - val_loss: 0.5178 - val_acc: 0.7516\n",
      "lr: 0.0236331372681\n",
      "Epoch 35/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8539Epoch 00035: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 697us/step - loss: 0.3209 - acc: 0.8541 - val_loss: 0.4016 - val_acc: 0.8106\n",
      "lr: 0.025\n",
      "Epoch 36/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.8742Epoch 00036: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 794us/step - loss: 0.3080 - acc: 0.8736 - val_loss: 2.7070 - val_acc: 0.5311\n",
      "lr: 0.00389068332302\n",
      "Epoch 37/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.8094Epoch 00037: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 749us/step - loss: 0.5638 - acc: 0.8097 - val_loss: 0.4178 - val_acc: 0.8075\n",
      "lr: 0.0245359707527\n",
      "Epoch 38/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.8742Epoch 00038: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 732us/step - loss: 0.3060 - acc: 0.8752 - val_loss: 0.4615 - val_acc: 0.7857\n",
      "lr: 0.0196693217334\n",
      "Epoch 39/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.8805Epoch 00039: val_loss improved from 0.36015 to 0.28869, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 686us/step - loss: 0.2830 - acc: 0.8807 - val_loss: 0.2887 - val_acc: 0.8696\n",
      "lr: 0.025\n",
      "Epoch 40/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.8852Epoch 00040: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 698us/step - loss: 0.2684 - acc: 0.8838 - val_loss: 3.0644 - val_acc: 0.5311\n",
      "lr: 0.025\n",
      "Epoch 41/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8750Epoch 00041: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 757us/step - loss: 0.4037 - acc: 0.8752 - val_loss: 0.3244 - val_acc: 0.8540\n",
      "lr: 0.0244400091734\n",
      "Epoch 42/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.8898Epoch 00042: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 751us/step - loss: 0.2654 - acc: 0.8892 - val_loss: 7.0263 - val_acc: 0.5280\n",
      "lr: 0.025\n",
      "Epoch 43/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.8416 - acc: 0.7852Epoch 00043: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 713us/step - loss: 0.8411 - acc: 0.7847 - val_loss: 0.4143 - val_acc: 0.7826\n",
      "lr: 0.00172642369073\n",
      "Epoch 44/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8398Epoch 00044: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 723us/step - loss: 0.3541 - acc: 0.8401 - val_loss: 0.3866 - val_acc: 0.8261\n",
      "lr: 0.025\n",
      "Epoch 45/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.3195 - acc: 0.8586Epoch 00045: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 732us/step - loss: 0.3174 - acc: 0.8596 - val_loss: 0.4211 - val_acc: 0.7888\n",
      "lr: 0.0196848575368\n",
      "Epoch 46/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.8694Epoch 00046: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 698us/step - loss: 0.2872 - acc: 0.8729 - val_loss: 0.5938 - val_acc: 0.6957\n",
      "lr: 0.00390501862403\n",
      "Epoch 47/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.8852Epoch 00047: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 724us/step - loss: 0.2578 - acc: 0.8853 - val_loss: 0.3370 - val_acc: 0.8416\n",
      "lr: 0.0146598695364\n",
      "Epoch 48/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.2483 - acc: 0.8964Epoch 00048: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 756us/step - loss: 0.2518 - acc: 0.8955 - val_loss: 0.3017 - val_acc: 0.8696\n",
      "lr: 0.025\n",
      "Epoch 49/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.8859Epoch 00049: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 744us/step - loss: 0.2510 - acc: 0.8861 - val_loss: 0.4209 - val_acc: 0.8075\n",
      "lr: 0.0152021242021\n",
      "Epoch 50/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9055Epoch 00050: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 723us/step - loss: 0.2359 - acc: 0.9056 - val_loss: 0.3299 - val_acc: 0.8571\n",
      "lr: 0.0142914303055\n",
      "Epoch 51/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.8997Epoch 00051: val_loss improved from 0.28869 to 0.26211, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 763us/step - loss: 0.2341 - acc: 0.9009 - val_loss: 0.2621 - val_acc: 0.8851\n",
      "lr: 0.00462439819496\n",
      "Epoch 52/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9031Epoch 00052: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 774us/step - loss: 0.2243 - acc: 0.9033 - val_loss: 0.3563 - val_acc: 0.8385\n",
      "lr: 0.0212222114826\n",
      "Epoch 53/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9031Epoch 00053: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 701us/step - loss: 0.2256 - acc: 0.9033 - val_loss: 0.3041 - val_acc: 0.8696\n",
      "lr: 0.0230635431741\n",
      "Epoch 54/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9109Epoch 00054: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 710us/step - loss: 0.2267 - acc: 0.9111 - val_loss: 0.5762 - val_acc: 0.7578\n",
      "lr: 0.025\n",
      "Epoch 55/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9055Epoch 00055: val_loss improved from 0.26211 to 0.25583, saving model to ./models/model10/model1.h5\n",
      "1282/1282 [==============================] - 1s 745us/step - loss: 0.2326 - acc: 0.9056 - val_loss: 0.2558 - val_acc: 0.8913\n",
      "lr: 0.00448461454212\n",
      "Epoch 56/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.8977Epoch 00056: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 693us/step - loss: 0.2130 - acc: 0.8978 - val_loss: 0.3111 - val_acc: 0.8696\n",
      "lr: 0.025\n",
      "Epoch 57/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.8984Epoch 00057: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 751us/step - loss: 0.2202 - acc: 0.8986 - val_loss: 0.2648 - val_acc: 0.8882\n",
      "lr: 0.0147107508274\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9219Epoch 00058: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 794us/step - loss: 0.1959 - acc: 0.9220 - val_loss: 0.3340 - val_acc: 0.8478\n",
      "lr: 0.0245896308004\n",
      "Epoch 59/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.1999 - acc: 0.9104Epoch 00059: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 779us/step - loss: 0.2069 - acc: 0.9064 - val_loss: 0.3243 - val_acc: 0.8571\n",
      "lr: 0.0176581293416\n",
      "Epoch 60/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9207Epoch 00060: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 770us/step - loss: 0.1886 - acc: 0.9204 - val_loss: 0.7040 - val_acc: 0.6894\n",
      "lr: 0.0209315534808\n",
      "Epoch 61/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9156Epoch 00061: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 710us/step - loss: 0.1989 - acc: 0.9158 - val_loss: 0.3513 - val_acc: 0.8354\n",
      "lr: 0.025\n",
      "Epoch 62/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9135Epoch 00062: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 730us/step - loss: 0.2090 - acc: 0.9126 - val_loss: 0.2852 - val_acc: 0.8758\n",
      "lr: 0.025\n",
      "Epoch 63/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9148Epoch 00063: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 713us/step - loss: 0.1998 - acc: 0.9150 - val_loss: 0.2908 - val_acc: 0.8851\n",
      "lr: 0.0123166251702\n",
      "Epoch 64/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9167- ETA: 0s - loss: 0.2009 - acc: 0.9Epoch 00064: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 757us/step - loss: 0.1852 - acc: 0.9173 - val_loss: 0.3666 - val_acc: 0.8416\n",
      "lr: 0.0227419145103\n",
      "Epoch 65/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9279Epoch 00065: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 697us/step - loss: 0.1827 - acc: 0.9275 - val_loss: 0.2636 - val_acc: 0.8820\n",
      "lr: 0.025\n",
      "Epoch 66/200\n",
      "1248/1282 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9183Epoch 00066: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 749us/step - loss: 0.1885 - acc: 0.9173 - val_loss: 0.3355 - val_acc: 0.8665\n",
      "lr: 0.00728312986265\n",
      "Epoch 67/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9156Epoch 00067: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 794us/step - loss: 0.2052 - acc: 0.9158 - val_loss: 0.3012 - val_acc: 0.8665\n",
      "lr: 0.0236316573285\n",
      "Epoch 68/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9148Epoch 00068: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 745us/step - loss: 0.1956 - acc: 0.9150 - val_loss: 0.3113 - val_acc: 0.8634\n",
      "lr: 0.00022038462483\n",
      "Epoch 69/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9297Epoch 00069: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 771us/step - loss: 0.1665 - acc: 0.9298 - val_loss: 0.3096 - val_acc: 0.8602\n",
      "lr: 0.0162004634331\n",
      "Epoch 70/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9242Epoch 00070: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 758us/step - loss: 0.1714 - acc: 0.9243 - val_loss: 0.4267 - val_acc: 0.8323\n",
      "lr: 0.025\n",
      "Epoch 71/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9227Epoch 00071: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 669us/step - loss: 0.1786 - acc: 0.9220 - val_loss: 1.8100 - val_acc: 0.5621\n",
      "lr: 0.022094467159\n",
      "Epoch 72/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9187Epoch 00072: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 780us/step - loss: 0.2310 - acc: 0.9181 - val_loss: 0.6030 - val_acc: 0.7919\n",
      "lr: 0.00713311403587\n",
      "Epoch 73/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9211Epoch 00073: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 778us/step - loss: 0.1841 - acc: 0.9212 - val_loss: 0.2928 - val_acc: 0.8820\n",
      "lr: 0.00769115167793\n",
      "Epoch 74/200\n",
      "1280/1282 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9453Epoch 00074: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 705us/step - loss: 0.1509 - acc: 0.9454 - val_loss: 0.3046 - val_acc: 0.8820\n",
      "lr: 0.0114152787045\n",
      "Epoch 75/200\n",
      "1216/1282 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9375Epoch 00075: val_loss did not improve\n",
      "1282/1282 [==============================] - 1s 788us/step - loss: 0.1581 - acc: 0.9384 - val_loss: 0.3759 - val_acc: 0.8416\n",
      "Epoch 00075: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce79adaa10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model10/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=20, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, schedule_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 2s 255us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  4.32659319e-04],\n",
       "       [  8.25419486e-01],\n",
       "       [  9.69080150e-01],\n",
       "       ..., \n",
       "       [  9.57645752e-05],\n",
       "       [  9.99154449e-01],\n",
       "       [  9.99993563e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict without TTA\n",
    "y_test_p = model.predict(X_test, verbose=1)\n",
    "y_test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 262us/step\n",
      "8424/8424 [==============================] - 3s 350us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 3s 342us/step\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "8424/8424 [==============================] - 3s 337us/step\n",
      "8424/8424 [==============================] - 2s 271us/step\n",
      "8424/8424 [==============================] - 3s 321us/step\n",
      "8424/8424 [==============================] - 2s 257us/step\n",
      "8424/8424 [==============================] - 2s 258us/step\n",
      "8424/8424 [==============================] - 2s 256us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "8424/8424 [==============================] - 3s 340us/step\n",
      "8424/8424 [==============================] - 2s 268us/step\n",
      "8424/8424 [==============================] - 3s 336us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "8424/8424 [==============================] - 2s 256us/step\n",
      "8424/8424 [==============================] - 2s 258us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 276us/step\n",
      "8424/8424 [==============================] - 3s 338us/step\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "8424/8424 [==============================] - 3s 310us/step\n",
      "8424/8424 [==============================] - 2s 260us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 277us/step\n",
      "8424/8424 [==============================] - 3s 342us/step\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "8424/8424 [==============================] - 3s 328us/step\n",
      "8424/8424 [==============================] - 2s 261us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "8424/8424 [==============================] - 2s 258us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model10/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub21.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
