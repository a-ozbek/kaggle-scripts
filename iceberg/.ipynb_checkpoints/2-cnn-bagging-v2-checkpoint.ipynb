{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)\n",
    "\n",
    "Ideas2:\n",
    "* Add more aug (h_flip + rot90, etc.)\n",
    "* Add more layers to CNN\n",
    "* Different CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras import losses, optimizers, callbacks\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1284, 75, 75, 2)\n",
      "y_train.shape: (1284,)\n",
      "X_val.shape: (320, 75, 75, 2)\n",
      "y_val.shape: (320,)\n",
      "np.mean(y_train): 0.469626168224\n",
      "np.mean(y_val): 0.46875\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(aug_funcs): 25\n"
     ]
    }
   ],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "# Flips\n",
    "def hflip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def vflip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hvflip(x):\n",
    "    return hflip(vflip(x))\n",
    "\n",
    "# Rotations\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "# Combinations\n",
    "def merge_f(t):\n",
    "    \"\"\"\n",
    "    Combines functions in the tuple\n",
    "    t: (tuple)\n",
    "    \"\"\"\n",
    "    f1, f2 = t\n",
    "    return lambda x: f1(f2(x))\n",
    "\n",
    "# Flips, Rotations\n",
    "flips = [hflip, vflip, hvflip]\n",
    "rots = [rot90, rot180, rot270]\n",
    "# Combine\n",
    "flips_and_rots = [merge_f(t) for t in list(product(flips, rots))]\n",
    "rots_and_flips = [merge_f(t) for t in list(product(rots, flips))]\n",
    "\n",
    "\n",
    "aug_funcs = [bypass] + flips + rots + flips_and_rots + rots_and_flips\n",
    "print 'len(aug_funcs):', len(aug_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (32100, 75, 75, 2)\n",
      "y_train.shape: (32100,)\n",
      "X_val.shape: (8000, 75, 75, 2)\n",
      "y_val.shape: (8000,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.33))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))   \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Adam()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32100 samples, validate on 8000 samples\n",
      "Epoch 1/200\n",
      "32064/32100 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8331Epoch 00001: val_loss improved from inf to 0.24627, saving model to ./models/model7/model5.h5\n",
      "32100/32100 [==============================] - 20s 624us/step - loss: 0.3528 - acc: 0.8332 - val_loss: 0.2463 - val_acc: 0.8928\n",
      "Epoch 2/200\n",
      "32032/32100 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9137Epoch 00002: val_loss improved from 0.24627 to 0.20999, saving model to ./models/model7/model5.h5\n",
      "32100/32100 [==============================] - 19s 593us/step - loss: 0.2068 - acc: 0.9138 - val_loss: 0.2100 - val_acc: 0.9155\n",
      "Epoch 3/200\n",
      "32096/32100 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9255Epoch 00003: val_loss did not improve\n",
      "32100/32100 [==============================] - 19s 591us/step - loss: 0.1859 - acc: 0.9255 - val_loss: 0.2249 - val_acc: 0.9134\n",
      "Epoch 4/200\n",
      "32032/32100 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9430Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.000500000023749.\n",
      "32100/32100 [==============================] - 19s 600us/step - loss: 0.1414 - acc: 0.9431 - val_loss: 0.2499 - val_acc: 0.9107\n",
      "Epoch 5/200\n",
      "32032/32100 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9625Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.000250000011874.\n",
      "32100/32100 [==============================] - 19s 603us/step - loss: 0.0938 - acc: 0.9625 - val_loss: 0.2789 - val_acc: 0.9166\n",
      "Epoch 6/200\n",
      "32032/32100 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9761Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000125000005937.\n",
      "32100/32100 [==============================] - 19s 604us/step - loss: 0.0615 - acc: 0.9760 - val_loss: 0.2904 - val_acc: 0.9121\n",
      "Epoch 7/200\n",
      "32032/32100 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9812Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 6.25000029686e-05.\n",
      "32100/32100 [==============================] - 19s 593us/step - loss: 0.0481 - acc: 0.9812 - val_loss: 0.3058 - val_acc: 0.9181\n",
      "Epoch 8/200\n",
      "32064/32100 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9851Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 3.12500014843e-05.\n",
      "32100/32100 [==============================] - 19s 597us/step - loss: 0.0397 - acc: 0.9851 - val_loss: 0.3410 - val_acc: 0.9179\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99309797d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 1e-3 + 1e-7)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model7/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=6, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 257us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "8424/8424 [==============================] - 2s 199us/step\n",
      "8424/8424 [==============================] - 2s 212us/step\n",
      "8424/8424 [==============================] - 2s 192us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 253us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 206us/step\n",
      "8424/8424 [==============================] - 2s 207us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 1s 171us/step\n",
      "8424/8424 [==============================] - 2s 220us/step\n",
      "8424/8424 [==============================] - 2s 205us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 198us/step\n",
      "8424/8424 [==============================] - 2s 194us/step\n",
      "8424/8424 [==============================] - 2s 194us/step\n",
      "8424/8424 [==============================] - 2s 196us/step\n",
      "8424/8424 [==============================] - 2s 197us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 206us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 198us/step\n",
      "8424/8424 [==============================] - 2s 236us/step\n",
      "8424/8424 [==============================] - 2s 192us/step\n",
      "8424/8424 [==============================] - 2s 194us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n",
      "8424/8424 [==============================] - 2s 185us/step\n",
      "8424/8424 [==============================] - 2s 184us/step\n",
      "8424/8424 [==============================] - 2s 191us/step\n",
      "8424/8424 [==============================] - 2s 241us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 188us/step\n",
      "8424/8424 [==============================] - 1s 175us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 211us/step\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 189us/step\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 1s 174us/step\n",
      "8424/8424 [==============================] - 1s 177us/step\n",
      "8424/8424 [==============================] - 2s 183us/step\n",
      "8424/8424 [==============================] - 2s 197us/step\n",
      "8424/8424 [==============================] - 2s 294us/step\n",
      "8424/8424 [==============================] - 3s 305us/step\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 286us/step\n",
      "8424/8424 [==============================] - 3s 300us/step\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 238us/step\n",
      "8424/8424 [==============================] - 2s 241us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 225us/step\n",
      "8424/8424 [==============================] - 2s 244us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 253us/step\n",
      "8424/8424 [==============================] - 3s 324us/step\n",
      "8424/8424 [==============================] - 2s 223us/step\n",
      "8424/8424 [==============================] - 2s 230us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 3s 311us/step\n",
      "8424/8424 [==============================] - 3s 311us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 227us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 3s 323us/step\n",
      "8424/8424 [==============================] - 2s 295us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 203us/step\n",
      "8424/8424 [==============================] - 2s 187us/step\n",
      "8424/8424 [==============================] - 2s 184us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 197us/step\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "8424/8424 [==============================] - 1s 177us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 187us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - ETA:  - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 236us/step\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "8424/8424 [==============================] - 3s 298us/step\n",
      "8424/8424 [==============================] - 3s 304us/step\n",
      "8424/8424 [==============================] - 2s 295us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 182us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 196us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.3, 0.4 / 3, 0.3, 0.4 / 3, 0.4 / 3]\n",
    "weights = [11, 10, 14, 4, 11]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model4/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * float(sum(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub17.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
