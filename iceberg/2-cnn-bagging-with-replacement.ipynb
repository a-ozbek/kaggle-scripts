{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, add Test data predictions to Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Create Dataset](#Create-Dataset)\n",
    "* [Train - Val Split](#Train---Val-Split)\n",
    "* [Data Augmentation](#Data-Augmentation)\n",
    "* [Training](#Training)\n",
    "* [Predict Test](#Predict-Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.signal import fftconvolve\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df_train = pd.read_json('./data/train.json')\n",
    "\n",
    "# Test\n",
    "df_test = pd.read_json('./data/test.json')\n",
    "df_test_labels = pd.read_csv('./submissions/ourbest_explorestack_2.csv')\n",
    "df_test = pd.merge(df_test, df_test_labels, on='id')\n",
    "iceberg_cond = (df_test['is_iceberg'] >= 0.9) & (df_test['is_iceberg'] <= 1.0)\n",
    "not_iceberg_cond = (df_test['is_iceberg'] >= 0.0) & (df_test['is_iceberg'] <= 0.1)\n",
    "df_test = df_test[iceberg_cond | not_iceberg_cond]\n",
    "\n",
    "# Merge Train and Test\n",
    "df = pd.concat([df_train, df_test])\n",
    "df\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    im = im - np.mean(im)\n",
    "    im = im / np.std(im)\n",
    "    return im\n",
    "\n",
    "def get_convolve(im1, im2):\n",
    "    im1 = im1 - np.mean(im1)\n",
    "    im2 = im2 - np.mean(im2)\n",
    "    im_conv = fftconvolve(im1, im2[::-1, ::-1], mode='same')\n",
    "    return normalize(im_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4927, 75, 75, 2)\n",
      "y.shape: (4927,)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "#     im = np.concatenate([normalize(im_band1), normalize(im_band2), get_convolve(im_band1, im_band2)], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_random_split(size, train_ratio):\n",
    "    indices = range(size)\n",
    "    cutoff = int(round(train_ratio * size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_i = indices[:cutoff]\n",
    "    test_i = indices[cutoff:]\n",
    "    return train_i, test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "        # Architecture\n",
    "        model = Sequential()\n",
    "        # Block 1\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                         activation='relu',\n",
    "                         input_shape=input_shape))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 2\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 3\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        # Block 4\n",
    "        model.add(Conv2D(256, (3, 3)))\n",
    "        model.add(LeakyReLU())          \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        # FC\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model\n",
    "        loss = losses.binary_crossentropy\n",
    "        optimizer = optimizers.Adam()\n",
    "        metrics = ['accuracy']\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "        #     \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For - Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** MODEL: 1  ***\n",
      "train_i[:5] [4630, 1750, 2066, 4474, 4720]\n",
      "val_i[:5] [2465, 2849, 2680, 1086, 2103]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.433649795455\n",
      "np.mean(y_val): 0.426573623244\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.2625Epoch 00001: val_loss improved from inf to 0.19709, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.3635 - acc: 0.2625 - val_loss: 0.1971 - val_acc: 0.2790\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.2925Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.2105 - acc: 0.2924 - val_loss: 0.2146 - val_acc: 0.2782\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.2975Epoch 00003: val_loss improved from 0.19709 to 0.16060, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 15s 872us/step - loss: 0.1857 - acc: 0.2975 - val_loss: 0.1606 - val_acc: 0.2877\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.2991Epoch 00004: val_loss improved from 0.16060 to 0.15066, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 15s 860us/step - loss: 0.1712 - acc: 0.2990 - val_loss: 0.1507 - val_acc: 0.2910\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.3030Epoch 00005: val_loss improved from 0.15066 to 0.14653, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 15s 855us/step - loss: 0.1575 - acc: 0.3030 - val_loss: 0.1465 - val_acc: 0.2919\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.3058Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 864us/step - loss: 0.1504 - acc: 0.3059 - val_loss: 0.1743 - val_acc: 0.2854\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.3074Epoch 00007: val_loss improved from 0.14653 to 0.14254, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 15s 868us/step - loss: 0.1439 - acc: 0.3073 - val_loss: 0.1425 - val_acc: 0.2935\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.3097Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 862us/step - loss: 0.1397 - acc: 0.3097 - val_loss: 0.1480 - val_acc: 0.2926\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.3115Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 860us/step - loss: 0.1355 - acc: 0.3112 - val_loss: 0.1756 - val_acc: 0.2879\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.3117Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.1332 - acc: 0.3114 - val_loss: 0.1596 - val_acc: 0.2883\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.3171Epoch 00011: val_loss improved from 0.14254 to 0.13296, saving model to ./models/model16/model1.h5\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.1130 - acc: 0.3172 - val_loss: 0.1330 - val_acc: 0.2952\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.3202Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 857us/step - loss: 0.1112 - acc: 0.3200 - val_loss: 0.1494 - val_acc: 0.2913\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.3210Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.1090 - acc: 0.3208 - val_loss: 0.1404 - val_acc: 0.2959\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.3224Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.1061 - acc: 0.3225 - val_loss: 0.1422 - val_acc: 0.2955\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.3247Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 860us/step - loss: 0.0992 - acc: 0.3250 - val_loss: 0.1473 - val_acc: 0.2941\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.3246Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 862us/step - loss: 0.0975 - acc: 0.3247 - val_loss: 0.1349 - val_acc: 0.2970\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.3253Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 859us/step - loss: 0.0968 - acc: 0.3255 - val_loss: 0.1431 - val_acc: 0.2952\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.3265Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 861us/step - loss: 0.0952 - acc: 0.3268 - val_loss: 0.1458 - val_acc: 0.2937\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.3262Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 860us/step - loss: 0.0948 - acc: 0.3262 - val_loss: 0.1439 - val_acc: 0.2946\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.3264Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 864us/step - loss: 0.0939 - acc: 0.3265 - val_loss: 0.1464 - val_acc: 0.2941\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.3275Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 870us/step - loss: 0.0925 - acc: 0.3275 - val_loss: 0.1448 - val_acc: 0.2941\n",
      "Epoch 00021: early stopping\n",
      "*** MODEL: 2  ***\n",
      "train_i[:5] [395, 4926, 4440, 850, 1087]\n",
      "val_i[:5] [2593, 4195, 725, 3132, 4209]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.427244084131\n",
      "np.mean(y_val): 0.432981935343\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.2521Epoch 00001: val_loss improved from inf to 0.22744, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.3702 - acc: 0.2519 - val_loss: 0.2274 - val_acc: 0.2859\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.2859Epoch 00002: val_loss improved from 0.22744 to 0.15925, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 15s 875us/step - loss: 0.1992 - acc: 0.2858 - val_loss: 0.1592 - val_acc: 0.2966\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.2898Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 873us/step - loss: 0.1773 - acc: 0.2901 - val_loss: 0.1748 - val_acc: 0.2920\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.2947Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 858us/step - loss: 0.1608 - acc: 0.2946 - val_loss: 0.1708 - val_acc: 0.2944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.2944Epoch 00005: val_loss improved from 0.15925 to 0.15174, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 15s 895us/step - loss: 0.1544 - acc: 0.2943 - val_loss: 0.1517 - val_acc: 0.2972\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.2987Epoch 00006: val_loss improved from 0.15174 to 0.14828, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.1473 - acc: 0.2987 - val_loss: 0.1483 - val_acc: 0.2977\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.2991Epoch 00007: val_loss improved from 0.14828 to 0.14125, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.1457 - acc: 0.2991 - val_loss: 0.1412 - val_acc: 0.3009\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.3031Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1368 - acc: 0.3029 - val_loss: 0.1463 - val_acc: 0.2984\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.3039Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1317 - acc: 0.3039 - val_loss: 0.1536 - val_acc: 0.2996\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.3048Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1287 - acc: 0.3047 - val_loss: 0.1622 - val_acc: 0.2968\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.3087Epoch 00011: val_loss improved from 0.14125 to 0.13267, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1118 - acc: 0.3089 - val_loss: 0.1327 - val_acc: 0.3045\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.3110Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1075 - acc: 0.3113 - val_loss: 0.1335 - val_acc: 0.3037\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.3152Epoch 00013: val_loss improved from 0.13267 to 0.13106, saving model to ./models/model16/model2.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1027 - acc: 0.3151 - val_loss: 0.1311 - val_acc: 0.3047\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.3141Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1023 - acc: 0.3140 - val_loss: 0.1369 - val_acc: 0.3026\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.3163Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0994 - acc: 0.3162 - val_loss: 0.1316 - val_acc: 0.3045\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.3141Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0994 - acc: 0.3143 - val_loss: 0.1350 - val_acc: 0.3033\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.3188Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0935 - acc: 0.3186 - val_loss: 0.1324 - val_acc: 0.3044\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.3168Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.0940 - acc: 0.3171 - val_loss: 0.1315 - val_acc: 0.3047\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.3194Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0908 - acc: 0.3195 - val_loss: 0.1313 - val_acc: 0.3048\n",
      "Epoch 20/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.3192Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0905 - acc: 0.3197 - val_loss: 0.1345 - val_acc: 0.3042\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.3203Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0888 - acc: 0.3203 - val_loss: 0.1318 - val_acc: 0.3045\n",
      "Epoch 22/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.3204Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.0885 - acc: 0.3204 - val_loss: 0.1338 - val_acc: 0.3038\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.3203Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0893 - acc: 0.3203 - val_loss: 0.1324 - val_acc: 0.3043\n",
      "Epoch 00023: early stopping\n",
      "*** MODEL: 3  ***\n",
      "train_i[:5] [165, 1350, 2740, 378, 4834]\n",
      "val_i[:5] [3246, 914, 3724, 3870, 3462]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.417257916782\n",
      "np.mean(y_val): 0.442972157166\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.2595Epoch 00001: val_loss improved from inf to 0.17208, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.3285 - acc: 0.2594 - val_loss: 0.1721 - val_acc: 0.2937\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.2890Epoch 00002: val_loss improved from 0.17208 to 0.16305, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1907 - acc: 0.2890 - val_loss: 0.1631 - val_acc: 0.2947\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.2921Epoch 00003: val_loss improved from 0.16305 to 0.15484, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1721 - acc: 0.2921 - val_loss: 0.1548 - val_acc: 0.2974\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.2954Epoch 00004: val_loss improved from 0.15484 to 0.15265, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1593 - acc: 0.2957 - val_loss: 0.1526 - val_acc: 0.2954\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.2980Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 885us/step - loss: 0.1511 - acc: 0.2979 - val_loss: 0.1646 - val_acc: 0.2955\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.2992Epoch 00006: val_loss improved from 0.15265 to 0.15034, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.1472 - acc: 0.2992 - val_loss: 0.1503 - val_acc: 0.2963\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.3016Epoch 00007: val_loss improved from 0.15034 to 0.14995, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1416 - acc: 0.3017 - val_loss: 0.1500 - val_acc: 0.2985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.3027Epoch 00008: val_loss improved from 0.14995 to 0.13952, saving model to ./models/model16/model3.h5\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1365 - acc: 0.3028 - val_loss: 0.1395 - val_acc: 0.2993\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.3050Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.1314 - acc: 0.3050 - val_loss: 0.1474 - val_acc: 0.2996\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.3062Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.1265 - acc: 0.3065 - val_loss: 0.1548 - val_acc: 0.2978\n",
      "Epoch 11/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.3085Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1222 - acc: 0.3090 - val_loss: 0.1597 - val_acc: 0.2960\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.3139Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1065 - acc: 0.3138 - val_loss: 0.1399 - val_acc: 0.3003\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.3158Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1022 - acc: 0.3159 - val_loss: 0.1500 - val_acc: 0.2958\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.3173Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.0972 - acc: 0.3171 - val_loss: 0.1410 - val_acc: 0.3008\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.3167Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.0981 - acc: 0.3166 - val_loss: 0.1458 - val_acc: 0.2995\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.3176Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.0953 - acc: 0.3178 - val_loss: 0.1426 - val_acc: 0.3007\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.3185Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0940 - acc: 0.3182 - val_loss: 0.1450 - val_acc: 0.2994\n",
      "Epoch 18/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.3191Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.0944 - acc: 0.3191 - val_loss: 0.1430 - val_acc: 0.3006\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 4  ***\n",
      "train_i[:5] [2839, 3307, 3095, 3035, 2303]\n",
      "val_i[:5] [943, 785, 3850, 2102, 1582]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.42529358748\n",
      "np.mean(y_val): 0.434933223914\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.2532Epoch 00001: val_loss improved from inf to 0.22921, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.3303 - acc: 0.2532 - val_loss: 0.2292 - val_acc: 0.2894\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.2813Epoch 00002: val_loss improved from 0.22921 to 0.18156, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1865 - acc: 0.2813 - val_loss: 0.1816 - val_acc: 0.2950\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.2881Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.1588 - acc: 0.2878 - val_loss: 0.1989 - val_acc: 0.2948\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.2924Epoch 00004: val_loss improved from 0.18156 to 0.15683, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1474 - acc: 0.2923 - val_loss: 0.1568 - val_acc: 0.3017\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.2961Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1382 - acc: 0.2959 - val_loss: 0.1882 - val_acc: 0.2984\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.2964Epoch 00006: val_loss improved from 0.15683 to 0.15067, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1322 - acc: 0.2967 - val_loss: 0.1507 - val_acc: 0.3040\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.2976Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.1303 - acc: 0.2977 - val_loss: 0.1509 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.3003Epoch 00008: val_loss improved from 0.15067 to 0.15048, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1246 - acc: 0.3004 - val_loss: 0.1505 - val_acc: 0.3057\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.3007Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1238 - acc: 0.3006 - val_loss: 0.1540 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.3004Epoch 00010: val_loss improved from 0.15048 to 0.14590, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.1218 - acc: 0.3004 - val_loss: 0.1459 - val_acc: 0.3082\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.3048Epoch 00011: val_loss improved from 0.14590 to 0.14548, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1132 - acc: 0.3046 - val_loss: 0.1455 - val_acc: 0.3082\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.3034Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.1131 - acc: 0.3034 - val_loss: 0.1482 - val_acc: 0.3098\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.3051Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1137 - acc: 0.3052 - val_loss: 0.1456 - val_acc: 0.3063\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.3058Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1083 - acc: 0.3060 - val_loss: 0.1494 - val_acc: 0.3085\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.3098Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 895us/step - loss: 0.0949 - acc: 0.3097 - val_loss: 0.1499 - val_acc: 0.3081\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.3127Epoch 00016: val_loss improved from 0.14548 to 0.14398, saving model to ./models/model16/model4.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0916 - acc: 0.3126 - val_loss: 0.1440 - val_acc: 0.3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.3117Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.0898 - acc: 0.3116 - val_loss: 0.1450 - val_acc: 0.3102\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.3131Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.0882 - acc: 0.3130 - val_loss: 0.1488 - val_acc: 0.3080\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.3130Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0897 - acc: 0.3128 - val_loss: 0.1475 - val_acc: 0.3096\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.3142Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0849 - acc: 0.3141 - val_loss: 0.1467 - val_acc: 0.3085\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.3147Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0840 - acc: 0.3147 - val_loss: 0.1455 - val_acc: 0.3091\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.3141Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.0822 - acc: 0.3145 - val_loss: 0.1460 - val_acc: 0.3095\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.3154Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0814 - acc: 0.3155 - val_loss: 0.1444 - val_acc: 0.3096\n",
      "Epoch 24/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.3157Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.0823 - acc: 0.3157 - val_loss: 0.1451 - val_acc: 0.3091\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.3158Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0817 - acc: 0.3158 - val_loss: 0.1454 - val_acc: 0.3091\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.3160Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0808 - acc: 0.3160 - val_loss: 0.1454 - val_acc: 0.3090\n",
      "Epoch 00026: early stopping\n",
      "*** MODEL: 5  ***\n",
      "train_i[:5] [550, 4619, 894, 480, 454]\n",
      "val_i[:5] [4362, 4651, 3886, 3718, 4581]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.430245923072\n",
      "np.mean(y_val): 0.429978877629\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.2483Epoch 00001: val_loss improved from inf to 0.17045, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.3193 - acc: 0.2484 - val_loss: 0.1704 - val_acc: 0.3118\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.2746Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.1850 - acc: 0.2744 - val_loss: 0.1705 - val_acc: 0.3102\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.2763Epoch 00003: val_loss improved from 0.17045 to 0.16438, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1725 - acc: 0.2764 - val_loss: 0.1644 - val_acc: 0.3066\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.2813Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.1604 - acc: 0.2812 - val_loss: 0.2191 - val_acc: 0.3006\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.2834Epoch 00005: val_loss improved from 0.16438 to 0.14485, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1541 - acc: 0.2835 - val_loss: 0.1449 - val_acc: 0.3137\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.2833Epoch 00006: val_loss improved from 0.14485 to 0.13041, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1515 - acc: 0.2832 - val_loss: 0.1304 - val_acc: 0.3169\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.2862Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.1452 - acc: 0.2866 - val_loss: 0.1742 - val_acc: 0.3046\n",
      "Epoch 8/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.2871Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1398 - acc: 0.2875 - val_loss: 0.1341 - val_acc: 0.3176\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.2902Epoch 00009: val_loss improved from 0.13041 to 0.12981, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1361 - acc: 0.2902 - val_loss: 0.1298 - val_acc: 0.3168\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.2906Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1306 - acc: 0.2908 - val_loss: 0.1395 - val_acc: 0.3141\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.2910Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.1315 - acc: 0.2911 - val_loss: 0.1314 - val_acc: 0.3159\n",
      "Epoch 12/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.2937Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1256 - acc: 0.2936 - val_loss: 0.1364 - val_acc: 0.3180\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.2982Epoch 00013: val_loss improved from 0.12981 to 0.12915, saving model to ./models/model16/model5.h5\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1084 - acc: 0.2984 - val_loss: 0.1291 - val_acc: 0.3173\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.2997Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 895us/step - loss: 0.1062 - acc: 0.2997 - val_loss: 0.1301 - val_acc: 0.3173\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.3009Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1046 - acc: 0.3009 - val_loss: 0.1360 - val_acc: 0.3163\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.3007Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1031 - acc: 0.3007 - val_loss: 0.1307 - val_acc: 0.3177\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.3045Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.0964 - acc: 0.3043 - val_loss: 0.1316 - val_acc: 0.3167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.3051Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0953 - acc: 0.3052 - val_loss: 0.1331 - val_acc: 0.3169\n",
      "Epoch 19/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.3062Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.0938 - acc: 0.3059 - val_loss: 0.1311 - val_acc: 0.3175\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.3059Epoch 00020: val_loss improved from 0.12915 to 0.12911, saving model to ./models/model16/model5.h5\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.0924 - acc: 0.3060 - val_loss: 0.1291 - val_acc: 0.3174\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.3050Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0931 - acc: 0.3048 - val_loss: 0.1333 - val_acc: 0.3165\n",
      "Epoch 22/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.3062Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.0915 - acc: 0.3059 - val_loss: 0.1318 - val_acc: 0.3172\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.3054Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.0924 - acc: 0.3051 - val_loss: 0.1321 - val_acc: 0.3169\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3059Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0922 - acc: 0.3060 - val_loss: 0.1324 - val_acc: 0.3169\n",
      "Epoch 25/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3056Epoch 00025: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0934 - acc: 0.3054 - val_loss: 0.1326 - val_acc: 0.3168\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.3054Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 4.26184415119e-07.\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.0926 - acc: 0.3054 - val_loss: 0.1327 - val_acc: 0.3169\n",
      "Epoch 27/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.3055Epoch 00027: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.0931 - acc: 0.3054 - val_loss: 0.1326 - val_acc: 0.3169\n",
      "Epoch 28/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.3055Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 1.40640856614e-07.\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.0926 - acc: 0.3053 - val_loss: 0.1326 - val_acc: 0.3169\n",
      "Epoch 29/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.3053Epoch 00029: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0935 - acc: 0.3053 - val_loss: 0.1325 - val_acc: 0.3169\n",
      "Epoch 30/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3048Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 4.64114805254e-08.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.0934 - acc: 0.3046 - val_loss: 0.1325 - val_acc: 0.3169\n",
      "Epoch 00030: early stopping\n",
      "*** MODEL: 6  ***\n",
      "train_i[:5] [4884, 33, 2555, 249, 4256]\n",
      "val_i[:5] [4001, 2091, 3475, 3978, 590]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.418930683117\n",
      "np.mean(y_val): 0.441298711673\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.2576Epoch 00001: val_loss improved from inf to 0.21840, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.3174 - acc: 0.2574 - val_loss: 0.2184 - val_acc: 0.2883\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.2851Epoch 00002: val_loss improved from 0.21840 to 0.15296, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1850 - acc: 0.2853 - val_loss: 0.1530 - val_acc: 0.3022\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.2880Epoch 00003: val_loss improved from 0.15296 to 0.15158, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1665 - acc: 0.2880 - val_loss: 0.1516 - val_acc: 0.3006\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.2927Epoch 00004: val_loss improved from 0.15158 to 0.14814, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1555 - acc: 0.2926 - val_loss: 0.1481 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.2943Epoch 00005: val_loss improved from 0.14814 to 0.14774, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1492 - acc: 0.2944 - val_loss: 0.1477 - val_acc: 0.3035\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.2952Epoch 00006: val_loss improved from 0.14774 to 0.14583, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1440 - acc: 0.2953 - val_loss: 0.1458 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.2979Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1354 - acc: 0.2979 - val_loss: 0.1510 - val_acc: 0.2998\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.3001Epoch 00008: val_loss improved from 0.14583 to 0.13838, saving model to ./models/model16/model6.h5\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1351 - acc: 0.3000 - val_loss: 0.1384 - val_acc: 0.3053\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.3005Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1307 - acc: 0.3008 - val_loss: 0.1426 - val_acc: 0.3064\n",
      "Epoch 10/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.3023Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1267 - acc: 0.3024 - val_loss: 0.1426 - val_acc: 0.3027\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.3062Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1198 - acc: 0.3062 - val_loss: 0.1718 - val_acc: 0.2955\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.3085Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1046 - acc: 0.3087 - val_loss: 0.1539 - val_acc: 0.3017\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.3109Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1028 - acc: 0.3109 - val_loss: 0.1509 - val_acc: 0.3029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.3138Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.0970 - acc: 0.3134 - val_loss: 0.1442 - val_acc: 0.3038\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.3130Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.0965 - acc: 0.3133 - val_loss: 0.1619 - val_acc: 0.2999\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.3130Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.0947 - acc: 0.3133 - val_loss: 0.1449 - val_acc: 0.3033\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.3146Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.0939 - acc: 0.3147 - val_loss: 0.1499 - val_acc: 0.3034\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.3135Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0942 - acc: 0.3135 - val_loss: 0.1473 - val_acc: 0.3032\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 7  ***\n",
      "train_i[:5] [4779, 101, 3889, 4481, 3642]\n",
      "val_i[:5] [4646, 3543, 3578, 4615, 982]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.42709809789\n",
      "np.mean(y_val): 0.433127980857\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.2617Epoch 00001: val_loss improved from inf to 0.19377, saving model to ./models/model16/model7.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.3407 - acc: 0.2615 - val_loss: 0.1938 - val_acc: 0.2843\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.2943Epoch 00002: val_loss improved from 0.19377 to 0.17207, saving model to ./models/model16/model7.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1855 - acc: 0.2944 - val_loss: 0.1721 - val_acc: 0.2895\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.3000Epoch 00003: val_loss improved from 0.17207 to 0.15319, saving model to ./models/model16/model7.h5\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1600 - acc: 0.3000 - val_loss: 0.1532 - val_acc: 0.2912\n",
      "Epoch 4/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.3023Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1525 - acc: 0.3023 - val_loss: 0.1542 - val_acc: 0.2960\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.3048Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1456 - acc: 0.3049 - val_loss: 0.1638 - val_acc: 0.2939\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.3054Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1417 - acc: 0.3055 - val_loss: 0.1739 - val_acc: 0.2919\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.3123Epoch 00007: val_loss improved from 0.15319 to 0.14932, saving model to ./models/model16/model7.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1214 - acc: 0.3122 - val_loss: 0.1493 - val_acc: 0.2956\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.3134Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1176 - acc: 0.3137 - val_loss: 0.1530 - val_acc: 0.2953\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.3155Epoch 00009: val_loss improved from 0.14932 to 0.13887, saving model to ./models/model16/model7.h5\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1129 - acc: 0.3155 - val_loss: 0.1389 - val_acc: 0.2961\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.3167Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1085 - acc: 0.3170 - val_loss: 0.1397 - val_acc: 0.2967\n",
      "Epoch 11/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.3175Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 895us/step - loss: 0.1078 - acc: 0.3178 - val_loss: 0.1400 - val_acc: 0.2958\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.3186Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.1054 - acc: 0.3183 - val_loss: 0.1483 - val_acc: 0.2957\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.3208Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.0988 - acc: 0.3206 - val_loss: 0.1420 - val_acc: 0.2971\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.3223Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.0962 - acc: 0.3224 - val_loss: 0.1482 - val_acc: 0.2973\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3231Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.0934 - acc: 0.3232 - val_loss: 0.1434 - val_acc: 0.2970\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3237Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.0935 - acc: 0.3235 - val_loss: 0.1420 - val_acc: 0.2969\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.3240Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0927 - acc: 0.3240 - val_loss: 0.1446 - val_acc: 0.2975\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.3233Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0931 - acc: 0.3233 - val_loss: 0.1443 - val_acc: 0.2968\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.3237Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.0920 - acc: 0.3236 - val_loss: 0.1457 - val_acc: 0.2969\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 8  ***\n",
      "train_i[:5] [4023, 2950, 1933, 3189, 730]\n",
      "val_i[:5] [3261, 3102, 4650, 1931, 2895]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.447398471185\n",
      "np.mean(y_val): 0.412819365428\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.2577Epoch 00001: val_loss improved from inf to 0.19578, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.3434 - acc: 0.2577 - val_loss: 0.1958 - val_acc: 0.2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.2902Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1920 - acc: 0.2901 - val_loss: 0.2313 - val_acc: 0.2800\n",
      "Epoch 3/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.2942Epoch 00003: val_loss improved from 0.19578 to 0.14417, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1737 - acc: 0.2941 - val_loss: 0.1442 - val_acc: 0.2962\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.2976Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.1606 - acc: 0.2975 - val_loss: 0.1911 - val_acc: 0.2902\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.3000Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1513 - acc: 0.3001 - val_loss: 0.1456 - val_acc: 0.2982\n",
      "Epoch 6/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.3007Epoch 00006: val_loss improved from 0.14417 to 0.14325, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.1474 - acc: 0.3009 - val_loss: 0.1433 - val_acc: 0.2965\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.3041Epoch 00007: val_loss improved from 0.14325 to 0.14127, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1422 - acc: 0.3040 - val_loss: 0.1413 - val_acc: 0.2992\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.3054Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1376 - acc: 0.3053 - val_loss: 0.1436 - val_acc: 0.2984\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.3070Epoch 00009: val_loss improved from 0.14127 to 0.13831, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 15s 895us/step - loss: 0.1276 - acc: 0.3075 - val_loss: 0.1383 - val_acc: 0.2983\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.3070Epoch 00010: val_loss improved from 0.13831 to 0.13785, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1279 - acc: 0.3069 - val_loss: 0.1378 - val_acc: 0.2978\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.3096Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1254 - acc: 0.3095 - val_loss: 0.1417 - val_acc: 0.2975\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.3115Epoch 00012: val_loss improved from 0.13785 to 0.13553, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1209 - acc: 0.3114 - val_loss: 0.1355 - val_acc: 0.3001\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.3118Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1181 - acc: 0.3119 - val_loss: 0.1413 - val_acc: 0.2991\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.3127Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.1135 - acc: 0.3126 - val_loss: 0.1428 - val_acc: 0.2981\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.3151Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1129 - acc: 0.3152 - val_loss: 0.1457 - val_acc: 0.3000\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.3185Epoch 00016: val_loss improved from 0.13553 to 0.13169, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.0983 - acc: 0.3185 - val_loss: 0.1317 - val_acc: 0.3018\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.3199Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.0948 - acc: 0.3200 - val_loss: 0.1374 - val_acc: 0.3003\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.3217Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0914 - acc: 0.3220 - val_loss: 0.1358 - val_acc: 0.3001\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.3223Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.0910 - acc: 0.3226 - val_loss: 0.1388 - val_acc: 0.3007\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.3231Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.0894 - acc: 0.3233 - val_loss: 0.1331 - val_acc: 0.3021\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.3232Epoch 00021: val_loss improved from 0.13169 to 0.12974, saving model to ./models/model16/model8.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0874 - acc: 0.3234 - val_loss: 0.1297 - val_acc: 0.3032\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.3235Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0864 - acc: 0.3233 - val_loss: 0.1362 - val_acc: 0.2998\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.3248Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0857 - acc: 0.3247 - val_loss: 0.1321 - val_acc: 0.3022\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.3236Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.0861 - acc: 0.3239 - val_loss: 0.1434 - val_acc: 0.2989\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.3250Epoch 00025: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0834 - acc: 0.3251 - val_loss: 0.1349 - val_acc: 0.3012\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.3242Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.0835 - acc: 0.3244 - val_loss: 0.1336 - val_acc: 0.3011\n",
      "Epoch 27/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.3246Epoch 00027: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0836 - acc: 0.3246 - val_loss: 0.1349 - val_acc: 0.3011\n",
      "Epoch 28/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.3246Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.0829 - acc: 0.3248 - val_loss: 0.1325 - val_acc: 0.3020\n",
      "Epoch 29/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.3246Epoch 00029: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0845 - acc: 0.3246 - val_loss: 0.1334 - val_acc: 0.3018\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.3253Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.0822 - acc: 0.3253 - val_loss: 0.1340 - val_acc: 0.3016\n",
      "Epoch 31/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.3240Epoch 00031: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0853 - acc: 0.3240 - val_loss: 0.1344 - val_acc: 0.3014\n",
      "Epoch 00031: early stopping\n",
      "*** MODEL: 9  ***\n",
      "train_i[:5] [3802, 3387, 157, 1873, 3865]\n",
      "val_i[:5] [903, 3429, 844, 2239, 7]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.428735114306\n",
      "np.mean(y_val): 0.431490299797\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.2624Epoch 00001: val_loss improved from inf to 0.19795, saving model to ./models/model16/model9.h5\n",
      "17248/17248 [==============================] - 17s 972us/step - loss: 0.3265 - acc: 0.2624 - val_loss: 0.1980 - val_acc: 0.2862\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.2918Epoch 00002: val_loss improved from 0.19795 to 0.17548, saving model to ./models/model16/model9.h5\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1911 - acc: 0.2920 - val_loss: 0.1755 - val_acc: 0.2902\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.2991Epoch 00003: val_loss improved from 0.17548 to 0.17023, saving model to ./models/model16/model9.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1670 - acc: 0.2993 - val_loss: 0.1702 - val_acc: 0.2904\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.3007Epoch 00004: val_loss improved from 0.17023 to 0.15124, saving model to ./models/model16/model9.h5\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1559 - acc: 0.3008 - val_loss: 0.1512 - val_acc: 0.2942\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.3048Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1442 - acc: 0.3047 - val_loss: 0.1721 - val_acc: 0.2901\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.3050Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1443 - acc: 0.3050 - val_loss: 0.1537 - val_acc: 0.2951\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.3073Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.1363 - acc: 0.3075 - val_loss: 0.1734 - val_acc: 0.2903\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.3141Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1160 - acc: 0.3139 - val_loss: 0.1660 - val_acc: 0.2935\n",
      "Epoch 9/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.3150Epoch 00009: val_loss improved from 0.15124 to 0.14098, saving model to ./models/model16/model9.h5\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1132 - acc: 0.3153 - val_loss: 0.1410 - val_acc: 0.2967\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.3167Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1089 - acc: 0.3167 - val_loss: 0.1534 - val_acc: 0.2934\n",
      "Epoch 11/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.3173Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1076 - acc: 0.3174 - val_loss: 0.1733 - val_acc: 0.2926\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.3199Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.1042 - acc: 0.3198 - val_loss: 0.1578 - val_acc: 0.2948\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.3197Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1013 - acc: 0.3199 - val_loss: 0.1567 - val_acc: 0.2942\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3220Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0960 - acc: 0.3220 - val_loss: 0.1536 - val_acc: 0.2950\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3226Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0961 - acc: 0.3228 - val_loss: 0.1576 - val_acc: 0.2944\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.3223Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0948 - acc: 0.3223 - val_loss: 0.1532 - val_acc: 0.2945\n",
      "Epoch 17/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.3231Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.0948 - acc: 0.3231 - val_loss: 0.1582 - val_acc: 0.2944\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.3237Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.0935 - acc: 0.3238 - val_loss: 0.1577 - val_acc: 0.2947\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.3228Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.0942 - acc: 0.3228 - val_loss: 0.1578 - val_acc: 0.2944\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 10  ***\n",
      "train_i[:5] [4436, 549, 2777, 3345, 694]\n",
      "val_i[:5] [1268, 4471, 4562, 3763, 2585]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.444387864915\n",
      "np.mean(y_val): 0.415831194032\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.2534Epoch 00001: val_loss improved from inf to 0.20115, saving model to ./models/model16/model10.h5\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.3387 - acc: 0.2535 - val_loss: 0.2012 - val_acc: 0.2859\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.2851Epoch 00002: val_loss improved from 0.20115 to 0.17446, saving model to ./models/model16/model10.h5\n",
      "17248/17248 [==============================] - 16s 901us/step - loss: 0.1879 - acc: 0.2849 - val_loss: 0.1745 - val_acc: 0.2916\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.2910Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1642 - acc: 0.2909 - val_loss: 0.1986 - val_acc: 0.2883\n",
      "Epoch 4/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.2941Epoch 00004: val_loss improved from 0.17446 to 0.14780, saving model to ./models/model16/model10.h5\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1540 - acc: 0.2941 - val_loss: 0.1478 - val_acc: 0.2992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.2974Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 890us/step - loss: 0.1440 - acc: 0.2974 - val_loss: 0.1723 - val_acc: 0.2941\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.2980Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1427 - acc: 0.2976 - val_loss: 0.1658 - val_acc: 0.2975\n",
      "Epoch 7/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.3007Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1368 - acc: 0.3004 - val_loss: 0.1556 - val_acc: 0.2986\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.3055Epoch 00008: val_loss improved from 0.14780 to 0.14725, saving model to ./models/model16/model10.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1179 - acc: 0.3054 - val_loss: 0.1473 - val_acc: 0.3003\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.3072Epoch 00009: val_loss improved from 0.14725 to 0.14449, saving model to ./models/model16/model10.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1152 - acc: 0.3071 - val_loss: 0.1445 - val_acc: 0.3022\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.3100Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1084 - acc: 0.3099 - val_loss: 0.1693 - val_acc: 0.2964\n",
      "Epoch 11/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.3100Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1098 - acc: 0.3098 - val_loss: 0.1451 - val_acc: 0.3011\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.3105Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1054 - acc: 0.3105 - val_loss: 0.1602 - val_acc: 0.3003\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.3139Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0992 - acc: 0.3138 - val_loss: 0.1480 - val_acc: 0.3002\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.3144Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0967 - acc: 0.3142 - val_loss: 0.1481 - val_acc: 0.3014\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.3152Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.0956 - acc: 0.3152 - val_loss: 0.1575 - val_acc: 0.3003\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.3151Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0941 - acc: 0.3151 - val_loss: 0.1495 - val_acc: 0.3006\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.3150Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.0937 - acc: 0.3149 - val_loss: 0.1555 - val_acc: 0.2999\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3156Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0923 - acc: 0.3159 - val_loss: 0.1515 - val_acc: 0.3004\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3153Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0934 - acc: 0.3156 - val_loss: 0.1521 - val_acc: 0.3003\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 11  ***\n",
      "train_i[:5] [1490, 3280, 1479, 977, 90]\n",
      "val_i[:5] [2542, 2968, 3590, 3314, 4500]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.427161967979\n",
      "np.mean(y_val): 0.433064084836\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.2692Epoch 00001: val_loss improved from inf to 0.18790, saving model to ./models/model16/model11.h5\n",
      "17248/17248 [==============================] - 16s 950us/step - loss: 0.3459 - acc: 0.2692 - val_loss: 0.1879 - val_acc: 0.2761\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.3030Epoch 00002: val_loss improved from 0.18790 to 0.15643, saving model to ./models/model16/model11.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1960 - acc: 0.3030 - val_loss: 0.1564 - val_acc: 0.2848\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.3073Epoch 00003: val_loss improved from 0.15643 to 0.14833, saving model to ./models/model16/model11.h5\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.1740 - acc: 0.3073 - val_loss: 0.1483 - val_acc: 0.2847\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.3112Epoch 00004: val_loss improved from 0.14833 to 0.14728, saving model to ./models/model16/model11.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1637 - acc: 0.3109 - val_loss: 0.1473 - val_acc: 0.2851\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.3107Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1572 - acc: 0.3111 - val_loss: 0.1603 - val_acc: 0.2830\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.3144Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1486 - acc: 0.3144 - val_loss: 0.1736 - val_acc: 0.2816\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.3149Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1437 - acc: 0.3151 - val_loss: 0.1518 - val_acc: 0.2825\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.3213Epoch 00008: val_loss improved from 0.14728 to 0.13383, saving model to ./models/model16/model11.h5\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1220 - acc: 0.3214 - val_loss: 0.1338 - val_acc: 0.2890\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.3223Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1209 - acc: 0.3220 - val_loss: 0.1418 - val_acc: 0.2861\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.3241Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1132 - acc: 0.3240 - val_loss: 0.1502 - val_acc: 0.2852\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.3255Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1111 - acc: 0.3253 - val_loss: 0.1425 - val_acc: 0.2879\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.3284Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.1041 - acc: 0.3284 - val_loss: 0.1422 - val_acc: 0.2886\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.3281Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1038 - acc: 0.3282 - val_loss: 0.1430 - val_acc: 0.2883\n",
      "Epoch 14/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.3301Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0990 - acc: 0.3301 - val_loss: 0.1462 - val_acc: 0.2876\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.3297Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1007 - acc: 0.3296 - val_loss: 0.1491 - val_acc: 0.2877\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.3300Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0996 - acc: 0.3300 - val_loss: 0.1459 - val_acc: 0.2886\n",
      "Epoch 17/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.3297Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0973 - acc: 0.3298 - val_loss: 0.1444 - val_acc: 0.2883\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.3308Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0972 - acc: 0.3306 - val_loss: 0.1464 - val_acc: 0.2881\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 12  ***\n",
      "train_i[:5] [1675, 4616, 974, 283, 3124]\n",
      "val_i[:5] [3288, 1009, 451, 1027, 1343]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.414199931981\n",
      "np.mean(y_val): 0.446031383536\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.2615Epoch 00001: val_loss improved from inf to 0.18787, saving model to ./models/model16/model12.h5\n",
      "17248/17248 [==============================] - 16s 945us/step - loss: 0.3486 - acc: 0.2616 - val_loss: 0.1879 - val_acc: 0.2920\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.2897Epoch 00002: val_loss improved from 0.18787 to 0.15508, saving model to ./models/model16/model12.h5\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.2071 - acc: 0.2899 - val_loss: 0.1551 - val_acc: 0.2932\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.2959Epoch 00003: val_loss improved from 0.15508 to 0.13650, saving model to ./models/model16/model12.h5\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1820 - acc: 0.2961 - val_loss: 0.1365 - val_acc: 0.2980\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.2994Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1660 - acc: 0.2992 - val_loss: 0.1378 - val_acc: 0.2972\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.3025Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 894us/step - loss: 0.1590 - acc: 0.3026 - val_loss: 0.1465 - val_acc: 0.2961\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.3035Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1522 - acc: 0.3038 - val_loss: 0.1382 - val_acc: 0.2948\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.3115Epoch 00007: val_loss improved from 0.13650 to 0.12786, saving model to ./models/model16/model12.h5\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1297 - acc: 0.3116 - val_loss: 0.1279 - val_acc: 0.2994\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.3122Epoch 00008: val_loss improved from 0.12786 to 0.12540, saving model to ./models/model16/model12.h5\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1251 - acc: 0.3123 - val_loss: 0.1254 - val_acc: 0.2988\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.3136Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1222 - acc: 0.3137 - val_loss: 0.1419 - val_acc: 0.2924\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.3148Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1193 - acc: 0.3145 - val_loss: 0.1366 - val_acc: 0.2957\n",
      "Epoch 11/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.3165Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1174 - acc: 0.3166 - val_loss: 0.1424 - val_acc: 0.2949\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.3195Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1077 - acc: 0.3195 - val_loss: 0.1297 - val_acc: 0.2979\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.3196Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1076 - acc: 0.3195 - val_loss: 0.1267 - val_acc: 0.2989\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.3221Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1036 - acc: 0.3220 - val_loss: 0.1354 - val_acc: 0.2966\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.3224Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1026 - acc: 0.3227 - val_loss: 0.1308 - val_acc: 0.2984\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.3224Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.1028 - acc: 0.3222 - val_loss: 0.1317 - val_acc: 0.2984\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.3226Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1034 - acc: 0.3225 - val_loss: 0.1341 - val_acc: 0.2971\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.3211Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.1035 - acc: 0.3213 - val_loss: 0.1319 - val_acc: 0.2980\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 13  ***\n",
      "train_i[:5] [3025, 2352, 3979, 3087, 682]\n",
      "val_i[:5] [297, 2767, 753, 1443, 3844]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.436761536404\n",
      "np.mean(y_val): 0.4234606189\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.2640Epoch 00001: val_loss improved from inf to 0.17657, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 17s 959us/step - loss: 0.3261 - acc: 0.2640 - val_loss: 0.1766 - val_acc: 0.2914\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.2876Epoch 00002: val_loss improved from 0.17657 to 0.16286, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1945 - acc: 0.2880 - val_loss: 0.1629 - val_acc: 0.2948\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.2944Epoch 00003: val_loss improved from 0.16286 to 0.15545, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.1733 - acc: 0.2946 - val_loss: 0.1554 - val_acc: 0.2940\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.2975Epoch 00004: val_loss improved from 0.15545 to 0.14549, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1609 - acc: 0.2974 - val_loss: 0.1455 - val_acc: 0.2959\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.2999Epoch 00005: val_loss improved from 0.14549 to 0.14345, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1529 - acc: 0.2999 - val_loss: 0.1435 - val_acc: 0.2969\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.3013Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1469 - acc: 0.3011 - val_loss: 0.1650 - val_acc: 0.2904\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.3035Epoch 00007: val_loss improved from 0.14345 to 0.14056, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1405 - acc: 0.3036 - val_loss: 0.1406 - val_acc: 0.2966\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.3048Epoch 00008: val_loss improved from 0.14056 to 0.13482, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1383 - acc: 0.3046 - val_loss: 0.1348 - val_acc: 0.2987\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.3077Epoch 00009: val_loss improved from 0.13482 to 0.13052, saving model to ./models/model16/model13.h5\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.1331 - acc: 0.3077 - val_loss: 0.1305 - val_acc: 0.3020\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.3077Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1309 - acc: 0.3077 - val_loss: 0.1855 - val_acc: 0.2882\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.3108Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1266 - acc: 0.3107 - val_loss: 0.1382 - val_acc: 0.2986\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.3092Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.1227 - acc: 0.3095 - val_loss: 0.1511 - val_acc: 0.2966\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.3162Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1072 - acc: 0.3160 - val_loss: 0.1393 - val_acc: 0.2989\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.3172Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1053 - acc: 0.3170 - val_loss: 0.1457 - val_acc: 0.2977\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.3200Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.0975 - acc: 0.3202 - val_loss: 0.1438 - val_acc: 0.2977\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.3206Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.0981 - acc: 0.3207 - val_loss: 0.1363 - val_acc: 0.2995\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3212Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 891us/step - loss: 0.0945 - acc: 0.3208 - val_loss: 0.1386 - val_acc: 0.2991\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.3203Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0966 - acc: 0.3203 - val_loss: 0.1376 - val_acc: 0.2992\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.3219Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.0937 - acc: 0.3220 - val_loss: 0.1389 - val_acc: 0.2984\n",
      "Epoch 00019: early stopping\n",
      "*** MODEL: 14  ***\n",
      "train_i[:5] [900, 2748, 3250, 2596, 2496]\n",
      "val_i[:5] [438, 3702, 4788, 1622, 4813]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.424970732589\n",
      "np.mean(y_val): 0.435256209886\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.2578Epoch 00001: val_loss improved from inf to 0.19316, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 17s 960us/step - loss: 0.3501 - acc: 0.2579 - val_loss: 0.1932 - val_acc: 0.2839\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.2943Epoch 00002: val_loss improved from 0.19316 to 0.15691, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 15s 880us/step - loss: 0.1866 - acc: 0.2941 - val_loss: 0.1569 - val_acc: 0.2895\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.3005Epoch 00003: val_loss improved from 0.15691 to 0.15624, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1658 - acc: 0.3006 - val_loss: 0.1562 - val_acc: 0.2909\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.3016Epoch 00004: val_loss improved from 0.15624 to 0.14846, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1608 - acc: 0.3017 - val_loss: 0.1485 - val_acc: 0.2927\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.3049Epoch 00005: val_loss improved from 0.14846 to 0.13875, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1461 - acc: 0.3051 - val_loss: 0.1388 - val_acc: 0.2946\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.3091Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1417 - acc: 0.3094 - val_loss: 0.1682 - val_acc: 0.2902\n",
      "Epoch 7/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.3099Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 893us/step - loss: 0.1374 - acc: 0.3101 - val_loss: 0.1446 - val_acc: 0.2937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.3108Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 934us/step - loss: 0.1335 - acc: 0.3107 - val_loss: 0.1454 - val_acc: 0.2951\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.3163Epoch 00009: val_loss improved from 0.13875 to 0.13412, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.1161 - acc: 0.3164 - val_loss: 0.1341 - val_acc: 0.2966\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.3181Epoch 00010: val_loss improved from 0.13412 to 0.13248, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1111 - acc: 0.3185 - val_loss: 0.1325 - val_acc: 0.2951\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.3190Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1102 - acc: 0.3191 - val_loss: 0.1349 - val_acc: 0.2970\n",
      "Epoch 12/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.3212Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.1062 - acc: 0.3213 - val_loss: 0.1354 - val_acc: 0.2952\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.3223Epoch 00013: val_loss improved from 0.13248 to 0.13198, saving model to ./models/model16/model14.h5\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1043 - acc: 0.3224 - val_loss: 0.1320 - val_acc: 0.2969\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.3236Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1006 - acc: 0.3236 - val_loss: 0.1334 - val_acc: 0.2967\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.3229Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1024 - acc: 0.3229 - val_loss: 0.1417 - val_acc: 0.2950\n",
      "Epoch 16/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.3251Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0985 - acc: 0.3250 - val_loss: 0.1361 - val_acc: 0.2961\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.3264Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0950 - acc: 0.3268 - val_loss: 0.1367 - val_acc: 0.2966\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.3269Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0923 - acc: 0.3266 - val_loss: 0.1328 - val_acc: 0.2978\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.3277Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0916 - acc: 0.3278 - val_loss: 0.1361 - val_acc: 0.2964\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.3284Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.0901 - acc: 0.3283 - val_loss: 0.1386 - val_acc: 0.2960\n",
      "Epoch 21/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.3286Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0893 - acc: 0.3283 - val_loss: 0.1357 - val_acc: 0.2969\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.3283Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.0892 - acc: 0.3284 - val_loss: 0.1345 - val_acc: 0.2969\n",
      "Epoch 23/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.3280Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.0892 - acc: 0.3283 - val_loss: 0.1354 - val_acc: 0.2967\n",
      "Epoch 00023: early stopping\n",
      "*** MODEL: 15  ***\n",
      "train_i[:5] [4006, 4206, 1093, 874, 323]\n",
      "val_i[:5] [4223, 2845, 4523, 1862, 4487]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.427977871124\n",
      "np.mean(y_val): 0.432247850426\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.2620Epoch 00001: val_loss improved from inf to 0.21920, saving model to ./models/model16/model15.h5\n",
      "17248/17248 [==============================] - 16s 945us/step - loss: 0.3605 - acc: 0.2618 - val_loss: 0.2192 - val_acc: 0.2668\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.2924Epoch 00002: val_loss improved from 0.21920 to 0.16129, saving model to ./models/model16/model15.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.2049 - acc: 0.2926 - val_loss: 0.1613 - val_acc: 0.2833\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.2975Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1814 - acc: 0.2976 - val_loss: 0.1648 - val_acc: 0.2860\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.3029Epoch 00004: val_loss improved from 0.16129 to 0.15000, saving model to ./models/model16/model15.h5\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1648 - acc: 0.3031 - val_loss: 0.1500 - val_acc: 0.2904\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.3048Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.1577 - acc: 0.3049 - val_loss: 0.2202 - val_acc: 0.2683\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.3076Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1527 - acc: 0.3076 - val_loss: 0.1547 - val_acc: 0.2883\n",
      "Epoch 7/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.3083Epoch 00007: val_loss improved from 0.15000 to 0.14512, saving model to ./models/model16/model15.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1474 - acc: 0.3087 - val_loss: 0.1451 - val_acc: 0.2907\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.3113Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1419 - acc: 0.3114 - val_loss: 0.1530 - val_acc: 0.2857\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.3119Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1386 - acc: 0.3120 - val_loss: 0.1492 - val_acc: 0.2864\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.3142Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1343 - acc: 0.3142 - val_loss: 0.1935 - val_acc: 0.2857\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.3185Epoch 00011: val_loss improved from 0.14512 to 0.13543, saving model to ./models/model16/model15.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1175 - acc: 0.3189 - val_loss: 0.1354 - val_acc: 0.2911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.3216Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.1130 - acc: 0.3218 - val_loss: 0.1357 - val_acc: 0.2937\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.3231Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1092 - acc: 0.3232 - val_loss: 0.1468 - val_acc: 0.2897\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.3230Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1059 - acc: 0.3232 - val_loss: 0.1533 - val_acc: 0.2899\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.3260Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0993 - acc: 0.3262 - val_loss: 0.1440 - val_acc: 0.2912\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.3277Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.0983 - acc: 0.3276 - val_loss: 0.1394 - val_acc: 0.2922\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.3285Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0964 - acc: 0.3283 - val_loss: 0.1444 - val_acc: 0.2915\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.3274Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.0977 - acc: 0.3272 - val_loss: 0.1404 - val_acc: 0.2926\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.3282Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0963 - acc: 0.3282 - val_loss: 0.1418 - val_acc: 0.2924\n",
      "Epoch 20/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.3282Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.0959 - acc: 0.3283 - val_loss: 0.1406 - val_acc: 0.2924\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3280Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.0944 - acc: 0.3279 - val_loss: 0.1430 - val_acc: 0.2917\n",
      "Epoch 00021: early stopping\n",
      "*** MODEL: 16  ***\n",
      "train_i[:5] [3450, 4147, 3201, 1158, 2941]\n",
      "val_i[:5] [753, 917, 3903, 3228, 31]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.437327939367\n",
      "np.mean(y_val): 0.422893985972\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.2564Epoch 00001: val_loss improved from inf to 0.16876, saving model to ./models/model16/model16.h5\n",
      "17248/17248 [==============================] - 17s 962us/step - loss: 0.3482 - acc: 0.2567 - val_loss: 0.1688 - val_acc: 0.2919\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.2894Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1933 - acc: 0.2894 - val_loss: 0.1761 - val_acc: 0.2911\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.2951Epoch 00003: val_loss improved from 0.16876 to 0.14300, saving model to ./models/model16/model16.h5\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1697 - acc: 0.2952 - val_loss: 0.1430 - val_acc: 0.2980\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.2987Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1578 - acc: 0.2986 - val_loss: 0.1548 - val_acc: 0.2942\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.3004Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1525 - acc: 0.3003 - val_loss: 0.1493 - val_acc: 0.2964\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.3051Epoch 00006: val_loss improved from 0.14300 to 0.13699, saving model to ./models/model16/model16.h5\n",
      "17248/17248 [==============================] - 15s 847us/step - loss: 0.1415 - acc: 0.3051 - val_loss: 0.1370 - val_acc: 0.2971\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.3062Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 806us/step - loss: 0.1380 - acc: 0.3062 - val_loss: 0.1391 - val_acc: 0.3013\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.3065Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 884us/step - loss: 0.1358 - acc: 0.3062 - val_loss: 0.1415 - val_acc: 0.2972\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.3103Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 885us/step - loss: 0.1270 - acc: 0.3104 - val_loss: 0.1813 - val_acc: 0.2894\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.3135Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.1156 - acc: 0.3134 - val_loss: 0.1413 - val_acc: 0.2985\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.3153Epoch 00011: val_loss improved from 0.13699 to 0.13244, saving model to ./models/model16/model16.h5\n",
      "17248/17248 [==============================] - 15s 885us/step - loss: 0.1083 - acc: 0.3152 - val_loss: 0.1324 - val_acc: 0.3011\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.3163Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 883us/step - loss: 0.1098 - acc: 0.3161 - val_loss: 0.1495 - val_acc: 0.2960\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.3185Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1036 - acc: 0.3185 - val_loss: 0.1488 - val_acc: 0.2969\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.3200Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1000 - acc: 0.3198 - val_loss: 0.1425 - val_acc: 0.2991\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.3219Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 878us/step - loss: 0.0955 - acc: 0.3220 - val_loss: 0.1449 - val_acc: 0.2990\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.3205Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 882us/step - loss: 0.0958 - acc: 0.3205 - val_loss: 0.1385 - val_acc: 0.2995\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.3216Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 874us/step - loss: 0.0940 - acc: 0.3214 - val_loss: 0.1406 - val_acc: 0.3000\n",
      "Epoch 18/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.3231Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.0913 - acc: 0.3231 - val_loss: 0.1401 - val_acc: 0.2995\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.3225Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0915 - acc: 0.3224 - val_loss: 0.1415 - val_acc: 0.2993\n",
      "Epoch 20/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.3233Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0909 - acc: 0.3237 - val_loss: 0.1429 - val_acc: 0.2993\n",
      "Epoch 21/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.3239Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.0904 - acc: 0.3236 - val_loss: 0.1407 - val_acc: 0.2995\n",
      "Epoch 00021: early stopping\n",
      "*** MODEL: 17  ***\n",
      "train_i[:5] [1146, 4321, 2780, 1084, 3364]\n",
      "val_i[:5] [3295, 1447, 2251, 2442, 756]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.430913757163\n",
      "np.mean(y_val): 0.429310772391\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.2598Epoch 00001: val_loss improved from inf to 0.18964, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 17s 959us/step - loss: 0.3549 - acc: 0.2600 - val_loss: 0.1896 - val_acc: 0.2780\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.3011Epoch 00002: val_loss improved from 0.18964 to 0.17005, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1821 - acc: 0.3011 - val_loss: 0.1701 - val_acc: 0.2807\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.3050Epoch 00003: val_loss improved from 0.17005 to 0.16855, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1677 - acc: 0.3049 - val_loss: 0.1686 - val_acc: 0.2828\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.3058Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.1601 - acc: 0.3059 - val_loss: 0.1698 - val_acc: 0.2825\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.3099Epoch 00005: val_loss improved from 0.16855 to 0.16445, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1472 - acc: 0.3101 - val_loss: 0.1645 - val_acc: 0.2840\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.3111Epoch 00006: val_loss improved from 0.16445 to 0.16429, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 902us/step - loss: 0.1452 - acc: 0.3112 - val_loss: 0.1643 - val_acc: 0.2826\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.3156Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1339 - acc: 0.3156 - val_loss: 0.1927 - val_acc: 0.2810\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.3140Epoch 00008: val_loss improved from 0.16429 to 0.15107, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1321 - acc: 0.3141 - val_loss: 0.1511 - val_acc: 0.2878\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.3155Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1321 - acc: 0.3158 - val_loss: 0.1650 - val_acc: 0.2815\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.3184Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.1217 - acc: 0.3184 - val_loss: 0.1593 - val_acc: 0.2858\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.3216Epoch 00011: val_loss improved from 0.15107 to 0.14874, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1185 - acc: 0.3215 - val_loss: 0.1487 - val_acc: 0.2886\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.3204Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1186 - acc: 0.3204 - val_loss: 0.1589 - val_acc: 0.2849\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.3218Epoch 00013: val_loss improved from 0.14874 to 0.14846, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1149 - acc: 0.3218 - val_loss: 0.1485 - val_acc: 0.2882\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.3225Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1127 - acc: 0.3225 - val_loss: 0.1540 - val_acc: 0.2843\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.3239Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.1108 - acc: 0.3239 - val_loss: 0.1581 - val_acc: 0.2861\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.3234Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1100 - acc: 0.3235 - val_loss: 0.1540 - val_acc: 0.2874\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.3288Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0972 - acc: 0.3286 - val_loss: 0.1591 - val_acc: 0.2872\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.3301Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.0925 - acc: 0.3301 - val_loss: 0.1516 - val_acc: 0.2883\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.3311Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0889 - acc: 0.3312 - val_loss: 0.1485 - val_acc: 0.2884\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.3324Epoch 00020: val_loss improved from 0.14846 to 0.14834, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0877 - acc: 0.3323 - val_loss: 0.1483 - val_acc: 0.2887\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.3318Epoch 00021: val_loss improved from 0.14834 to 0.14542, saving model to ./models/model16/model17.h5\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.0879 - acc: 0.3317 - val_loss: 0.1454 - val_acc: 0.2887\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.3325Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.0862 - acc: 0.3323 - val_loss: 0.1491 - val_acc: 0.2888\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.3327Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0863 - acc: 0.3326 - val_loss: 0.1477 - val_acc: 0.2886\n",
      "Epoch 24/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.3321Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0871 - acc: 0.3321 - val_loss: 0.1473 - val_acc: 0.2894\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.3334Epoch 00025: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0848 - acc: 0.3337 - val_loss: 0.1476 - val_acc: 0.2887\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.3335Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.0848 - acc: 0.3334 - val_loss: 0.1470 - val_acc: 0.2888\n",
      "Epoch 27/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.3329Epoch 00027: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.0851 - acc: 0.3329 - val_loss: 0.1479 - val_acc: 0.2890\n",
      "Epoch 28/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.3327Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.0845 - acc: 0.3327 - val_loss: 0.1476 - val_acc: 0.2888\n",
      "Epoch 29/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.3331Epoch 00029: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.0843 - acc: 0.3329 - val_loss: 0.1475 - val_acc: 0.2888\n",
      "Epoch 30/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.3331Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0848 - acc: 0.3330 - val_loss: 0.1478 - val_acc: 0.2887\n",
      "Epoch 31/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.3331Epoch 00031: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0842 - acc: 0.3334 - val_loss: 0.1478 - val_acc: 0.2887\n",
      "Epoch 00031: early stopping\n",
      "*** MODEL: 18  ***\n",
      "train_i[:5] [4198, 270, 65, 348, 2385]\n",
      "val_i[:5] [2459, 4081, 3565, 3411, 2488]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.421280392512\n",
      "np.mean(y_val): 0.438948048274\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.2678Epoch 00001: val_loss improved from inf to 0.23313, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 17s 976us/step - loss: 0.3422 - acc: 0.2678 - val_loss: 0.2331 - val_acc: 0.2780\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.3013Epoch 00002: val_loss improved from 0.23313 to 0.15076, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1834 - acc: 0.3011 - val_loss: 0.1508 - val_acc: 0.2874\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.3071Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1660 - acc: 0.3070 - val_loss: 0.1625 - val_acc: 0.2825\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.3098Epoch 00004: val_loss improved from 0.15076 to 0.14502, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1603 - acc: 0.3097 - val_loss: 0.1450 - val_acc: 0.2874\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.3142Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1468 - acc: 0.3144 - val_loss: 0.1505 - val_acc: 0.2855\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.3152Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.1411 - acc: 0.3149 - val_loss: 0.1458 - val_acc: 0.2848\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.3160Epoch 00007: val_loss improved from 0.14502 to 0.13770, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1396 - acc: 0.3160 - val_loss: 0.1377 - val_acc: 0.2903\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.3194Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1289 - acc: 0.3193 - val_loss: 0.1745 - val_acc: 0.2806\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.3194Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1319 - acc: 0.3194 - val_loss: 0.1477 - val_acc: 0.2876\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.3210Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1247 - acc: 0.3208 - val_loss: 0.1422 - val_acc: 0.2899\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.3262Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1085 - acc: 0.3262 - val_loss: 0.1455 - val_acc: 0.2890\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.3286Epoch 00012: val_loss improved from 0.13770 to 0.13625, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1034 - acc: 0.3284 - val_loss: 0.1362 - val_acc: 0.2923\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.3278Epoch 00013: val_loss improved from 0.13625 to 0.13285, saving model to ./models/model16/model18.h5\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1020 - acc: 0.3277 - val_loss: 0.1328 - val_acc: 0.2928\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.3298Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 905us/step - loss: 0.0993 - acc: 0.3299 - val_loss: 0.1339 - val_acc: 0.2917\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.3308Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 898us/step - loss: 0.0975 - acc: 0.3308 - val_loss: 0.1425 - val_acc: 0.2909\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.3316Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.0960 - acc: 0.3315 - val_loss: 0.1400 - val_acc: 0.2911\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.3325Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 903us/step - loss: 0.0918 - acc: 0.3324 - val_loss: 0.1368 - val_acc: 0.2921\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.3336Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0903 - acc: 0.3337 - val_loss: 0.1370 - val_acc: 0.2922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.3338Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.0891 - acc: 0.3339 - val_loss: 0.1407 - val_acc: 0.2920\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.3337Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.0881 - acc: 0.3339 - val_loss: 0.1449 - val_acc: 0.2915\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.3342Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.0876 - acc: 0.3338 - val_loss: 0.1410 - val_acc: 0.2921\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.3349Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.0868 - acc: 0.3352 - val_loss: 0.1396 - val_acc: 0.2918\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.3338Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.0881 - acc: 0.3335 - val_loss: 0.1389 - val_acc: 0.2922\n",
      "Epoch 00023: early stopping\n",
      "*** MODEL: 19  ***\n",
      "train_i[:5] [1258, 3838, 4789, 4264, 256]\n",
      "val_i[:5] [3297, 4350, 2422, 1407, 3151]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.439810358949\n",
      "np.mean(y_val): 0.420410558506\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.2644Epoch 00001: val_loss improved from inf to 0.17674, saving model to ./models/model16/model19.h5\n",
      "17248/17248 [==============================] - 17s 974us/step - loss: 0.3200 - acc: 0.2643 - val_loss: 0.1767 - val_acc: 0.2887\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.2919Epoch 00002: val_loss improved from 0.17674 to 0.14863, saving model to ./models/model16/model19.h5\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1953 - acc: 0.2919 - val_loss: 0.1486 - val_acc: 0.2939\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.2961Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1699 - acc: 0.2959 - val_loss: 0.1692 - val_acc: 0.2890\n",
      "Epoch 4/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.2975Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1605 - acc: 0.2973 - val_loss: 0.1623 - val_acc: 0.2895\n",
      "Epoch 5/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.3020Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.1487 - acc: 0.3022 - val_loss: 0.1691 - val_acc: 0.2894\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.3093Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.1257 - acc: 0.3093 - val_loss: 0.1533 - val_acc: 0.2942\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.3099Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.1221 - acc: 0.3099 - val_loss: 0.1572 - val_acc: 0.2935\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.3127Epoch 00008: val_loss improved from 0.14863 to 0.14729, saving model to ./models/model16/model19.h5\n",
      "17248/17248 [==============================] - 16s 934us/step - loss: 0.1148 - acc: 0.3126 - val_loss: 0.1473 - val_acc: 0.2959\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.3146Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 909us/step - loss: 0.1111 - acc: 0.3143 - val_loss: 0.1495 - val_acc: 0.2955\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.3144Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1114 - acc: 0.3144 - val_loss: 0.1520 - val_acc: 0.2957\n",
      "Epoch 11/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.3152Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1079 - acc: 0.3154 - val_loss: 0.1521 - val_acc: 0.2949\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.3160Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.1046 - acc: 0.3159 - val_loss: 0.1546 - val_acc: 0.2946\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.3176Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1044 - acc: 0.3177 - val_loss: 0.1589 - val_acc: 0.2937\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.3177Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1022 - acc: 0.3178 - val_loss: 0.1506 - val_acc: 0.2957\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.3169Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 899us/step - loss: 0.1024 - acc: 0.3170 - val_loss: 0.1523 - val_acc: 0.2956\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.3174Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1031 - acc: 0.3173 - val_loss: 0.1553 - val_acc: 0.2948\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.3179Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.1027 - acc: 0.3178 - val_loss: 0.1563 - val_acc: 0.2947\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.3171Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 896us/step - loss: 0.1033 - acc: 0.3173 - val_loss: 0.1553 - val_acc: 0.2949\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 20  ***\n",
      "train_i[:5] [24, 2131, 322, 1919, 3568]\n",
      "val_i[:5] [3354, 3646, 4736, 4899, 2023]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.435303412845\n",
      "np.mean(y_val): 0.42491933447\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.2550Epoch 00001: val_loss improved from inf to 0.23384, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 17s 969us/step - loss: 0.3530 - acc: 0.2550 - val_loss: 0.2338 - val_acc: 0.2772\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.2875Epoch 00002: val_loss improved from 0.23384 to 0.17676, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1971 - acc: 0.2874 - val_loss: 0.1768 - val_acc: 0.2847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.2931Epoch 00003: val_loss improved from 0.17676 to 0.16135, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1765 - acc: 0.2933 - val_loss: 0.1613 - val_acc: 0.2920\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.2965Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1612 - acc: 0.2965 - val_loss: 0.1660 - val_acc: 0.2937\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.3009Epoch 00005: val_loss improved from 0.16135 to 0.14202, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.1506 - acc: 0.3007 - val_loss: 0.1420 - val_acc: 0.2975\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.3022Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 911us/step - loss: 0.1464 - acc: 0.3023 - val_loss: 0.1623 - val_acc: 0.2909\n",
      "Epoch 7/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.3030Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.1427 - acc: 0.3029 - val_loss: 0.1460 - val_acc: 0.2984\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.3054Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.1365 - acc: 0.3055 - val_loss: 0.1479 - val_acc: 0.2952\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.3110Epoch 00009: val_loss improved from 0.14202 to 0.14043, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1178 - acc: 0.3111 - val_loss: 0.1404 - val_acc: 0.3001\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.3131Epoch 00010: val_loss improved from 0.14043 to 0.13739, saving model to ./models/model16/model20.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1126 - acc: 0.3130 - val_loss: 0.1374 - val_acc: 0.2994\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.3134Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1104 - acc: 0.3132 - val_loss: 0.1418 - val_acc: 0.2980\n",
      "Epoch 12/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.3152Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1068 - acc: 0.3152 - val_loss: 0.1496 - val_acc: 0.2965\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.3159Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1070 - acc: 0.3162 - val_loss: 0.1624 - val_acc: 0.2932\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.3181Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1001 - acc: 0.3181 - val_loss: 0.1485 - val_acc: 0.2978\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.3195Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.0991 - acc: 0.3195 - val_loss: 0.1493 - val_acc: 0.2979\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.3198Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.0959 - acc: 0.3196 - val_loss: 0.1506 - val_acc: 0.2975\n",
      "Epoch 17/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.3199Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 930us/step - loss: 0.0938 - acc: 0.3197 - val_loss: 0.1515 - val_acc: 0.2968\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3210Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.0934 - acc: 0.3210 - val_loss: 0.1534 - val_acc: 0.2965\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3192Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0960 - acc: 0.3194 - val_loss: 0.1442 - val_acc: 0.2983\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.3203Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0942 - acc: 0.3206 - val_loss: 0.1512 - val_acc: 0.2973\n",
      "Epoch 00020: early stopping\n",
      "*** MODEL: 21  ***\n",
      "train_i[:5] [3993, 1172, 2538, 4410, 2529]\n",
      "val_i[:5] [650, 1110, 710, 4058, 3372]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.42336208196\n",
      "np.mean(y_val): 0.436865513642\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.2768Epoch 00001: val_loss improved from inf to 0.18457, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 17s 985us/step - loss: 0.3345 - acc: 0.2770 - val_loss: 0.1846 - val_acc: 0.2664\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.3074Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1846 - acc: 0.3075 - val_loss: 0.1869 - val_acc: 0.2675\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.3145Epoch 00003: val_loss improved from 0.18457 to 0.16133, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1656 - acc: 0.3144 - val_loss: 0.1613 - val_acc: 0.2758\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.3161Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1567 - acc: 0.3162 - val_loss: 0.1981 - val_acc: 0.2638\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.3174Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1510 - acc: 0.3174 - val_loss: 0.1711 - val_acc: 0.2740\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.3207Epoch 00006: val_loss improved from 0.16133 to 0.15421, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 16s 904us/step - loss: 0.1392 - acc: 0.3204 - val_loss: 0.1542 - val_acc: 0.2775\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.3212Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1377 - acc: 0.3213 - val_loss: 0.1641 - val_acc: 0.2748\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.3231Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1312 - acc: 0.3231 - val_loss: 0.1647 - val_acc: 0.2745\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.3244Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 938us/step - loss: 0.1332 - acc: 0.3242 - val_loss: 0.1570 - val_acc: 0.2782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.3284Epoch 00010: val_loss improved from 0.15421 to 0.14348, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 16s 941us/step - loss: 0.1144 - acc: 0.3283 - val_loss: 0.1435 - val_acc: 0.2795\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.3304Epoch 00011: val_loss improved from 0.14348 to 0.14046, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1113 - acc: 0.3302 - val_loss: 0.1405 - val_acc: 0.2808\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.3316Epoch 00012: val_loss improved from 0.14046 to 0.13652, saving model to ./models/model16/model21.h5\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1065 - acc: 0.3316 - val_loss: 0.1365 - val_acc: 0.2830\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.3321Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1063 - acc: 0.3322 - val_loss: 0.1447 - val_acc: 0.2806\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.3334Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 934us/step - loss: 0.1033 - acc: 0.3334 - val_loss: 0.1378 - val_acc: 0.2811\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.3341Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1025 - acc: 0.3342 - val_loss: 0.1425 - val_acc: 0.2817\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.3350Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.0968 - acc: 0.3351 - val_loss: 0.1410 - val_acc: 0.2819\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.3370Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.0937 - acc: 0.3370 - val_loss: 0.1444 - val_acc: 0.2806\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.3367Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.0930 - acc: 0.3369 - val_loss: 0.1491 - val_acc: 0.2794\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.3374Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.0934 - acc: 0.3374 - val_loss: 0.1449 - val_acc: 0.2806\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.3383Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0906 - acc: 0.3382 - val_loss: 0.1447 - val_acc: 0.2807\n",
      "Epoch 21/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.3378Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.0914 - acc: 0.3377 - val_loss: 0.1451 - val_acc: 0.2804\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.3381Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.0915 - acc: 0.3379 - val_loss: 0.1470 - val_acc: 0.2800\n",
      "Epoch 00022: early stopping\n",
      "*** MODEL: 22  ***\n",
      "train_i[:5] [1102, 2041, 3939, 265, 4222]\n",
      "val_i[:5] [4675, 901, 1164, 2808, 1511]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.424308717472\n",
      "np.mean(y_val): 0.435918493788\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.2643Epoch 00001: val_loss improved from inf to 0.22784, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 17s 982us/step - loss: 0.3224 - acc: 0.2644 - val_loss: 0.2278 - val_acc: 0.2756\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.2918Epoch 00002: val_loss improved from 0.22784 to 0.16190, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1948 - acc: 0.2916 - val_loss: 0.1619 - val_acc: 0.2886\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.2989Epoch 00003: val_loss improved from 0.16190 to 0.16018, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1669 - acc: 0.2989 - val_loss: 0.1602 - val_acc: 0.2894\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.3018Epoch 00004: val_loss improved from 0.16018 to 0.15515, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.1540 - acc: 0.3019 - val_loss: 0.1552 - val_acc: 0.2903\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.3045Epoch 00005: val_loss improved from 0.15515 to 0.14946, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1426 - acc: 0.3042 - val_loss: 0.1495 - val_acc: 0.2911\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.3067Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 907us/step - loss: 0.1398 - acc: 0.3068 - val_loss: 0.1857 - val_acc: 0.2883\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.3090Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1361 - acc: 0.3090 - val_loss: 0.1558 - val_acc: 0.2905\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.3093Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 934us/step - loss: 0.1320 - acc: 0.3093 - val_loss: 0.1542 - val_acc: 0.2933\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.3161Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 900us/step - loss: 0.1127 - acc: 0.3161 - val_loss: 0.1513 - val_acc: 0.2942\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.3160Epoch 00010: val_loss improved from 0.14946 to 0.14221, saving model to ./models/model16/model22.h5\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.1095 - acc: 0.3163 - val_loss: 0.1422 - val_acc: 0.2956\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.3173Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1090 - acc: 0.3175 - val_loss: 0.1452 - val_acc: 0.2957\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.3184Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 908us/step - loss: 0.1051 - acc: 0.3185 - val_loss: 0.1480 - val_acc: 0.2939\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.3197Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.1031 - acc: 0.3199 - val_loss: 0.1449 - val_acc: 0.2956\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.3224Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 915us/step - loss: 0.0969 - acc: 0.3222 - val_loss: 0.1510 - val_acc: 0.2947\n",
      "Epoch 15/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.3222Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.0950 - acc: 0.3225 - val_loss: 0.1469 - val_acc: 0.2955\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.3240Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.0932 - acc: 0.3240 - val_loss: 0.1440 - val_acc: 0.2961\n",
      "Epoch 17/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.3229Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.0940 - acc: 0.3231 - val_loss: 0.1454 - val_acc: 0.2957\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.3256Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0914 - acc: 0.3254 - val_loss: 0.1495 - val_acc: 0.2948\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3235Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.0923 - acc: 0.3236 - val_loss: 0.1477 - val_acc: 0.2956\n",
      "Epoch 20/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.3252Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.0907 - acc: 0.3251 - val_loss: 0.1486 - val_acc: 0.2950\n",
      "Epoch 00020: early stopping\n",
      "*** MODEL: 23  ***\n",
      "train_i[:5] [2594, 2140, 4103, 793, 4844]\n",
      "val_i[:5] [3393, 1262, 694, 1143, 4339]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.427405752963\n",
      "np.mean(y_val): 0.432820200873\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.2599Epoch 00001: val_loss improved from inf to 0.24101, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 17s 979us/step - loss: 0.3356 - acc: 0.2599 - val_loss: 0.2410 - val_acc: 0.2803\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.2936Epoch 00002: val_loss improved from 0.24101 to 0.17483, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1756 - acc: 0.2933 - val_loss: 0.1748 - val_acc: 0.2898\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.2963Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 936us/step - loss: 0.1602 - acc: 0.2964 - val_loss: 0.2022 - val_acc: 0.2874\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.3005Epoch 00004: val_loss improved from 0.17483 to 0.17034, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.1513 - acc: 0.3004 - val_loss: 0.1703 - val_acc: 0.2904\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.3033Epoch 00005: val_loss improved from 0.17034 to 0.16614, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1410 - acc: 0.3033 - val_loss: 0.1661 - val_acc: 0.2902\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.3073Epoch 00006: val_loss improved from 0.16614 to 0.16417, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1302 - acc: 0.3072 - val_loss: 0.1642 - val_acc: 0.2916\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.3073Epoch 00007: val_loss improved from 0.16417 to 0.14459, saving model to ./models/model16/model23.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1309 - acc: 0.3073 - val_loss: 0.1446 - val_acc: 0.2947\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.3086Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1273 - acc: 0.3086 - val_loss: 0.1818 - val_acc: 0.2896\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.3114Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1199 - acc: 0.3112 - val_loss: 0.1607 - val_acc: 0.2930\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.3127Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 941us/step - loss: 0.1159 - acc: 0.3126 - val_loss: 0.1509 - val_acc: 0.2955\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.3156Epoch 00011: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1028 - acc: 0.3157 - val_loss: 0.1588 - val_acc: 0.2951\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.3181Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.0982 - acc: 0.3183 - val_loss: 0.1709 - val_acc: 0.2931\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.3197Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.0929 - acc: 0.3195 - val_loss: 0.1522 - val_acc: 0.2957\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.3200Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.0921 - acc: 0.3199 - val_loss: 0.1533 - val_acc: 0.2957\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.3210Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0896 - acc: 0.3210 - val_loss: 0.1572 - val_acc: 0.2956\n",
      "Epoch 16/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.3223Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0883 - acc: 0.3221 - val_loss: 0.1517 - val_acc: 0.2947\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.3209Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.0902 - acc: 0.3208 - val_loss: 0.1525 - val_acc: 0.2953\n",
      "Epoch 00017: early stopping\n",
      "*** MODEL: 24  ***\n",
      "train_i[:5] [3518, 4042, 340, 4624, 4707]\n",
      "val_i[:5] [3317, 1569, 307, 372, 4522]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.420841953571\n",
      "np.mean(y_val): 0.439386665225\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.2559Epoch 00001: val_loss improved from inf to 0.17739, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 17s 1ms/step - loss: 0.3655 - acc: 0.2562 - val_loss: 0.1774 - val_acc: 0.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.2897Epoch 00002: val_loss improved from 0.17739 to 0.17241, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 16s 936us/step - loss: 0.1996 - acc: 0.2899 - val_loss: 0.1724 - val_acc: 0.2883\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.2984Epoch 00003: val_loss improved from 0.17241 to 0.15534, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.1725 - acc: 0.2984 - val_loss: 0.1553 - val_acc: 0.2883\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.2995Epoch 00004: val_loss improved from 0.15534 to 0.15331, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1581 - acc: 0.2995 - val_loss: 0.1533 - val_acc: 0.2909\n",
      "Epoch 5/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.3035Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 950us/step - loss: 0.1538 - acc: 0.3034 - val_loss: 0.1591 - val_acc: 0.2901\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.3045Epoch 00006: val_loss improved from 0.15331 to 0.13954, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 16s 930us/step - loss: 0.1423 - acc: 0.3044 - val_loss: 0.1395 - val_acc: 0.2948\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.3065Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1391 - acc: 0.3065 - val_loss: 0.1453 - val_acc: 0.2916\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.3095Epoch 00008: val_loss improved from 0.13954 to 0.13573, saving model to ./models/model16/model24.h5\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.1345 - acc: 0.3095 - val_loss: 0.1357 - val_acc: 0.2973\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.3118Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 944us/step - loss: 0.1264 - acc: 0.3119 - val_loss: 0.1464 - val_acc: 0.2909\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.3124Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.1281 - acc: 0.3124 - val_loss: 0.1394 - val_acc: 0.2940\n",
      "Epoch 11/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.3126Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 17s 960us/step - loss: 0.1233 - acc: 0.3127 - val_loss: 0.1590 - val_acc: 0.2897\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.3173Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.1088 - acc: 0.3173 - val_loss: 0.1432 - val_acc: 0.2927\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.3190Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.1070 - acc: 0.3190 - val_loss: 0.1405 - val_acc: 0.2935\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.3218Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.0990 - acc: 0.3217 - val_loss: 0.1378 - val_acc: 0.2940\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.3226Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.0977 - acc: 0.3226 - val_loss: 0.1484 - val_acc: 0.2917\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.3232Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 940us/step - loss: 0.0958 - acc: 0.3231 - val_loss: 0.1425 - val_acc: 0.2931\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3237Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 938us/step - loss: 0.0943 - acc: 0.3237 - val_loss: 0.1431 - val_acc: 0.2932\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.3244Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 941us/step - loss: 0.0936 - acc: 0.3243 - val_loss: 0.1458 - val_acc: 0.2924\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 25  ***\n",
      "train_i[:5] [102, 2788, 2760, 1180, 3433]\n",
      "val_i[:5] [1275, 638, 513, 2240, 3986]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.424195271449\n",
      "np.mean(y_val): 0.436031985871\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.2584Epoch 00001: val_loss improved from inf to 0.24190, saving model to ./models/model16/model25.h5\n",
      "17248/17248 [==============================] - 17s 995us/step - loss: 0.3284 - acc: 0.2587 - val_loss: 0.2419 - val_acc: 0.2824\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.2865Epoch 00002: val_loss improved from 0.24190 to 0.15946, saving model to ./models/model16/model25.h5\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1922 - acc: 0.2865 - val_loss: 0.1595 - val_acc: 0.2974\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.2936Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.1652 - acc: 0.2937 - val_loss: 0.1887 - val_acc: 0.2927\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.2950Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 930us/step - loss: 0.1599 - acc: 0.2950 - val_loss: 0.1604 - val_acc: 0.2986\n",
      "Epoch 5/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.2988Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 957us/step - loss: 0.1479 - acc: 0.2987 - val_loss: 0.1943 - val_acc: 0.2935\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.3044Epoch 00006: val_loss improved from 0.15946 to 0.15879, saving model to ./models/model16/model25.h5\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.1239 - acc: 0.3046 - val_loss: 0.1588 - val_acc: 0.2977\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.3065Epoch 00007: val_loss improved from 0.15879 to 0.14970, saving model to ./models/model16/model25.h5\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.1199 - acc: 0.3064 - val_loss: 0.1497 - val_acc: 0.2985\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.3082Epoch 00008: val_loss improved from 0.14970 to 0.13830, saving model to ./models/model16/model25.h5\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.1178 - acc: 0.3080 - val_loss: 0.1383 - val_acc: 0.3024\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.3105Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1117 - acc: 0.3106 - val_loss: 0.1425 - val_acc: 0.3014\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.3101Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 914us/step - loss: 0.1127 - acc: 0.3101 - val_loss: 0.1602 - val_acc: 0.2977\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.3122Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.1074 - acc: 0.3124 - val_loss: 0.1514 - val_acc: 0.3011\n",
      "Epoch 12/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.3148Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.1008 - acc: 0.3149 - val_loss: 0.1530 - val_acc: 0.3000\n",
      "Epoch 13/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.3156Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.0984 - acc: 0.3157 - val_loss: 0.1458 - val_acc: 0.3020\n",
      "Epoch 14/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.3159Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 17s 958us/step - loss: 0.0969 - acc: 0.3164 - val_loss: 0.1461 - val_acc: 0.3013\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.3163Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 943us/step - loss: 0.0958 - acc: 0.3163 - val_loss: 0.1434 - val_acc: 0.3020\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.3165Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 930us/step - loss: 0.0943 - acc: 0.3167 - val_loss: 0.1510 - val_acc: 0.2995\n",
      "Epoch 17/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.3166Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 934us/step - loss: 0.0959 - acc: 0.3166 - val_loss: 0.1458 - val_acc: 0.3013\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.3176Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.0936 - acc: 0.3173 - val_loss: 0.1467 - val_acc: 0.3012\n",
      "Epoch 00018: early stopping\n",
      "*** MODEL: 26  ***\n",
      "train_i[:5] [3890, 4650, 3792, 894, 271]\n",
      "val_i[:5] [1845, 1541, 1942, 2225, 4917]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.448344310045\n",
      "np.mean(y_val): 0.41187314255\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.2580Epoch 00001: val_loss improved from inf to 0.17012, saving model to ./models/model16/model26.h5\n",
      "17248/17248 [==============================] - 17s 1ms/step - loss: 0.3353 - acc: 0.2580 - val_loss: 0.1701 - val_acc: 0.2906\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.2903Epoch 00002: val_loss improved from 0.17012 to 0.14506, saving model to ./models/model16/model26.h5\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1914 - acc: 0.2903 - val_loss: 0.1451 - val_acc: 0.2945\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.2973Epoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.1706 - acc: 0.2971 - val_loss: 0.1712 - val_acc: 0.2894\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.2989Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 938us/step - loss: 0.1591 - acc: 0.2989 - val_loss: 0.1799 - val_acc: 0.2883\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.3010Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 17s 962us/step - loss: 0.1536 - acc: 0.3009 - val_loss: 0.1667 - val_acc: 0.2888\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.3080Epoch 00006: val_loss improved from 0.14506 to 0.14484, saving model to ./models/model16/model26.h5\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1309 - acc: 0.3080 - val_loss: 0.1448 - val_acc: 0.2948\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.3083Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1285 - acc: 0.3081 - val_loss: 0.1458 - val_acc: 0.2952\n",
      "Epoch 8/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.3093Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.1266 - acc: 0.3092 - val_loss: 0.1531 - val_acc: 0.2928\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.3111Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.1220 - acc: 0.3112 - val_loss: 0.1585 - val_acc: 0.2913\n",
      "Epoch 10/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.3136Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.1149 - acc: 0.3135 - val_loss: 0.1479 - val_acc: 0.2944\n",
      "Epoch 11/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.3153Epoch 00011: val_loss improved from 0.14484 to 0.14064, saving model to ./models/model16/model26.h5\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.1124 - acc: 0.3155 - val_loss: 0.1406 - val_acc: 0.2968\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.3157Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 910us/step - loss: 0.1097 - acc: 0.3156 - val_loss: 0.1493 - val_acc: 0.2939\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.3159Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 943us/step - loss: 0.1080 - acc: 0.3158 - val_loss: 0.1418 - val_acc: 0.2962\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.3155Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1090 - acc: 0.3153 - val_loss: 0.1552 - val_acc: 0.2920\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.3159Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1061 - acc: 0.3162 - val_loss: 0.1451 - val_acc: 0.2952\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.3179Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1042 - acc: 0.3179 - val_loss: 0.1485 - val_acc: 0.2936\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.3184Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1020 - acc: 0.3185 - val_loss: 0.1465 - val_acc: 0.2946\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.3183Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1033 - acc: 0.3181 - val_loss: 0.1464 - val_acc: 0.2943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.3184Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1024 - acc: 0.3184 - val_loss: 0.1482 - val_acc: 0.2938\n",
      "Epoch 20/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.3188Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 16s 947us/step - loss: 0.1003 - acc: 0.3189 - val_loss: 0.1472 - val_acc: 0.2942\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.3191Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1023 - acc: 0.3192 - val_loss: 0.1474 - val_acc: 0.2941\n",
      "Epoch 00021: early stopping\n",
      "*** MODEL: 27  ***\n",
      "train_i[:5] [408, 1649, 1811, 2669, 4904]\n",
      "val_i[:5] [4053, 4343, 3463, 3264, 3632]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.435267229383\n",
      "np.mean(y_val): 0.424955532623\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.2614Epoch 00001: val_loss improved from inf to 0.19198, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 17s 1ms/step - loss: 0.3347 - acc: 0.2615 - val_loss: 0.1920 - val_acc: 0.2860\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.2913Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1907 - acc: 0.2910 - val_loss: 0.2369 - val_acc: 0.2723\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.2988Epoch 00003: val_loss improved from 0.19198 to 0.16522, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.1698 - acc: 0.2991 - val_loss: 0.1652 - val_acc: 0.2905\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.3013Epoch 00004: val_loss improved from 0.16522 to 0.14911, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 16s 923us/step - loss: 0.1505 - acc: 0.3013 - val_loss: 0.1491 - val_acc: 0.2939\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.3045Epoch 00005: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.1469 - acc: 0.3043 - val_loss: 0.1631 - val_acc: 0.2913\n",
      "Epoch 6/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.3044Epoch 00006: val_loss improved from 0.14911 to 0.14806, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 16s 941us/step - loss: 0.1460 - acc: 0.3046 - val_loss: 0.1481 - val_acc: 0.2948\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.3065Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.1332 - acc: 0.3065 - val_loss: 0.1859 - val_acc: 0.2873\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.3082Epoch 00008: val_loss improved from 0.14806 to 0.14129, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.1289 - acc: 0.3080 - val_loss: 0.1413 - val_acc: 0.2968\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.3101Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.1254 - acc: 0.3102 - val_loss: 0.1455 - val_acc: 0.2944\n",
      "Epoch 10/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.3123Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1196 - acc: 0.3120 - val_loss: 0.1850 - val_acc: 0.2880\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.3142Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 16s 948us/step - loss: 0.1159 - acc: 0.3142 - val_loss: 0.1434 - val_acc: 0.2973\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.3187Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.1016 - acc: 0.3189 - val_loss: 0.1563 - val_acc: 0.2926\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.3192Epoch 00013: val_loss improved from 0.14129 to 0.14051, saving model to ./models/model16/model27.h5\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.0980 - acc: 0.3193 - val_loss: 0.1405 - val_acc: 0.2981\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.3203Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 916us/step - loss: 0.0980 - acc: 0.3205 - val_loss: 0.1648 - val_acc: 0.2925\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.3203Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 913us/step - loss: 0.0962 - acc: 0.3203 - val_loss: 0.1465 - val_acc: 0.2964\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.3217Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 917us/step - loss: 0.0928 - acc: 0.3220 - val_loss: 0.1472 - val_acc: 0.2957\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.3232Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.0879 - acc: 0.3232 - val_loss: 0.1471 - val_acc: 0.2962\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.3245Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 924us/step - loss: 0.0867 - acc: 0.3243 - val_loss: 0.1545 - val_acc: 0.2947\n",
      "Epoch 19/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.3245Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 935us/step - loss: 0.0854 - acc: 0.3248 - val_loss: 0.1498 - val_acc: 0.2947\n",
      "Epoch 20/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.3255Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.0854 - acc: 0.3253 - val_loss: 0.1485 - val_acc: 0.2957\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.3246Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 926us/step - loss: 0.0847 - acc: 0.3244 - val_loss: 0.1528 - val_acc: 0.2946\n",
      "Epoch 22/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.3255Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.0846 - acc: 0.3251 - val_loss: 0.1517 - val_acc: 0.2951\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.3245Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 922us/step - loss: 0.0850 - acc: 0.3245 - val_loss: 0.1523 - val_acc: 0.2949\n",
      "Epoch 00023: early stopping\n",
      "*** MODEL: 28  ***\n",
      "train_i[:5] [2311, 3513, 3393, 2499, 4778]\n",
      "val_i[:5] [4656, 2071, 1490, 4647, 1522]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.420444359233\n",
      "np.mean(y_val): 0.439784420991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.2495Epoch 00001: val_loss improved from inf to 0.17293, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 18s 1ms/step - loss: 0.3434 - acc: 0.2498 - val_loss: 0.1729 - val_acc: 0.3012\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.2835Epoch 00002: val_loss improved from 0.17293 to 0.15718, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 928us/step - loss: 0.1756 - acc: 0.2835 - val_loss: 0.1572 - val_acc: 0.3032\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.2875- EEpoch 00003: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.1560 - acc: 0.2873 - val_loss: 0.1664 - val_acc: 0.3038\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.2904Epoch 00004: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 931us/step - loss: 0.1476 - acc: 0.2904 - val_loss: 0.1692 - val_acc: 0.2983\n",
      "Epoch 5/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.2914Epoch 00005: val_loss improved from 0.15718 to 0.15301, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 942us/step - loss: 0.1457 - acc: 0.2916 - val_loss: 0.1530 - val_acc: 0.3068\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.2944Epoch 00006: val_loss improved from 0.15301 to 0.14575, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.1322 - acc: 0.2945 - val_loss: 0.1457 - val_acc: 0.3072\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.2969Epoch 00007: val_loss improved from 0.14575 to 0.14289, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 925us/step - loss: 0.1286 - acc: 0.2969 - val_loss: 0.1429 - val_acc: 0.3087\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.2990Epoch 00008: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.1228 - acc: 0.2990 - val_loss: 0.1650 - val_acc: 0.3053\n",
      "Epoch 9/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.3010Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.1176 - acc: 0.3009 - val_loss: 0.1595 - val_acc: 0.3046\n",
      "Epoch 10/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.3026Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 17s 980us/step - loss: 0.1160 - acc: 0.3029 - val_loss: 0.1580 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.3066Epoch 00011: val_loss improved from 0.14289 to 0.14189, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 918us/step - loss: 0.1024 - acc: 0.3064 - val_loss: 0.1419 - val_acc: 0.3081\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.3080Epoch 00012: val_loss improved from 0.14189 to 0.14183, saving model to ./models/model16/model28.h5\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.0988 - acc: 0.3082 - val_loss: 0.1418 - val_acc: 0.3086\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.3089- ETA: 1Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 937us/step - loss: 0.0966 - acc: 0.3093 - val_loss: 0.1590 - val_acc: 0.3049\n",
      "Epoch 14/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.3116Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 16s 920us/step - loss: 0.0932 - acc: 0.3111 - val_loss: 0.1597 - val_acc: 0.3037\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.3120Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 927us/step - loss: 0.0892 - acc: 0.3120 - val_loss: 0.1483 - val_acc: 0.3069\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.3128Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 16s 939us/step - loss: 0.0878 - acc: 0.3130 - val_loss: 0.1540 - val_acc: 0.3057\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.3131Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 942us/step - loss: 0.0867 - acc: 0.3131 - val_loss: 0.1571 - val_acc: 0.3046\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.3135Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 16s 933us/step - loss: 0.0860 - acc: 0.3138 - val_loss: 0.1542 - val_acc: 0.3055\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.3139Epoch 00019: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 921us/step - loss: 0.0862 - acc: 0.3138 - val_loss: 0.1529 - val_acc: 0.3062\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.3141Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 16s 932us/step - loss: 0.0851 - acc: 0.3141 - val_loss: 0.1525 - val_acc: 0.3061\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.3140Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 844us/step - loss: 0.0860 - acc: 0.3141 - val_loss: 0.1541 - val_acc: 0.3056\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.3146Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 14s 829us/step - loss: 0.0838 - acc: 0.3147 - val_loss: 0.1550 - val_acc: 0.3051\n",
      "Epoch 00022: early stopping\n",
      "*** MODEL: 29  ***\n",
      "train_i[:5] [813, 4343, 4107, 2903, 1352]\n",
      "val_i[:5] [2343, 3806, 3206, 3794, 157]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.429300319623\n",
      "np.mean(y_val): 0.430924865002\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.2524Epoch 00001: val_loss improved from inf to 0.20562, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 16s 929us/step - loss: 0.3623 - acc: 0.2522 - val_loss: 0.2056 - val_acc: 0.2784\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.2904Epoch 00002: val_loss improved from 0.20562 to 0.16650, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 15s 851us/step - loss: 0.1932 - acc: 0.2903 - val_loss: 0.1665 - val_acc: 0.2923\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.2946Epoch 00003: val_loss improved from 0.16650 to 0.15784, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 16s 919us/step - loss: 0.1773 - acc: 0.2945 - val_loss: 0.1578 - val_acc: 0.2964\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.2982Epoch 00004: val_loss improved from 0.15784 to 0.15071, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 15s 879us/step - loss: 0.1646 - acc: 0.2981 - val_loss: 0.1507 - val_acc: 0.2943\n",
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.2991Epoch 00005: val_loss improved from 0.15071 to 0.14920, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 15s 873us/step - loss: 0.1580 - acc: 0.2992 - val_loss: 0.1492 - val_acc: 0.2932\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.3010Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 889us/step - loss: 0.1487 - acc: 0.3014 - val_loss: 0.1509 - val_acc: 0.2964\n",
      "Epoch 7/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.3013Epoch 00007: val_loss did not improve\n",
      "17248/17248 [==============================] - 16s 906us/step - loss: 0.1500 - acc: 0.3017 - val_loss: 0.1685 - val_acc: 0.2948\n",
      "Epoch 8/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.3034Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 17s 963us/step - loss: 0.1409 - acc: 0.3029 - val_loss: 0.1589 - val_acc: 0.2908\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.3077Epoch 00009: val_loss improved from 0.14920 to 0.13732, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 14s 834us/step - loss: 0.1237 - acc: 0.3077 - val_loss: 0.1373 - val_acc: 0.2998\n",
      "Epoch 10/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.3098Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 897us/step - loss: 0.1189 - acc: 0.3094 - val_loss: 0.1377 - val_acc: 0.3000\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.3108Epoch 00011: val_loss improved from 0.13732 to 0.13390, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 15s 856us/step - loss: 0.1167 - acc: 0.3107 - val_loss: 0.1339 - val_acc: 0.3000\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.3122Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 829us/step - loss: 0.1145 - acc: 0.3121 - val_loss: 0.1364 - val_acc: 0.2988\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.3127Epoch 00013: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 794us/step - loss: 0.1119 - acc: 0.3130 - val_loss: 0.1374 - val_acc: 0.2993\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.3135Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 806us/step - loss: 0.1114 - acc: 0.3133 - val_loss: 0.1349 - val_acc: 0.3007\n",
      "Epoch 15/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.3164Epoch 00015: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 811us/step - loss: 0.1035 - acc: 0.3164 - val_loss: 0.1348 - val_acc: 0.3012\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.3175Epoch 00016: val_loss improved from 0.13390 to 0.13314, saving model to ./models/model16/model29.h5\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1019 - acc: 0.3173 - val_loss: 0.1331 - val_acc: 0.3000\n",
      "Epoch 17/200\n",
      "17152/17248 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.3183Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 888us/step - loss: 0.0995 - acc: 0.3183 - val_loss: 0.1380 - val_acc: 0.2999\n",
      "Epoch 18/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.3176Epoch 00018: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 801us/step - loss: 0.1013 - acc: 0.3173 - val_loss: 0.1386 - val_acc: 0.2986\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.3177Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 14s 832us/step - loss: 0.1001 - acc: 0.3177 - val_loss: 0.1364 - val_acc: 0.2998\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.3203Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 817us/step - loss: 0.0974 - acc: 0.3202 - val_loss: 0.1358 - val_acc: 0.2996\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.3198Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 812us/step - loss: 0.0958 - acc: 0.3200 - val_loss: 0.1365 - val_acc: 0.3003\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.3203Epoch 00022: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 803us/step - loss: 0.0960 - acc: 0.3203 - val_loss: 0.1356 - val_acc: 0.3004\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.3206Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 14s 835us/step - loss: 0.0956 - acc: 0.3204 - val_loss: 0.1350 - val_acc: 0.3008\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.3196Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 828us/step - loss: 0.0951 - acc: 0.3198 - val_loss: 0.1361 - val_acc: 0.3005\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.3203Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 15s 891us/step - loss: 0.0938 - acc: 0.3206 - val_loss: 0.1353 - val_acc: 0.3005\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.3199Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.0955 - acc: 0.3197 - val_loss: 0.1360 - val_acc: 0.3004\n",
      "Epoch 00026: early stopping\n",
      "*** MODEL: 30  ***\n",
      "train_i[:5] [737, 2287, 3751, 1251, 2026]\n",
      "val_i[:5] [3837, 3707, 3241, 4441, 2970]\n",
      "X_train.shape: (2464, 75, 75, 2)\n",
      "y_train.shape: (2464,)\n",
      "X_val.shape: (2463, 75, 75, 2)\n",
      "y_val.shape: (2463,)\n",
      "np.mean(y_train): 0.435572854789\n",
      "np.mean(y_val): 0.42464978313\n",
      "X_train.shape: (17248, 75, 75, 2)\n",
      "y_train.shape: (17248,)\n",
      "X_val.shape: (17241, 75, 75, 2)\n",
      "y_val.shape: (17241,)\n",
      "Train on 17248 samples, validate on 17241 samples\n",
      "Epoch 1/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.2513Epoch 00001: val_loss improved from inf to 0.19652, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 891us/step - loss: 0.3560 - acc: 0.2514 - val_loss: 0.1965 - val_acc: 0.2928\n",
      "Epoch 2/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.2861Epoch 00002: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 864us/step - loss: 0.1925 - acc: 0.2861 - val_loss: 0.1998 - val_acc: 0.2913\n",
      "Epoch 3/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.2900Epoch 00003: val_loss improved from 0.19652 to 0.16094, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 877us/step - loss: 0.1724 - acc: 0.2899 - val_loss: 0.1609 - val_acc: 0.2998\n",
      "Epoch 4/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.2926Epoch 00004: val_loss improved from 0.16094 to 0.15528, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 860us/step - loss: 0.1610 - acc: 0.2925 - val_loss: 0.1553 - val_acc: 0.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.2955Epoch 00005: val_loss improved from 0.15528 to 0.14924, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 887us/step - loss: 0.1501 - acc: 0.2955 - val_loss: 0.1492 - val_acc: 0.3014\n",
      "Epoch 6/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.2978Epoch 00006: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 866us/step - loss: 0.1450 - acc: 0.2980 - val_loss: 0.1574 - val_acc: 0.2979\n",
      "Epoch 7/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.2991Epoch 00007: val_loss improved from 0.14924 to 0.14018, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 899us/step - loss: 0.1415 - acc: 0.2990 - val_loss: 0.1402 - val_acc: 0.3029\n",
      "Epoch 8/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.3013Epoch 00008: val_loss improved from 0.14018 to 0.13910, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 858us/step - loss: 0.1314 - acc: 0.3014 - val_loss: 0.1391 - val_acc: 0.3044\n",
      "Epoch 9/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.3023Epoch 00009: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 824us/step - loss: 0.1305 - acc: 0.3025 - val_loss: 0.1606 - val_acc: 0.3018\n",
      "Epoch 10/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.3047Epoch 00010: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 794us/step - loss: 0.1261 - acc: 0.3048 - val_loss: 0.1461 - val_acc: 0.3020\n",
      "Epoch 11/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.3063Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000330000015674.\n",
      "17248/17248 [==============================] - 15s 881us/step - loss: 0.1194 - acc: 0.3061 - val_loss: 0.1471 - val_acc: 0.3011\n",
      "Epoch 12/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.3090Epoch 00012: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 844us/step - loss: 0.1055 - acc: 0.3093 - val_loss: 0.1518 - val_acc: 0.3018\n",
      "Epoch 13/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.3114Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000108900003252.\n",
      "17248/17248 [==============================] - 14s 836us/step - loss: 0.1038 - acc: 0.3114 - val_loss: 0.1451 - val_acc: 0.3039\n",
      "Epoch 14/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.3133Epoch 00014: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 859us/step - loss: 0.0968 - acc: 0.3133 - val_loss: 0.1405 - val_acc: 0.3053\n",
      "Epoch 15/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.3131Epoch 00015: val_loss improved from 0.13910 to 0.13880, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 852us/step - loss: 0.0978 - acc: 0.3134 - val_loss: 0.1388 - val_acc: 0.3054\n",
      "Epoch 16/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.3148Epoch 00016: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 853us/step - loss: 0.0949 - acc: 0.3148 - val_loss: 0.1422 - val_acc: 0.3047\n",
      "Epoch 17/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.3150Epoch 00017: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 871us/step - loss: 0.0938 - acc: 0.3149 - val_loss: 0.1394 - val_acc: 0.3051\n",
      "Epoch 18/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3154Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.59369999205e-05.\n",
      "17248/17248 [==============================] - 15s 892us/step - loss: 0.0923 - acc: 0.3156 - val_loss: 0.1394 - val_acc: 0.3046\n",
      "Epoch 19/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3158Epoch 00019: val_loss improved from 0.13880 to 0.13866, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 16s 912us/step - loss: 0.0923 - acc: 0.3159 - val_loss: 0.1387 - val_acc: 0.3052\n",
      "Epoch 20/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.3152Epoch 00020: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.0922 - acc: 0.3155 - val_loss: 0.1402 - val_acc: 0.3057\n",
      "Epoch 21/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.3157Epoch 00021: val_loss did not improve\n",
      "17248/17248 [==============================] - 15s 843us/step - loss: 0.0909 - acc: 0.3158 - val_loss: 0.1402 - val_acc: 0.3053\n",
      "Epoch 22/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.3173Epoch 00022: val_loss improved from 0.13866 to 0.13786, saving model to ./models/model16/model30.h5\n",
      "17248/17248 [==============================] - 15s 843us/step - loss: 0.0901 - acc: 0.3172 - val_loss: 0.1379 - val_acc: 0.3058\n",
      "Epoch 23/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.3160Epoch 00023: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 829us/step - loss: 0.0896 - acc: 0.3163 - val_loss: 0.1415 - val_acc: 0.3048\n",
      "Epoch 24/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.3154Epoch 00024: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 812us/step - loss: 0.0904 - acc: 0.3156 - val_loss: 0.1402 - val_acc: 0.3053\n",
      "Epoch 25/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.3166Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.18592095896e-05.\n",
      "17248/17248 [==============================] - 14s 811us/step - loss: 0.0891 - acc: 0.3168 - val_loss: 0.1381 - val_acc: 0.3057\n",
      "Epoch 26/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.3167Epoch 00026: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 800us/step - loss: 0.0896 - acc: 0.3168 - val_loss: 0.1417 - val_acc: 0.3049\n",
      "Epoch 27/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.3163Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 3.91353921259e-06.\n",
      "17248/17248 [==============================] - 15s 851us/step - loss: 0.0893 - acc: 0.3165 - val_loss: 0.1413 - val_acc: 0.3049\n",
      "Epoch 28/200\n",
      "17184/17248 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.3163Epoch 00028: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 834us/step - loss: 0.0895 - acc: 0.3164 - val_loss: 0.1408 - val_acc: 0.3054\n",
      "Epoch 29/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.3167Epoch 00029: val_loss did not improve\n",
      "\n",
      "Epoch 00029: reducing learning rate to 1.29146797917e-06.\n",
      "17248/17248 [==============================] - 15s 854us/step - loss: 0.0890 - acc: 0.3166 - val_loss: 0.1404 - val_acc: 0.3051\n",
      "Epoch 30/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.3162Epoch 00030: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 801us/step - loss: 0.0899 - acc: 0.3162 - val_loss: 0.1400 - val_acc: 0.3052\n",
      "Epoch 31/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.3171Epoch 00031: val_loss did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 4.26184415119e-07.\n",
      "17248/17248 [==============================] - 14s 804us/step - loss: 0.0884 - acc: 0.3170 - val_loss: 0.1403 - val_acc: 0.3052\n",
      "Epoch 32/200\n",
      "17216/17248 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.3183Epoch 00032: val_loss did not improve\n",
      "17248/17248 [==============================] - 14s 820us/step - loss: 0.0876 - acc: 0.3183 - val_loss: 0.1403 - val_acc: 0.3052\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_MODELS = 30\n",
    "train_ratio = 0.5\n",
    "MODEL_NUMBERS = range(1, N_MODELS + 1)\n",
    "# For - Loop\n",
    "for MODEL_NUMBER in MODEL_NUMBERS:\n",
    "    print '*** MODEL:', MODEL_NUMBER, ' ***'\n",
    "    # *** Train - Val Split ***\n",
    "    train_i, val_i = get_random_split(len(X), train_ratio)\n",
    "\n",
    "    #\n",
    "    X_train, y_train = X[train_i], y[train_i]\n",
    "    X_val, y_val = X[val_i], y[val_i]\n",
    "    print 'train_i[:5]', train_i[:5]\n",
    "    print 'val_i[:5]', val_i[:5]\n",
    "    print 'X_train.shape:', X_train.shape\n",
    "    print 'y_train.shape:', y_train.shape\n",
    "    print 'X_val.shape:', X_val.shape\n",
    "    print 'y_val.shape:', y_val.shape\n",
    "    print 'np.mean(y_train):', np.mean(y_train)\n",
    "    print 'np.mean(y_val):', np.mean(y_val)\n",
    "    \n",
    "    # *** Data Augmentation ***\n",
    "    # Train\n",
    "    X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "    y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "    # Validation\n",
    "    X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "    y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "    # \n",
    "    print 'X_train.shape:', X_train.shape\n",
    "    print 'y_train.shape:', y_train.shape\n",
    "    print 'X_val.shape:', X_val.shape\n",
    "    print 'y_val.shape:', y_val.shape\n",
    "    \n",
    "    # *** Training ***\n",
    "    model = get_model(input_shape=(75, 75, 2))\n",
    "    # Callbacks\n",
    "    def get_lr(epoch):\n",
    "        lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "        lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "        print 'lr:', lr\n",
    "        return lr\n",
    "    MODEL_PATH = './models/model16/model' + str(MODEL_NUMBER) + '.h5'\n",
    "    m_q = 'val_loss'\n",
    "    model_path = MODEL_PATH\n",
    "    check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "    early_stop = callbacks.EarlyStopping(patience=10, monitor=m_q, verbose=1)\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "    schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "    callback_list = [check_pt, early_stop, reduce_lr]\n",
    "    # fit\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 3s 328us/step\n",
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 2s 244us/step\n",
      "8424/8424 [==============================] - 2s 275us/step\n",
      "8424/8424 [==============================] - 2s 202us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 256us/step\n",
      "1\n",
      "8424/8424 [==============================] - 3s 323us/step\n",
      "8424/8424 [==============================] - 3s 298us/step\n",
      "8424/8424 [==============================] - 2s 229us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "2\n",
      "8424/8424 [==============================] - 3s 319us/step\n",
      "8424/8424 [==============================] - 2s 287us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 286us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "8424/8424 [==============================] - 2s 270us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 260us/step\n",
      "8424/8424 [==============================] - 2s 286us/step\n",
      "4\n",
      "8424/8424 [==============================] - 3s 362us/step\n",
      "8424/8424 [==============================] - 3s 354us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "5\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 253us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 237us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 233us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "6\n",
      "8424/8424 [==============================] - 3s 351us/step\n",
      "8424/8424 [==============================] - 3s 319us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 2s 287us/step\n",
      "8424/8424 [==============================] - 2s 296us/step\n",
      "8424/8424 [==============================] - 2s 266us/step\n",
      "7\n",
      "8424/8424 [==============================] - 3s 349us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "8424/8424 [==============================] - 2s 296us/step\n",
      "8424/8424 [==============================] - 3s 338us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 280us/step\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "8\n",
      "8424/8424 [==============================] - 3s 371us/step\n",
      "8424/8424 [==============================] - 3s 302us/step\n",
      "8424/8424 [==============================] - 2s 262us/step\n",
      "8424/8424 [==============================] - 3s 306us/step\n",
      "8424/8424 [==============================] - 2s 292us/step\n",
      "8424/8424 [==============================] - 2s 296us/step\n",
      "8424/8424 [==============================] - 3s 298us/step\n",
      "9\n",
      "8424/8424 [==============================] - 3s 384us/step\n",
      "8424/8424 [==============================] - 3s 336us/step\n",
      "8424/8424 [==============================] - 2s 295us/step\n",
      "8424/8424 [==============================] - 3s 340us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "8424/8424 [==============================] - 2s 273us/step\n",
      "10\n",
      "8424/8424 [==============================] - 3s 324us/step\n",
      "8424/8424 [==============================] - 3s 304us/step\n",
      "8424/8424 [==============================] - 2s 290us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 3s 307us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 290us/step\n",
      "11\n",
      "8424/8424 [==============================] - 3s 351us/step\n",
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 3s 312us/step\n",
      "8424/8424 [==============================] - 3s 324us/step\n",
      "8424/8424 [==============================] - 2s 291us/step\n",
      "8424/8424 [==============================] - 2s 270us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "12\n",
      "8424/8424 [==============================] - 3s 376us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 3s 303us/step\n",
      "8424/8424 [==============================] - 3s 336us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 3s 302us/step\n",
      "13\n",
      "8424/8424 [==============================] - 3s 390us/step\n",
      "8424/8424 [==============================] - 3s 349us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 276us/step\n",
      "8424/8424 [==============================] - 3s 305us/step\n",
      "8424/8424 [==============================] - 2s 294us/step\n",
      "14\n",
      "8424/8424 [==============================] - 3s 397us/step\n",
      "8424/8424 [==============================] - 3s 353us/step\n",
      "8424/8424 [==============================] - 2s 283us/step\n",
      "8424/8424 [==============================] - 3s 325us/step\n",
      "8424/8424 [==============================] - 2s 296us/step\n",
      "8424/8424 [==============================] - 3s 303us/step\n",
      "8424/8424 [==============================] - 2s 277us/step\n",
      "15\n",
      "8424/8424 [==============================] - 3s 354us/step\n",
      "8424/8424 [==============================] - 3s 337us/step\n",
      "8424/8424 [==============================] - 3s 322us/step\n",
      "8424/8424 [==============================] - 3s 331us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "8424/8424 [==============================] - 2s 235us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "16\n",
      "8424/8424 [==============================] - 3s 367us/step\n",
      "8424/8424 [==============================] - 3s 326us/step\n",
      "8424/8424 [==============================] - 3s 306us/step\n",
      "8424/8424 [==============================] - 3s 329us/step\n",
      "8424/8424 [==============================] - 3s 316us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 288us/step\n",
      "17\n",
      "8424/8424 [==============================] - 3s 386us/step\n",
      "8424/8424 [==============================] - 3s 340us/step\n",
      "8424/8424 [==============================] - 2s 296us/step\n",
      "8424/8424 [==============================] - 3s 326us/step\n",
      "8424/8424 [==============================] - 3s 311us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "18\n",
      "8424/8424 [==============================] - 3s 314us/step\n",
      "8424/8424 [==============================] - 2s 219us/step\n",
      "8424/8424 [==============================] - 2s 226us/step\n",
      "8424/8424 [==============================] - 2s 283us/step\n",
      "8424/8424 [==============================] - 2s 280us/step\n",
      "8424/8424 [==============================] - 2s 290us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "19\n",
      "8424/8424 [==============================] - 3s 415us/step\n",
      "8424/8424 [==============================] - 3s 324us/step\n",
      "8424/8424 [==============================] - 3s 311us/step\n",
      "8424/8424 [==============================] - 3s 331us/step\n",
      "8424/8424 [==============================] - 2s 279us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 3s 300us/step\n",
      "20\n",
      "8424/8424 [==============================] - 3s 412us/step\n",
      "8424/8424 [==============================] - 3s 337us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 3s 343us/step\n",
      "8424/8424 [==============================] - 3s 303us/step\n",
      "8424/8424 [==============================] - 3s 312us/step\n",
      "8424/8424 [==============================] - 2s 280us/step\n",
      "21\n",
      "8424/8424 [==============================] - 4s 428us/step\n",
      "8424/8424 [==============================] - 3s 342us/step\n",
      "8424/8424 [==============================] - 3s 306us/step\n",
      "8424/8424 [==============================] - 3s 347us/step\n",
      "8424/8424 [==============================] - 3s 301us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "22\n",
      "8424/8424 [==============================] - 3s 338us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "8424/8424 [==============================] - 2s 277us/step\n",
      "8424/8424 [==============================] - 3s 303us/step\n",
      "8424/8424 [==============================] - 3s 310us/step\n",
      "8424/8424 [==============================] - 3s 300us/step\n",
      "8424/8424 [==============================] - 2s 241us/step\n",
      "23\n",
      "8424/8424 [==============================] - 3s 415us/step\n",
      "8424/8424 [==============================] - 3s 352us/step\n",
      "8424/8424 [==============================] - 3s 315us/step\n",
      "8424/8424 [==============================] - 3s 344us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "24\n",
      "8424/8424 [==============================] - 4s 441us/step\n",
      "8424/8424 [==============================] - 3s 362us/step\n",
      "8424/8424 [==============================] - 3s 310us/step\n",
      "8424/8424 [==============================] - 3s 345us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 3s 313us/step\n",
      "8424/8424 [==============================] - 3s 299us/step\n",
      "25\n",
      "8424/8424 [==============================] - 4s 435us/step\n",
      "8424/8424 [==============================] - 3s 335us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "8424/8424 [==============================] - 3s 352us/step\n",
      "8424/8424 [==============================] - 3s 329us/step\n",
      "8424/8424 [==============================] - 3s 305us/step\n",
      "8424/8424 [==============================] - 3s 302us/step\n",
      "26\n",
      "8424/8424 [==============================] - 4s 418us/step\n",
      "8424/8424 [==============================] - 3s 326us/step\n",
      "8424/8424 [==============================] - 3s 305us/step\n",
      "8424/8424 [==============================] - 3s 340us/step\n",
      "8424/8424 [==============================] - 3s 320us/step\n",
      "8424/8424 [==============================] - 3s 313us/step\n",
      "8424/8424 [==============================] - 3s 310us/step\n",
      "27\n",
      "8424/8424 [==============================] - 4s 444us/step\n",
      "8424/8424 [==============================] - 3s 340us/step\n",
      "8424/8424 [==============================] - 3s 311us/step\n",
      "8424/8424 [==============================] - 3s 328us/step\n",
      "8424/8424 [==============================] - 3s 325us/step\n",
      "8424/8424 [==============================] - 3s 323us/step\n",
      "8424/8424 [==============================] - 3s 309us/step\n",
      "28\n",
      "8424/8424 [==============================] - 4s 430us/step\n",
      "8424/8424 [==============================] - 3s 344us/step\n",
      "8424/8424 [==============================] - 3s 310us/step\n",
      "8424/8424 [==============================] - 3s 321us/step\n",
      "8424/8424 [==============================] - 3s 308us/step\n",
      "8424/8424 [==============================] - 3s 316us/step\n",
      "8424/8424 [==============================] - 3s 319us/step\n",
      "29\n",
      "8424/8424 [==============================] - 3s 371us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 234us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 2s 252us/step\n",
      "8424/8424 [==============================] - 2s 212us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "# weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "# weights = [0.2, 0.18, 0.2, 0.2, 0.22]\n",
    "weights = [0.1] * N_MODELS\n",
    "for i, w in zip(range(N_MODELS), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model16/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub33.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
