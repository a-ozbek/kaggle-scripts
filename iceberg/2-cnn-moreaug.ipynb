{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras import losses, optimizers, callbacks\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from scipy import fftpack\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1604, 75, 75, 2)\n",
      "y_train.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X_train, y_train = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    # Append\n",
    "    X_train.append(im)\n",
    "    y_train.append(label)    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1283, 75, 75, 2)\n",
      "X_val.shape: (321, 75, 75, 2)\n",
      "y_train.shape: (1283,)\n",
      "y_val.shape: (321,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(im):\n",
    "    return im\n",
    "\n",
    "def h_flip(im):\n",
    "    return im[:, ::-1]\n",
    "\n",
    "def v_flip(x):\n",
    "    return im[::-1, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def random_rot(im):\n",
    "    angle = int(np.random.rand() * 360.0)\n",
    "    return transform.rotate(im, angle=angle, mode='reflect')\n",
    "\n",
    "aug_funcs = [h_flip, v_flip, random_rot]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(X, y, aug_funcs, batch_size=32, augment=True):\n",
    "    \"\"\"\n",
    "    Generates random data\n",
    "    \"\"\"\n",
    "    X = list(np.copy(X))\n",
    "    y = list(np.copy(y))\n",
    "    X_y = zip(X, y)\n",
    "    while True:\n",
    "        X_y_batch = random.sample(X_y, batch_size)\n",
    "        X_batch = [e[0] for e in X_y_batch]\n",
    "        y_batch = [e[1] for e in X_y_batch]\n",
    "        # Random augmentation        \n",
    "        for i, x in enumerate(X_batch):            \n",
    "            funcs2apply = random.sample(aug_funcs, np.random.randint(len(aug_funcs) + 1))\n",
    "            for f in funcs2apply:\n",
    "                x = f(x)\n",
    "            X_batch[i] = x\n",
    "        yield np.array(X_batch), np.array(y_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Adam()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.5912Epoch 00000: val_loss improved from inf to 0.53900, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.6434 - acc: 0.5908 - val_loss: 0.5390 - val_acc: 0.7414\n",
      "Epoch 2/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.6560Epoch 00001: val_loss improved from 0.53900 to 0.34497, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.5707 - acc: 0.6560 - val_loss: 0.3450 - val_acc: 0.8318\n",
      "Epoch 3/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.6821Epoch 00002: val_loss improved from 0.34497 to 0.27723, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.5227 - acc: 0.6814 - val_loss: 0.2772 - val_acc: 0.8785\n",
      "Epoch 4/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.6904Epoch 00003: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.5104 - acc: 0.6898 - val_loss: 0.2985 - val_acc: 0.8411\n",
      "Epoch 5/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.7073Epoch 00004: val_loss improved from 0.27723 to 0.20220, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4772 - acc: 0.7075 - val_loss: 0.2022 - val_acc: 0.9221\n",
      "Epoch 6/200\n",
      "317/320 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.7041Epoch 00005: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4881 - acc: 0.7042 - val_loss: 0.2370 - val_acc: 0.8941\n",
      "Epoch 7/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.7159Epoch 00006: val_loss improved from 0.20220 to 0.20191, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4690 - acc: 0.7157 - val_loss: 0.2019 - val_acc: 0.9065\n",
      "Epoch 8/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.7147Epoch 00007: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4667 - acc: 0.7150 - val_loss: 0.2419 - val_acc: 0.9159\n",
      "Epoch 9/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.7168Epoch 00008: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4581 - acc: 0.7174 - val_loss: 0.2299 - val_acc: 0.9034\n",
      "Epoch 10/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.7220Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000330000015674.\n",
      "320/320 [==============================] - 5s - loss: 0.4548 - acc: 0.7218 - val_loss: 0.2395 - val_acc: 0.8754\n",
      "Epoch 11/200\n",
      "317/320 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.7326Epoch 00010: val_loss improved from 0.20191 to 0.19854, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4337 - acc: 0.7335 - val_loss: 0.1985 - val_acc: 0.9221\n",
      "Epoch 12/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.7315Epoch 00011: val_loss improved from 0.19854 to 0.19492, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4258 - acc: 0.7317 - val_loss: 0.1949 - val_acc: 0.9315\n",
      "Epoch 13/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.7331Epoch 00012: val_loss improved from 0.19492 to 0.19163, saving model to ./models/model5.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4199 - acc: 0.7332 - val_loss: 0.1916 - val_acc: 0.9346\n",
      "Epoch 14/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.7382Epoch 00013: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4157 - acc: 0.7384 - val_loss: 0.2176 - val_acc: 0.9346\n",
      "Epoch 15/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.7381Epoch 00014: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4140 - acc: 0.7385 - val_loss: 0.2283 - val_acc: 0.9159\n",
      "Epoch 16/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.7289Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000108900003252.\n",
      "320/320 [==============================] - 5s - loss: 0.4262 - acc: 0.7292 - val_loss: 0.2193 - val_acc: 0.9283\n",
      "Epoch 17/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.7459Epoch 00016: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4075 - acc: 0.7458 - val_loss: 0.2080 - val_acc: 0.9315\n",
      "Epoch 18/200\n",
      "317/320 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.7472Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 3.59369999205e-05.\n",
      "320/320 [==============================] - 5s - loss: 0.4082 - acc: 0.7467 - val_loss: 0.2125 - val_acc: 0.9283\n",
      "Epoch 19/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.7458Epoch 00018: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4040 - acc: 0.7459 - val_loss: 0.2089 - val_acc: 0.9315\n",
      "Epoch 20/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.7409Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 1.18592095896e-05.\n",
      "320/320 [==============================] - 5s - loss: 0.4056 - acc: 0.7410 - val_loss: 0.2167 - val_acc: 0.9315\n",
      "Epoch 21/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.7385Epoch 00020: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4075 - acc: 0.7386 - val_loss: 0.2109 - val_acc: 0.9346\n",
      "Epoch 22/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.7368Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 3.91353921259e-06.\n",
      "320/320 [==============================] - 5s - loss: 0.4072 - acc: 0.7368 - val_loss: 0.2107 - val_acc: 0.9377\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cb0bd9890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "MODEL_PATH = './models/model5.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=8, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# Fit generator\n",
    "batch_size = 32\n",
    "steps_per_epoch = (len(X_train) / batch_size) * 8\n",
    "gen_obj = data_generator(X_train, y_train, batch_size=batch_size, aug_funcs=aug_funcs)\n",
    "model.fit_generator(gen_obj, steps_per_epoch=steps_per_epoch, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.7498Epoch 00000: val_loss improved from inf to 0.21181, saving model to ./models/model5-i3.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4010 - acc: 0.7495 - val_loss: 0.2118 - val_acc: 0.9408\n",
      "Epoch 2/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.7477Epoch 00001: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4017 - acc: 0.7480 - val_loss: 0.2124 - val_acc: 0.9377\n",
      "Epoch 3/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.7455Epoch 00002: val_loss improved from 0.21181 to 0.21059, saving model to ./models/model5-i3.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4065 - acc: 0.7459 - val_loss: 0.2106 - val_acc: 0.9377\n",
      "Epoch 4/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.7427Epoch 00003: val_loss improved from 0.21059 to 0.20848, saving model to ./models/model5-i3.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4090 - acc: 0.7420 - val_loss: 0.2085 - val_acc: 0.9346\n",
      "Epoch 5/200\n",
      "317/320 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.7419Epoch 00004: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4050 - acc: 0.7417 - val_loss: 0.2092 - val_acc: 0.9346\n",
      "Epoch 6/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.7398Epoch 00005: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4064 - acc: 0.7401 - val_loss: 0.2090 - val_acc: 0.9346\n",
      "Epoch 7/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.7446Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 1.29146797917e-06.\n",
      "320/320 [==============================] - 5s - loss: 0.4022 - acc: 0.7443 - val_loss: 0.2105 - val_acc: 0.9377\n",
      "Epoch 8/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.7411Epoch 00007: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4050 - acc: 0.7411 - val_loss: 0.2104 - val_acc: 0.9377\n",
      "Epoch 9/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.7509Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 4.26184415119e-07.\n",
      "320/320 [==============================] - 5s - loss: 0.4010 - acc: 0.7509 - val_loss: 0.2108 - val_acc: 0.9377\n",
      "Epoch 10/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.7390Epoch 00009: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4093 - acc: 0.7396 - val_loss: 0.2106 - val_acc: 0.9377\n",
      "Epoch 11/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.7505Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 1.40640856614e-07.\n",
      "320/320 [==============================] - 5s - loss: 0.3983 - acc: 0.7505 - val_loss: 0.2107 - val_acc: 0.9377\n",
      "Epoch 12/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.7457Epoch 00011: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.3988 - acc: 0.7455 - val_loss: 0.2107 - val_acc: 0.9377\n",
      "Epoch 13/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.7363Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 4.64114805254e-08.\n",
      "320/320 [==============================] - 5s - loss: 0.4073 - acc: 0.7362 - val_loss: 0.2107 - val_acc: 0.9377\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cb0bd97d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "MODEL_PATH = './models/model5-i3.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=8, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# Fit generator\n",
    "batch_size = 32\n",
    "steps_per_epoch = (len(X_train) / batch_size) * 8\n",
    "gen_obj = data_generator(X_train, y_train, batch_size=batch_size, aug_funcs=aug_funcs)\n",
    "model.fit_generator(gen_obj, steps_per_epoch=steps_per_epoch, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.7356Epoch 00000: val_loss improved from inf to 0.22135, saving model to ./models/model5-i2.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4181 - acc: 0.7352 - val_loss: 0.2213 - val_acc: 0.9128\n",
      "Epoch 2/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.7355Epoch 00001: val_loss improved from 0.22135 to 0.20667, saving model to ./models/model5-i2.h5\n",
      "320/320 [==============================] - 5s - loss: 0.4191 - acc: 0.7363 - val_loss: 0.2067 - val_acc: 0.9346\n",
      "Epoch 3/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.7402Epoch 00002: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4154 - acc: 0.7402 - val_loss: 0.2149 - val_acc: 0.9221\n",
      "Epoch 4/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.7371Epoch 00003: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4145 - acc: 0.7372 - val_loss: 0.2439 - val_acc: 0.9283\n",
      "Epoch 5/200\n",
      "317/320 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.7402Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.000108900003252.\n",
      "320/320 [==============================] - 5s - loss: 0.4147 - acc: 0.7408 - val_loss: 0.2146 - val_acc: 0.9190\n",
      "Epoch 6/200\n",
      "318/320 [============================>.] - ETA: 0s - loss: 0.4118 - acc: 0.7353Epoch 00005: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4121 - acc: 0.7353 - val_loss: 0.2130 - val_acc: 0.9190\n",
      "Epoch 7/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.7416Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 3.59369999205e-05.\n",
      "320/320 [==============================] - 5s - loss: 0.4042 - acc: 0.7413 - val_loss: 0.2210 - val_acc: 0.9221\n",
      "Epoch 8/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.7345Epoch 00007: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4113 - acc: 0.7348 - val_loss: 0.2262 - val_acc: 0.9221\n",
      "Epoch 9/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.7412Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 1.18592095896e-05.\n",
      "320/320 [==============================] - 5s - loss: 0.4057 - acc: 0.7408 - val_loss: 0.2186 - val_acc: 0.9252\n",
      "Epoch 10/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.7449Epoch 00009: val_loss did not improve\n",
      "320/320 [==============================] - 5s - loss: 0.4034 - acc: 0.7447 - val_loss: 0.2199 - val_acc: 0.9252\n",
      "Epoch 11/200\n",
      "316/320 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.7434Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 3.91353921259e-06.\n",
      "320/320 [==============================] - 5s - loss: 0.4059 - acc: 0.7433 - val_loss: 0.2156 - val_acc: 0.9252\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ca411e510>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "model = load_model('./models/model5.h5')\n",
    "MODEL_PATH = './models/model5-i2.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=8, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# Fit generator\n",
    "batch_size = 32\n",
    "steps_per_epoch = (len(X_train) / batch_size) * 8\n",
    "gen_obj = data_generator(X_train, y_train, batch_size=batch_size, aug_funcs=aug_funcs)\n",
    "model.fit_generator(gen_obj, steps_per_epoch=steps_per_epoch, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)   \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)    \n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "MODEL_PATH = './models/model5.h5'\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_test_p = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8320/8424 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# predict - tta\n",
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "tta_aug_funcs = [bypass, \n",
    "                 h_flip, v_flip, hv_flip,\n",
    "                 rot90, rot180, rot270,\n",
    "                 rot45, rot135, rot315]\n",
    "\n",
    "y_test_p = 0\n",
    "for func in tta_aug_funcs:\n",
    "    y_test_p += model.predict(func(X_test), verbose=1).flatten()\n",
    "y_test_p = y_test_p / len(tta_aug_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
