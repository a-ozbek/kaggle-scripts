{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras import losses, optimizers, callbacks\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test\n",
    "# df = pd.read_json('./data/test.json')\n",
    "# X_test, y_test = [], []\n",
    "# for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "#     im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "#     im_band2 = np.array(im_band2).reshape(75, 75, 1)\n",
    "#     im_band1 /= np.std(im_band1)\n",
    "#     im_band2 /= np.std(im_band2)    \n",
    "#     im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "#     X_test.append(im)\n",
    "#     y_test.append(label)    \n",
    "# X_test = np.array(X_test)\n",
    "# y_test = np.array(y_test)\n",
    "# print 'X_test.shape:', X_test.shape\n",
    "# print 'y_test.shape:', y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1284, 75, 75, 2)\n",
      "y_train.shape: (1284,)\n",
      "X_val.shape: (320, 75, 75, 2)\n",
      "y_val.shape: (320,)\n",
      "np.mean(y_train): 0.469626168224\n",
      "np.mean(y_val): 0.46875\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8988, 75, 75, 2)\n",
      "y_train.shape: (8988,)\n",
      "X_val.shape: (2240, 75, 75, 2)\n",
      "y_val.shape: (2240,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    # Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    loss = losses.binary_crossentropy\n",
    "    optimizer = optimizers.Adam()\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    #     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(75, 75, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8988 samples, validate on 2240 samples\n",
      "Epoch 1/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.7362Epoch 00000: val_loss improved from inf to 0.36493, saving model to ./models/model4/model5.h5\n",
      "8988/8988 [==============================] - 6s - loss: 0.4930 - acc: 0.7365 - val_loss: 0.3649 - val_acc: 0.8304\n",
      "Epoch 2/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.8662Epoch 00001: val_loss improved from 0.36493 to 0.25742, saving model to ./models/model4/model5.h5\n",
      "8988/8988 [==============================] - 5s - loss: 0.3011 - acc: 0.8665 - val_loss: 0.2574 - val_acc: 0.8705\n",
      "Epoch 3/200\n",
      "8928/8988 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.8922Epoch 00002: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.2567 - acc: 0.8922 - val_loss: 0.2925 - val_acc: 0.8656\n",
      "Epoch 4/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9106Epoch 00003: val_loss improved from 0.25742 to 0.23715, saving model to ./models/model4/model5.h5\n",
      "8988/8988 [==============================] - 5s - loss: 0.2188 - acc: 0.9107 - val_loss: 0.2371 - val_acc: 0.9031\n",
      "Epoch 5/200\n",
      "8864/8988 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9173Epoch 00004: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.1977 - acc: 0.9173 - val_loss: 0.2424 - val_acc: 0.8920\n",
      "Epoch 6/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9296Epoch 00005: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.1781 - acc: 0.9292 - val_loss: 0.2830 - val_acc: 0.8777\n",
      "Epoch 7/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9263Epoch 00006: val_loss improved from 0.23715 to 0.21768, saving model to ./models/model4/model5.h5\n",
      "8988/8988 [==============================] - 5s - loss: 0.1715 - acc: 0.9266 - val_loss: 0.2177 - val_acc: 0.9094\n",
      "Epoch 8/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9376Epoch 00007: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.1542 - acc: 0.9377 - val_loss: 0.2528 - val_acc: 0.8960\n",
      "Epoch 9/200\n",
      "8960/8988 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9434Epoch 00008: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.1417 - acc: 0.9435 - val_loss: 0.2586 - val_acc: 0.9232\n",
      "Epoch 10/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9455Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000500000023749.\n",
      "8988/8988 [==============================] - 5s - loss: 0.1300 - acc: 0.9454 - val_loss: 0.2575 - val_acc: 0.9161\n",
      "Epoch 11/200\n",
      "8864/8988 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9637Epoch 00010: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.0928 - acc: 0.9634 - val_loss: 0.2459 - val_acc: 0.9138\n",
      "Epoch 12/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9679Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.000250000011874.\n",
      "8988/8988 [==============================] - 5s - loss: 0.0794 - acc: 0.9682 - val_loss: 0.3287 - val_acc: 0.9040\n",
      "Epoch 13/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9737Epoch 00012: val_loss did not improve\n",
      "8988/8988 [==============================] - 5s - loss: 0.0641 - acc: 0.9733 - val_loss: 0.3015 - val_acc: 0.9129\n",
      "Epoch 14/200\n",
      "8896/8988 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9800Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000125000005937.\n",
      "8988/8988 [==============================] - 5s - loss: 0.0510 - acc: 0.9799 - val_loss: 0.3066 - val_acc: 0.9116\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc6b435dd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 1e-3 + 1e-7)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model4/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=6, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 3s 304us/step\n",
      "8424/8424 [==============================] - 2s 289us/step\n",
      "8424/8424 [==============================] - 2s 255us/step\n",
      "8424/8424 [==============================] - 3s 339us/step\n",
      "8424/8424 [==============================] - 2s 211us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 2s 196us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "8424/8424 [==============================] - 2s 284us/step\n",
      "8424/8424 [==============================] - 2s 238us/step\n",
      "8424/8424 [==============================] - 2s 194us/step\n",
      "8424/8424 [==============================] - 2s 218us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 246us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 242us/step\n",
      "8424/8424 [==============================] - 2s 293us/step\n",
      "8424/8424 [==============================] - 2s 245us/step\n",
      "8424/8424 [==============================] - 2s 259us/step\n",
      "8424/8424 [==============================] - 2s 243us/step\n",
      "8424/8424 [==============================] - 2s 224us/step\n",
      "8424/8424 [==============================] - 2s 195us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 219us/step\n",
      "8424/8424 [==============================] - 3s 325us/step\n",
      "8424/8424 [==============================] - 2s 258us/step\n",
      "8424/8424 [==============================] - 3s 331us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 186us/step\n",
      "8424/8424 [==============================] - 2s 191us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 205us/step\n",
      "8424/8424 [==============================] - 2s 231us/step\n",
      "8424/8424 [==============================] - 2s 193us/step\n",
      "8424/8424 [==============================] - 2s 232us/step\n",
      "8424/8424 [==============================] - 2s 185us/step\n",
      "8424/8424 [==============================] - 2s 190us/step\n",
      "8424/8424 [==============================] - 2s 190us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model4/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
