{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "\n",
    "* Add FFT channels to CNN (Tried, does not make much difference)\n",
    "* Finetune CNN (with SGD slow learning rate)\n",
    "* 5-fold CNN\n",
    "* Extract Features from CNN (before FC) and do XGB\n",
    "* TTA (tried, made it better)\n",
    "* More augmenting, additional 45, 135, 315 degrees\n",
    "* More augmenting, random rotations and flips\n",
    "* Predict test data and train with test\n",
    "* Train on all of the training data (no train-val split)\n",
    "* Try a different combination of combine predictions\n",
    "* Fine-tune on pre-trained models (Get rid of some top layers because input size is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, inception and resnet-like models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/can/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, LeakyReLU, BatchNormalization, Activation\n",
    "from keras import losses, optimizers, callbacks\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split, StratifiedKFold, KFold\n",
    "from scipy import fftpack\n",
    "import inception_like_model\n",
    "import resnet_like_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1604, 75, 75, 2)\n",
      "y.shape: (1604,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "df = pd.read_json('./data/train.json')\n",
    "X, y = [], []\n",
    "for im_band1, im_band2, label in zip(df['band_1'], df['band_2'], df['is_iceberg']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess\n",
    "    # - Zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # - Normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X.append(im)\n",
    "    y.append(label)    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "MODEL_NUMBER = 1\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, random_state=RANDOM_SEED, shuffle=True)\n",
    "cv = list(skf.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (1282, 75, 75, 2)\n",
      "y_train.shape: (1282,)\n",
      "X_val.shape: (322, 75, 75, 2)\n",
      "y_val.shape: (322,)\n",
      "np.mean(y_train): 0.469578783151\n",
      "np.mean(y_val): 0.468944099379\n"
     ]
    }
   ],
   "source": [
    "train_i, val_i = cv[MODEL_NUMBER - 1]\n",
    "X_train, y_train = X[train_i], y[train_i]\n",
    "X_val, y_val = X[val_i], y[val_i]\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape\n",
    "print 'np.mean(y_train):', np.mean(y_train)\n",
    "print 'np.mean(y_val):', np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# print 'X_train.shape:', X_train.shape\n",
    "# print 'X_val.shape:', X_val.shape\n",
    "# print 'y_train.shape:', y_train.shape\n",
    "# print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bypass(x):\n",
    "    return x\n",
    "\n",
    "def h_flip(x):\n",
    "    return x[:, :, ::-1, :]\n",
    "\n",
    "def v_flip(x):\n",
    "    return x[:, ::-1, :, :]\n",
    "\n",
    "def hv_flip(x):\n",
    "    return h_flip(v_flip(x))\n",
    "\n",
    "def rot90(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 90), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot180(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 180), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot270(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 270), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot45(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 45, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot135(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 135, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "def rot315(x):\n",
    "    return np.concatenate([np.expand_dims(transform.rotate(im, 315, mode='reflect'), axis=0) for im in x], axis=0)\n",
    "\n",
    "aug_funcs = [bypass, \n",
    "             h_flip, v_flip, hv_flip,\n",
    "             rot90, rot180, rot270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8974, 75, 75, 2)\n",
      "y_train.shape: (8974,)\n",
      "X_val.shape: (2254, 75, 75, 2)\n",
      "y_val.shape: (2254,)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train = np.concatenate([func(X_train) for func in aug_funcs], axis=0)\n",
    "y_train = np.concatenate([y_train] * len(aug_funcs))\n",
    "\n",
    "# Validation\n",
    "X_val = np.concatenate([func(X_val) for func in aug_funcs], axis=0)\n",
    "y_val = np.concatenate([y_val] * len(aug_funcs))\n",
    "\n",
    "# \n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_val.shape:', X_val.shape\n",
    "print 'y_val.shape:', y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_model(input_shape):\n",
    "#     # Architecture\n",
    "#     model = Sequential()\n",
    "#     # Block 1\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "#                      activation=None,\n",
    "#                      input_shape=input_shape))\n",
    "# #     model.add(BatchNormalization(axis=3, scale=False))\n",
    "#     model.add(Activation('relu'))\n",
    "#     # Block 2\n",
    "#     model.add(Conv2D(64, (3, 3), activation=None))    \n",
    "# #     model.add(BatchNormalization(axis=3, scale=False))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "#     # Block 3\n",
    "#     model.add(Conv2D(128, (3, 3), activation=None))\n",
    "# #     model.add(BatchNormalization(axis=3, scale=False))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))    \n",
    "#     # Block 4\n",
    "#     model.add(Conv2D(256, (3, 3), activation=None))\n",
    "#     model.add(BatchNormalization(axis=3, scale=False))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     # FC\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(32))\n",
    "#     model.add(BatchNormalization(scale=False))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     # Compile the model\n",
    "#     loss = losses.binary_crossentropy\n",
    "#     optimizer = optimizers.Adam()\n",
    "#     metrics = ['accuracy']\n",
    "#     model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "#     #     \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = resnet_like_model.get_model(input_shape=(75, 75, 2))\n",
    "\n",
    "# Compile the model\n",
    "loss = losses.binary_crossentropy\n",
    "optimizer = optimizers.SGD()\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8974 samples, validate on 2254 samples\n",
      "Epoch 1/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.5351Epoch 00001: val_loss improved from inf to 0.66072, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 4s 423us/step - loss: 0.6960 - acc: 0.5352 - val_loss: 0.6607 - val_acc: 0.5195\n",
      "Epoch 2/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.5903Epoch 00002: val_loss improved from 0.66072 to 0.62574, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 315us/step - loss: 0.6470 - acc: 0.5906 - val_loss: 0.6257 - val_acc: 0.6628\n",
      "Epoch 3/200\n",
      "8960/8974 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.6533Epoch 00003: val_loss improved from 0.62574 to 0.57182, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 317us/step - loss: 0.6064 - acc: 0.6536 - val_loss: 0.5718 - val_acc: 0.7138\n",
      "Epoch 4/200\n",
      "8960/8974 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.7342Epoch 00004: val_loss improved from 0.57182 to 0.46430, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 318us/step - loss: 0.5304 - acc: 0.7342 - val_loss: 0.4643 - val_acc: 0.7831\n",
      "Epoch 5/200\n",
      "8960/8974 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.7732Epoch 00005: val_loss improved from 0.46430 to 0.40506, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 320us/step - loss: 0.4596 - acc: 0.7732 - val_loss: 0.4051 - val_acc: 0.8021\n",
      "Epoch 6/200\n",
      "8800/8974 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.7925Epoch 00006: val_loss improved from 0.40506 to 0.38158, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 325us/step - loss: 0.4241 - acc: 0.7921 - val_loss: 0.3816 - val_acc: 0.8194\n",
      "Epoch 7/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8122Epoch 00007: val_loss improved from 0.38158 to 0.35447, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 321us/step - loss: 0.3980 - acc: 0.8125 - val_loss: 0.3545 - val_acc: 0.8279\n",
      "Epoch 8/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8281Epoch 00008: val_loss improved from 0.35447 to 0.33141, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 321us/step - loss: 0.3699 - acc: 0.8285 - val_loss: 0.3314 - val_acc: 0.8376\n",
      "Epoch 9/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8399Epoch 00009: val_loss improved from 0.33141 to 0.30661, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 322us/step - loss: 0.3493 - acc: 0.8405 - val_loss: 0.3066 - val_acc: 0.8616\n",
      "Epoch 10/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8520Epoch 00010: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 317us/step - loss: 0.3298 - acc: 0.8522 - val_loss: 0.3111 - val_acc: 0.8611\n",
      "Epoch 11/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8624Epoch 00011: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 318us/step - loss: 0.3147 - acc: 0.8607 - val_loss: 0.3202 - val_acc: 0.8492\n",
      "Epoch 12/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.8663Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.00329999992624.\n",
      "8974/8974 [==============================] - 3s 321us/step - loss: 0.3112 - acc: 0.8664 - val_loss: 0.3078 - val_acc: 0.8642\n",
      "Epoch 13/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.8753Epoch 00013: val_loss improved from 0.30661 to 0.28302, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 320us/step - loss: 0.2886 - acc: 0.8755 - val_loss: 0.2830 - val_acc: 0.8749\n",
      "Epoch 14/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.8842Epoch 00014: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 320us/step - loss: 0.2807 - acc: 0.8841 - val_loss: 0.2949 - val_acc: 0.8620\n",
      "Epoch 15/200\n",
      "8896/8974 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.8795Epoch 00015: val_loss improved from 0.28302 to 0.28110, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 319us/step - loss: 0.2829 - acc: 0.8793 - val_loss: 0.2811 - val_acc: 0.8740\n",
      "Epoch 16/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.8846Epoch 00016: val_loss improved from 0.28110 to 0.27758, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 319us/step - loss: 0.2810 - acc: 0.8848 - val_loss: 0.2776 - val_acc: 0.8736\n",
      "Epoch 17/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.8851Epoch 00017: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 321us/step - loss: 0.2745 - acc: 0.8848 - val_loss: 0.3042 - val_acc: 0.8558\n",
      "Epoch 18/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.8840Epoch 00018: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 314us/step - loss: 0.2779 - acc: 0.8840 - val_loss: 0.2826 - val_acc: 0.8691\n",
      "Epoch 19/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.8890Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0010889999941.\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2728 - acc: 0.8890 - val_loss: 0.2787 - val_acc: 0.8722\n",
      "Epoch 20/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.8941Epoch 00020: val_loss improved from 0.27758 to 0.27249, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 320us/step - loss: 0.2609 - acc: 0.8938 - val_loss: 0.2725 - val_acc: 0.8767\n",
      "Epoch 21/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.8936Epoch 00021: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2636 - acc: 0.8941 - val_loss: 0.2783 - val_acc: 0.8709\n",
      "Epoch 22/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.8945Epoch 00022: val_loss improved from 0.27249 to 0.27086, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 355us/step - loss: 0.2638 - acc: 0.8944 - val_loss: 0.2709 - val_acc: 0.8771\n",
      "Epoch 23/200\n",
      "8928/8974 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.8907Epoch 00023: val_loss improved from 0.27086 to 0.26865, saving model to ./models/model9/model1.h5\n",
      "8974/8974 [==============================] - 3s 341us/step - loss: 0.2692 - acc: 0.8906 - val_loss: 0.2687 - val_acc: 0.8780\n",
      "Epoch 24/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.8967Epoch 00024: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 321us/step - loss: 0.2565 - acc: 0.8960 - val_loss: 0.2802 - val_acc: 0.8722\n",
      "Epoch 25/200\n",
      "8800/8974 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.8917Epoch 00025: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 334us/step - loss: 0.2657 - acc: 0.8914 - val_loss: 0.2711 - val_acc: 0.8771\n",
      "Epoch 26/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.8942Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 0.000359370008809.\n",
      "8974/8974 [==============================] - 3s 315us/step - loss: 0.2614 - acc: 0.8950 - val_loss: 0.2687 - val_acc: 0.8762\n",
      "Epoch 27/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.8941Epoch 00027: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 314us/step - loss: 0.2619 - acc: 0.8945 - val_loss: 0.2722 - val_acc: 0.8767\n",
      "Epoch 28/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.8958Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 0.000118592098297.\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2627 - acc: 0.8957 - val_loss: 0.2718 - val_acc: 0.8771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.8916Epoch 00029: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 314us/step - loss: 0.2610 - acc: 0.8919 - val_loss: 0.2708 - val_acc: 0.8780\n",
      "Epoch 30/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.8976Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 3.91353927262e-05.\n",
      "8974/8974 [==============================] - 3s 315us/step - loss: 0.2543 - acc: 0.8971 - val_loss: 0.2713 - val_acc: 0.8776\n",
      "Epoch 31/200\n",
      "8800/8974 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.8962Epoch 00031: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2602 - acc: 0.8951 - val_loss: 0.2706 - val_acc: 0.8776\n",
      "Epoch 32/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.8952Epoch 00032: val_loss did not improve\n",
      "\n",
      "Epoch 00032: reducing learning rate to 1.29146797917e-05.\n",
      "8974/8974 [==============================] - 3s 312us/step - loss: 0.2581 - acc: 0.8939 - val_loss: 0.2712 - val_acc: 0.8767\n",
      "Epoch 33/200\n",
      "8896/8974 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9014Epoch 00033: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2561 - acc: 0.9014 - val_loss: 0.2714 - val_acc: 0.8758\n",
      "Epoch 34/200\n",
      "8896/8974 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.8942Epoch 00034: val_loss did not improve\n",
      "\n",
      "Epoch 00034: reducing learning rate to 4.26184445132e-06.\n",
      "8974/8974 [==============================] - 3s 318us/step - loss: 0.2569 - acc: 0.8946 - val_loss: 0.2721 - val_acc: 0.8762\n",
      "Epoch 35/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.8974Epoch 00035: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 318us/step - loss: 0.2590 - acc: 0.8971 - val_loss: 0.2716 - val_acc: 0.8758\n",
      "Epoch 36/200\n",
      "8896/8974 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.8996Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 1.40640865993e-06.\n",
      "8974/8974 [==============================] - 3s 316us/step - loss: 0.2557 - acc: 0.8994 - val_loss: 0.2703 - val_acc: 0.8784\n",
      "Epoch 37/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.8969Epoch 00037: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 318us/step - loss: 0.2566 - acc: 0.8969 - val_loss: 0.2709 - val_acc: 0.8780\n",
      "Epoch 38/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.8922Epoch 00038: val_loss did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 4.64114842771e-07.\n",
      "8974/8974 [==============================] - 3s 319us/step - loss: 0.2570 - acc: 0.8925 - val_loss: 0.2710 - val_acc: 0.8771\n",
      "Epoch 39/200\n",
      "8960/8974 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.8969Epoch 00039: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 325us/step - loss: 0.2598 - acc: 0.8968 - val_loss: 0.2703 - val_acc: 0.8784\n",
      "Epoch 40/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.8972Epoch 00040: val_loss did not improve\n",
      "\n",
      "Epoch 00040: reducing learning rate to 1.53157895113e-07.\n",
      "8974/8974 [==============================] - 3s 312us/step - loss: 0.2567 - acc: 0.8977 - val_loss: 0.2707 - val_acc: 0.8776\n",
      "Epoch 41/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.8937Epoch 00041: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 319us/step - loss: 0.2632 - acc: 0.8941 - val_loss: 0.2708 - val_acc: 0.8776\n",
      "Epoch 42/200\n",
      "8864/8974 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.8967Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 5.05421066066e-08.\n",
      "8974/8974 [==============================] - 3s 311us/step - loss: 0.2583 - acc: 0.8971 - val_loss: 0.2707 - val_acc: 0.8776\n",
      "Epoch 43/200\n",
      "8832/8974 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.8976Epoch 00043: val_loss did not improve\n",
      "8974/8974 [==============================] - 3s 314us/step - loss: 0.2523 - acc: 0.8969 - val_loss: 0.2701 - val_acc: 0.8780\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1920c4fcd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks\n",
    "def get_lr(epoch):\n",
    "    lr = (np.random.rand() * 4e-2 + 1e-7)\n",
    "    lr = np.clip(lr, a_min=None, a_max=0.025)\n",
    "    print 'lr:', lr\n",
    "    return lr\n",
    "MODEL_PATH = './models/model9/model' + str(MODEL_NUMBER) + '.h5'\n",
    "m_q = 'val_loss'\n",
    "model_path = MODEL_PATH\n",
    "check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(patience=20, monitor=m_q, verbose=1)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "schedule_lr = callbacks.LearningRateScheduler(get_lr)\n",
    "callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Finetune\n",
    "# loss = losses.binary_crossentropy\n",
    "# optimizer = optimizers.SGD(lr=1e-4)\n",
    "# metrics = ['accuracy']\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m_q = 'val_loss'\n",
    "# model_path = MODEL_PATH\n",
    "# check_pt = callbacks.ModelCheckpoint(filepath=model_path, monitor=m_q, save_best_only=True, verbose=1)\n",
    "# early_stop = callbacks.EarlyStopping(patience=5, monitor=m_q, verbose=1)\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(patience=2, factor=0.33, monitor=m_q, verbose=1)\n",
    "# callback_list = [check_pt, early_stop, reduce_lr]\n",
    "\n",
    "# model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=callback_list, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (8424, 75, 75, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_json('./data/test.json')\n",
    "X_test, y_test = [], []\n",
    "for im_band1, im_band2 in zip(df['band_1'], df['band_2']):\n",
    "    im_band1 = np.array(im_band1).reshape(75, 75, 1)\n",
    "    im_band2 = np.array(im_band2).reshape(75, 75, 1)    \n",
    "    # Preprocess - zero mean\n",
    "    im_band1 -= np.mean(im_band1)\n",
    "    im_band2 -= np.mean(im_band2)\n",
    "    # Preprocess - normalize\n",
    "    im_band1 /= np.std(im_band1)\n",
    "    im_band2 /= np.std(im_band2)    \n",
    "    im = np.concatenate([im_band1, im_band2], axis=2)\n",
    "    X_test.append(im)    \n",
    "X_test = np.array(X_test)\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8424/8424 [==============================] - 3s 317us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "8424/8424 [==============================] - 2s 250us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 222us/step\n",
      "8424/8424 [==============================] - 2s 218us/step\n",
      "1\n",
      "8424/8424 [==============================] - 2s 223us/step\n",
      "8424/8424 [==============================] - 2s 278us/step\n",
      "8424/8424 [==============================] - 2s 239us/step\n",
      "8424/8424 [==============================] - 2s 282us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "8424/8424 [==============================] - 2s 217us/step\n",
      "8424/8424 [==============================] - 2s 215us/step\n",
      "2\n",
      "8424/8424 [==============================] - 2s 228us/step\n",
      "8424/8424 [==============================] - 2s 269us/step\n",
      "8424/8424 [==============================] - 2s 221us/step\n",
      "8424/8424 [==============================] - 2s 240us/step\n",
      "8424/8424 [==============================] - 2s 207us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 213us/step\n",
      "3\n",
      "8424/8424 [==============================] - 2s 216us/step\n",
      "8424/8424 [==============================] - 2s 257us/step\n",
      "8424/8424 [==============================] - 2s 214us/step\n",
      "8424/8424 [==============================] - 2s 254us/step\n",
      "8424/8424 [==============================] - 2s 202us/step\n",
      "8424/8424 [==============================] - 2s 209us/step\n",
      "8424/8424 [==============================] - 2s 207us/step\n",
      "4\n",
      "8424/8424 [==============================] - 2s 210us/step\n",
      "8424/8424 [==============================] - 2s 247us/step\n",
      "8424/8424 [==============================] - 2s 208us/step\n",
      "8424/8424 [==============================] - 2s 251us/step\n",
      "8424/8424 [==============================] - 2s 198us/step\n",
      "8424/8424 [==============================] - 2s 204us/step\n",
      "8424/8424 [==============================] - 2s 202us/step\n"
     ]
    }
   ],
   "source": [
    "y_test_p = 0\n",
    "weights = [0.25, 0.4 / 3, 0.35, 0.4 / 3, 0.4 / 3]\n",
    "for i, w in zip(range(5), weights):\n",
    "    print i\n",
    "    # Load the model\n",
    "    MODEL_PATH = './models/model8/model' + str(i + 1) + '.h5'\n",
    "    model = load_model(MODEL_PATH)\n",
    "    # predict - tta    \n",
    "    for func in aug_funcs:\n",
    "        y_test_p += model.predict(func(X_test), verbose=1).flatten() * w\n",
    "# y_test_p = y_test_p / (len(aug_funcs) * 5.0)\n",
    "y_test_p = y_test_p / (len(aug_funcs) * sum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = df['id']\n",
    "df_sub['is_iceberg'] = y_test_p.flatten()\n",
    "df_sub.to_csv('./submissions/sub19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
